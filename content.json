{"pages":[{"title":"关于","text":"关于我和我的站点我叫李云龙，每当谈起我的名字总是能与某部电视剧联想起来。我总觉得怪不好意思的。给别人介绍的时候。都会重复一遍我的名字。 只能摸摸头。说着沾光了沾光了。 也许是我没有电视剧中的人物那么酷。 没有那种野性。也没有太多的闪光点。 不过也不算太糟糕。 我就是我。我没有必要成为别人想象中的样子。 做自己喜欢的事情足矣。 一个程序员我可能是小时候接触的互联网了。 在五六年级的时候，要么就是出去跟人一起打乒乓球，就是带着乒乓球去了网吧。 小时候也不知道互联网是什么。黑客是做什么的。玩过大大小小的游戏。因为舍不得充值，也葬送了我许多帐号。同时也葬送了我的成绩。 小时候成绩一直不好。英语从来也没有重视过。 不过还好，其他科目还算过的去。 不过因为这事情。也让我成为了一个复读生。 复读的时候也纠结过一阵子，当时如果没有复读 可能也不会在这里写博客了吧。 步入大学，精心选了一个个人觉得薪资比较高的专业。可能是因为复读的原因。比较努力对待这个专业。刚开始的时候满脑子都是 编程是什么 这些if有什么用。以前一直没有兴趣爱好。直到学Java我发现这是一个极具魅力的语言，许多东西是最基础，但也是最为重要的，当时为了快速学习搜罗了一大堆的网盘视频。天天都看。搞的有个舍友跟着学。 天天一起钻图书馆，重复看了好多基础书籍。。一直认为基础是最为重要的。 然后过了半个学期就入了一个组织。这个组织说起来也奇怪。也不归属学校。现在想起来。就是一个外包团队。就这样在这个团队中度过了我大学时光。 做的都是非常小的项目。 从代码到项目部署。全部流程都做了一遍。项目基本上都是用SpringBoot来做，在学校学的是Struts+Hibernate+Spring。总是不喜欢这种xml的风格。去spring官网看到了SpringBoot还有网上的各种吹捧。会变成主流。所以一直用着SpringBoot。也踏过了许许多多的坑。慢慢的也就踏上了社会，接过几个小项目。 然后也慢慢的喜欢上了看博客。总说好记性不如烂笔头。所以也就自己搭建了一个博客网站。收集一些文章。还有自己的总结。 找到许多特别有意思的博客。网站右边也有友链。 以前总是看视频，也没有怎么做笔记。忘记的也比较快。找到一个比较完整的书单。想好好的都看一看http://www.iocoder.cn/Architecture/books-recommended/","link":"/about/index.html"},{"title":"docker","text":"docker基本命令 docker logs 检查排错。如果启动不起容器，可以试着检查排错 docker安装jenkins及其相关问题解决 https://www.cnblogs.com/youcong/p/10182091.html systemctl stop firewalld.service 关闭防火墙 docker inspect 容器id 查询容器信息 docker stop 容器id 停止容器id docker rm 容器id 删除容器id systemctl restart docker 重启docker容器 docker exec -it 容器ID /bin/bash 进入容器 docker rm $(sudo docker ps -a -q) 删除所有未运行的容器 docker search elasticsearch 搜索镜像文件 docker run 创建并启动一个容器，在run后面加上-d参数，则会创建一个守护式容器在后台运行。 docker ps -a 查看已经创建的容器 docker ps -s 查看已经启动的容器 docker start con_name 启动容器名为con_name的容器 docker stop con_name 停止容器名为con_name的容器 docker rm con_name 删除容器名为con_name的容器 docker rename old_name new_name 重命名一个容器 docker attach con_name 将终端附着到正在运行的容器名为con_name的容器的终端上面去，前提是创建该容器时指定了相应的sh docker logs –tail=”10” 容器名称 查询容器日志信息","link":"/docker/index.html"}],"posts":[{"title":"Effective Java 读书笔记 - 09. 使用try-with-resources语句替代try-finally语句","text":"9. 使用 try-with-resources 语句替代 try-finally 语句 Java 类库中包含许多必须通过调用 close 方法手动关闭的资源。 比如 InputStream，OutputStream 和 java.sql.Connection。 客户经常忽视关闭资源，其性能结果可想而知。 尽管这些资源中有很多使用 finalizer 机制作为安全网，但 finalizer 机制却不能很好地工作（详见第 8 条）。 从以往来看，try-finally 语句是保证资源正确关闭的最佳方式，即使是在程序抛出异常或返回的情况下： 123456789// try-finally - No longer the best way to close resources!static String firstLineOfFile(String path) throws IOException { BufferedReader br = new BufferedReader(new FileReader(path)); try { return br.readLine(); } finally { br.close(); }} 这可能看起来并不坏，但是当添加第二个资源时，情况会变得更糟： 1234567891011121314151617// try-finally is ugly when used with more than one resource!static void copy(String src, String dst) throws IOException { InputStream in = new FileInputStream(src); try { OutputStream out = new FileOutputStream(dst); try { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); } finally { out.close(); } } finally { in.close(); }} 这可能很难相信，但即使是优秀的程序员，大多数时候也会犯错误。首先，我在 Java Puzzlers[Bloch05] 的第 88 页上弄错了，多年来没有人注意到。事实上，2007 年 Java 类库中使用 close 方法的三分之二都是错误的。 即使是用 try-finally 语句关闭资源的正确代码，如前面两个代码示例所示，也有一个微妙的缺陷。 try-with-resources 块和 finally 块中的代码都可以抛出异常。 例如，在 firstLineOfFile 方法中，由于底层物理设备发生故障，对 readLine 方法的调用可能会引发异常，并且由于相同的原因，调用 close 方法可能会失败。 在这种情况下，第二个异常完全冲掉了第一个异常。 在异常堆栈跟踪中没有第一个异常的记录，这可能使实际系统中的调试非常复杂——通常这是你想要诊断问题的第一个异常。 虽然可以编写代码来抑制第二个异常，但是实际上没有人这样做，因为它太冗长了。 当 Java 7 引入了 try-with-resources 语句时，所有这些问题一下子都得到了解决[JLS,14.20.3]。要使用这个构造，资源必须实现 AutoCloseable 接口，该接口由一个返回为 void 的 close 组成。Java 类库和第三方类库中的许多类和接口现在都实现或继承了 AutoCloseable 接口。如果你编写的类表示必须关闭的资源，那么这个类也应该实现 AutoCloseable 接口。 以下是我们的第一个使用 try-with-resources 的示例： 1234567// try-with-resources - the the best way to close resources!static String firstLineOfFile(String path) throws IOException { try (BufferedReader br = new BufferedReader( new FileReader(path))) { return br.readLine(); }} 以下是我们的第二个使用 try-with-resources 的示例： 12345678910// try-with-resources on multiple resources - short and sweetstatic void copy(String src, String dst) throws IOException { try (InputStream in = new FileInputStream(src); OutputStream out = new FileOutputStream(dst)) { byte[] buf = new byte[BUFFER_SIZE]; int n; while ((n = in.read(buf)) &gt;= 0) out.write(buf, 0, n); }} 不仅 try-with-resources 版本比原始版本更精简，更好的可读性，而且它们提供了更好的诊断。 考虑 firstLineOfFile 方法。 如果调用 readLine 和（不可见）close 方法都抛出异常，则后一个异常将被抑制（suppressed），而不是前者。 事实上，为了保留你真正想看到的异常，可能会抑制多个异常。 这些抑制的异常没有被抛弃， 而是打印在堆栈跟踪中，并标注为被抑制了。 你也可以使用 getSuppressed 方法以编程方式访问它们，该方法在 Java 7 中已添加到的 Throwable 中。 可以在 try-with-resources 语句中添加 catch 子句，就像在常规的 try-finally 语句中一样。这允许你处理异常，而不会在另一层嵌套中污染代码。作为一个稍微有些做作的例子，这里有一个版本的 firstLineOfFile 方法，它不会抛出异常，但是如果它不能打开或读取文件，则返回默认值： 123456789// try-with-resources with a catch clausestatic String firstLineOfFile(String path, String defaultVal) { try (BufferedReader br = new BufferedReader( new FileReader(path))) { return br.readLine(); } catch (IOException e) { return defaultVal; }} 结论很明确：在处理必须关闭的资源时，使用 try-with-resources 语句替代 try-finally 语句。 生成的代码更简洁，更清晰，并且生成的异常更有用。 try-with-resources 语句在编写必须关闭资源的代码时会更容易，也不会出错，而使用 try-finally 语句实际上是不可能的。","link":"/2019/05/29/Effective-Java-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-09-%E4%BD%BF%E7%94%A8try-with-resources%E8%AF%AD%E5%8F%A5%E6%9B%BF%E4%BB%A3try-finally%E8%AF%AD%E5%8F%A5/"},{"title":"J2EE基础知识","text":"这世上有三样东西是别人抢不走的：一是吃进胃里的食物，二是藏在心中的梦想，三是读进大脑的书本文引用 https://github.com/Snailclimb/JavaGuide自己进行了添加与修改 Servlet总结在Java Web程序中，Servlet主要负责接收用户请求HttpServletRequest,在doGet(),doPost()**中做相应的处理，并将回应HttpServletResponse反馈给用户。Servlet可以设置初始化参数，供Servlet内部使用。一个Servlet类只会有一个实例，在它初始化时调用init()方法，销毁时调用destroy()方法。Servlet需要在web.xml中配置（MyEclipse中创建Servlet会自动配置），一个Servlet可以设置多个URL访问。Servlet不是线程安全**，因此要谨慎使用类变量。 阐述Servlet和CGI的区别?CGI的不足之处:1，需要为每个请求启动一个操作CGI程序的系统进程。如果请求频繁，这将会带来很大的开销。 2，需要为每个请求加载和运行一个CGI程序，这将带来很大的开销 3，需要重复编写处理网络协议的代码以及编码，这些工作都是非常耗时的。 Servlet的优点:1，只需要启动一个操作系统进程以及加载一个JVM，大大降低了系统的开销 2，如果多个请求需要做同样处理的时候，这时候只需要加载一个类，这也大大降低了开销 3，所有动态加载的类可以实现对网络协议以及请求解码的共享，大大降低了工作量。 4，Servlet能直接和Web服务器交互，而普通的CGI程序不能。Servlet还能在各个程序之间共享数据，使数据库连接池之类的功能很容易实现。 补充：Sun Microsystems公司在1996年发布Servlet技术就是为了和CGI进行竞争，Servlet是一个特殊的Java程序，一个基于Java的Web应用通常包含一个或多个Servlet类。Servlet不能够自行创建并执行，它是在Servlet容器中运行的，容器将用户的请求传递给Servlet程序，并将Servlet的响应回传给用户。通常一个Servlet会关联一个或多个JSP页面。以前CGI经常因为性能开销上的问题被诟病，然而Fast CGI早就已经解决了CGI效率上的问题，所以面试的时候大可不必信口开河的诟病CGI，事实上有很多你熟悉的网站都使用了CGI技术。 参考：《javaweb整合开发王者归来》P7 Servlet接口中有哪些方法及Servlet生命周期探秘Servlet接口定义了5个方法，其中前三个方法与Servlet生命周期相关： void init(ServletConfig config) throws ServletException void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException void destory() java.lang.String getServletInfo() ServletConfig getServletConfig() 生命周期： Web容器加载Servlet并将其实例化后，Servlet生命周期开始，容器运行其init()方法进行Servlet的初始化；请求到达时调用Servlet的service()方法，service()方法会根据需要调用与请求对应的doGet或doPost等方法；当服务器关闭或项目被卸载时服务器会将Servlet实例销毁，此时会调用Servlet的destroy()方法。init方法和destroy方法只会执行一次，service方法客户端每次请求Servlet都会执行。Servlet中有时会用到一些需要初始化与销毁的资源，因此可以把初始化资源的代码放入init方法中，销毁资源的代码放入destroy方法中，这样就不需要每次处理客户端的请求都要初始化与销毁资源。 参考：《javaweb整合开发王者归来》P81 get和post请求的区别 网上也有文章说：get和post请求实际上是没有区别，大家可以自行查询相关文章（参考文章：https://www.cnblogs.com/logsharing/p/8448446.html，知乎对应的问题链接：get和post区别？）！我下面给出的只是一种常见的答案。 ①get请求用来从服务器上获得资源，而post是用来向服务器提交数据； ②get将表单中数据按照name=value的形式，添加到action 所指向的URL 后面，并且两者使用”?”连接，而各个变量之间使用”&amp;”连接；post是将表单中的数据放在HTTP协议的请求头或消息体中，传递到action所指向URL； ③get传输的数据要受到URL长度限制（最大长度是 2048 个字符）；而post可以传输大量的数据，上传文件通常要使用post方式； ④使用get时参数会显示在地址栏上，如果这些数据不是敏感数据，那么可以使用get；对于敏感数据还是应用使用post； ⑤get使用MIME类型application/x-www-form-urlencoded的URL编码（也叫百分号编码）文本的格式传递参数，保证被传送的参数由遵循规范的文本组成，例如一个空格的编码是”%20”。 补充：GET方式提交表单的典型应用是搜索引擎。GET方式就是被设计为查询用的。 还有另外一种回答。推荐大家看一下： https://www.zhihu.com/question/28586791 https://mp.weixin.qq.com/s?__biz=MzI3NzIzMzg3Mw==&amp;mid=100000054&amp;idx=1&amp;sn=71f6c214f3833d9ca20b9f7dcd9d33e4#rd 什么情况下调用doGet()和doPost()Form标签里的method的属性为get时调用doGet()，为post时调用doPost()。 转发(Forward)和重定向(Redirect)的区别转发是服务器行为，重定向是客户端行为。 转发（Forward）通过RequestDispatcher对象的forward（HttpServletRequest request,HttpServletResponse response）方法实现的。RequestDispatcher可以通过HttpServletRequest 的getRequestDispatcher()方法获得。例如下面的代码就是跳转到login_success.jsp页面。 1request.getRequestDispatcher(&quot;login_success.jsp&quot;).forward(request, response); 重定向（Redirect） 是利用服务器返回的状态码来实现的。客户端浏览器请求服务器的时候，服务器会返回一个状态码。服务器通过 HttpServletResponse 的 setStatus(int status) 方法设置状态码。如果服务器返回301或者302，则浏览器会到新的网址重新请求该资源。 从地址栏显示来说 forward是服务器请求资源,服务器直接访问目标地址的URL,把那个URL的响应内容读取过来,然后把这些内容再发给浏览器.浏览器根本不知道服务器发送的内容从哪里来的,所以它的地址栏还是原来的地址.redirect是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址.所以地址栏显示的是新的URL. 从数据共享来说 forward:转发页面和转发到的页面可以共享request里面的数据.redirect:不能共享数据. 从运用地方来说 forward:一般用于用户登陆的时候,根据角色转发到相应的模块.redirect:一般用于用户注销登陆时返回主页面和跳转到其它的网站等 从效率来说 forward:高.redirect:低. 自动刷新(Refresh)自动刷新不仅可以实现一段时间之后自动跳转到另一个页面，还可以实现一段时间之后自动刷新本页面。Servlet中通过HttpServletResponse对象设置Header属性实现自动刷新例如： 1Response.setHeader(&quot;Refresh&quot;,&quot;5;URL=http://localhost:8080/servlet/example.htm&quot;); 其中5为时间，单位为秒。URL指定就是要跳转的页面（如果设置自己的路径，就会实现每过5秒自动刷新本页面一次） Servlet与线程安全Servlet不是线程安全的，多线程并发的读写会导致数据不同步的问题。 解决的办法是尽量不要定义name属性，而是要把name变量分别定义在doGet()和doPost()方法内。虽然使用synchronized(name){}语句块可以解决问题，但是会造成线程的等待，不是很科学的办法。注意：多线程的并发的读写Servlet类属性会导致数据不同步。但是如果只是并发地读取属性而不写入，则不存在数据不同步的问题。因此Servlet里的只读属性最好定义为final类型的。 参考：《javaweb整合开发王者归来》P92 JSP和Servlet是什么关系其实这个问题在上面已经阐述过了，Servlet是一个特殊的Java程序，它运行于服务器的JVM中，能够依靠服务器的支持向浏览器提供显示内容。JSP本质上是Servlet的一种简易形式，JSP会被服务器处理成一个类似于Servlet的Java程序，可以简化页面内容的生成。Servlet和JSP最主要的不同点在于，Servlet的应用逻辑是在Java文件中，并且完全从表示层中的HTML分离开来。而JSP的情况是Java和HTML可以组合成一个扩展名为.jsp的文件。有人说，Servlet就是在Java中写HTML，而JSP就是在HTML中写Java代码，当然这个说法是很片面且不够准确的。JSP侧重于视图，Servlet更侧重于控制逻辑，在MVC架构模式中，JSP适合充当视图（view）而Servlet适合充当控制器（controller）。 JSP工作原理JSP是一种Servlet，但是与HttpServlet的工作方式不太一样。HttpServlet是先由源代码编译为class文件后部署到服务器下，为先编译后部署。而JSP则是先部署后编译。JSP会在客户端第一次请求JSP文件时被编译为HttpJspPage类（接口Servlet的一个子类）。该类会被服务器临时存放在服务器工作目录里面。下面通过实例给大家介绍。工程JspLoginDemo下有一个名为login.jsp的Jsp文件，把工程第一次部署到服务器上后访问这个Jsp文件，我们发现这个目录下多了下图这两个东东。.class文件便是JSP对应的Servlet。编译完毕后再运行class文件来响应客户端请求。以后客户端访问login.jsp的时候，Tomcat将不再重新编译JSP文件，而是直接调用class文件来响应客户端请求。由于JSP只会在客户端第一次请求的时候被编译 ，因此第一次请求JSP时会感觉比较慢，之后就会感觉快很多。如果把服务器保存的class文件删除，服务器也会重新编译JSP。 开发Web程序时经常需要修改JSP。Tomcat能够自动检测到JSP程序的改动。如果检测到JSP源代码发生了改动。Tomcat会在下次客户端请求JSP时重新编译JSP，而不需要重启Tomcat。这种自动检测功能是默认开启的，检测改动会消耗少量的时间，在部署Web应用的时候可以在web.xml中将它关掉。 参考：《javaweb整合开发王者归来》P97 JSP有哪些内置对象、作用分别是什么JSP内置对象 - CSDN博客 JSP有9个内置对象： request：封装客户端的请求，其中包含来自GET或POST请求的参数； response：封装服务器对客户端的响应； pageContext：通过该对象可以获取其他对象； session：封装用户会话的对象； application：封装服务器运行环境的对象； out：输出服务器响应的输出流对象； config：Web应用的配置对象； page：JSP页面本身（相当于Java程序中的this）； exception：封装页面抛出异常的对象。 Request对象的主要方法有哪些 setAttribute(String name,Object)：设置名字为name的request 的参数值 getAttribute(String name)：返回由name指定的属性值 getAttributeNames()：返回request 对象所有属性的名字集合，结果是一个枚举的实例 getCookies()：返回客户端的所有 Cookie 对象，结果是一个Cookie 数组 getCharacterEncoding() ：返回请求中的字符编码方式 = getContentLength() ：返回请求的 Body的长度 getHeader(String name) ：获得HTTP协议定义的文件头信息 getHeaders(String name) ：返回指定名字的request Header 的所有值，结果是一个枚举的实例 getHeaderNames() ：返回所以request Header 的名字，结果是一个枚举的实例 getInputStream() ：返回请求的输入流，用于获得请求中的数据 getMethod() ：获得客户端向服务器端传送数据的方法 getParameter(String name) ：获得客户端传送给服务器端的有 name指定的参数值 getParameterNames() ：获得客户端传送给服务器端的所有参数的名字，结果是一个枚举的实例 getParameterValues(String name)：获得有name指定的参数的所有值 getProtocol()：获取客户端向服务器端传送数据所依据的协议名称 getQueryString() ：获得查询字符串 getRequestURI() ：获取发出请求字符串的客户端地址 getRemoteAddr()：获取客户端的 IP 地址 getRemoteHost() ：获取客户端的名字 getSession([Boolean create]) ：返回和请求相关 Session getServerName() ：获取服务器的名字 getServletPath()：获取客户端所请求的脚本文件的路径 getServerPort()：获取服务器的端口号 removeAttribute(String name)：删除请求中的一个属性 request.getAttribute()和 request.getParameter()有何区别从获取方向来看： getParameter()是获取 POST/GET 传递的参数值； getAttribute()是获取对象容器中的数据值； 从用途来看： getParameter用于客户端重定向时，即点击了链接或提交按扭时传值用，即用于在用表单或url重定向传值时接收数据用。 getAttribute用于服务器端重定向时，即在 sevlet 中使用了 forward 函数,或 struts 中使用了mapping.findForward。 getAttribute 只能收到程序用 setAttribute 传过来的值。 另外，可以用 setAttribute,getAttribute 发送接收对象.而 getParameter 显然只能传字符串。setAttribute 是应用服务器把这个对象放在该页面所对应的一块内存中去，当你的页面服务器重定向到另一个页面时，应用服务器会把这块内存拷贝另一个页面所对应的内存中。这样getAttribute就能取得你所设下的值，当然这种方法可以传对象。session也一样，只是对象在内存中的生命周期不一样而已。getParameter只是应用服务器在分析你送上来的 request页面的文本时，取得你设在表单或 url 重定向时的值。 总结： getParameter 返回的是String,用于读取提交的表单中的值;（获取之后会根据实际需要转换为自己需要的相应类型，比如整型，日期类型啊等等） getAttribute 返回的是Object，需进行转换,可用setAttribute 设置成任意对象，使用很灵活，可随时用 include指令include的行为的区别include指令： JSP可以通过include指令来包含其他文件。被包含的文件可以是JSP文件、HTML文件或文本文件。包含的文件就好像是该JSP文件的一部分，会被同时编译执行。 语法格式如下：&lt;%@ include file=”文件相对 url 地址” %&gt; include动作： jsp:include动作元素用来包含静态和动态的文件。该动作把指定文件插入正在生成的页面。语法格式如下：&lt;jsp:include page=”相对 URL 地址” flush=”true” /&gt; JSP九大内置对象，七大动作，三大指令JSP九大内置对象，七大动作，三大指令总结 讲解JSP中的四种作用域JSP中的四种作用域包括page、request、session和application，具体来说： page代表与一个页面相关的对象和属性。 request代表与Web客户机发出的一个请求相关的对象和属性。一个请求可能跨越多个页面，涉及多个Web组件；需要在页面显示的临时数据可以置于此作用域。 session代表与某个用户与服务器建立的一次会话相关的对象和属性。跟某个用户相关的数据应该放在用户自己的session中。 application代表与整个Web应用程序相关的对象和属性，它实质上是跨越整个Web应用程序，包括多个页面、请求和会话的一个全局作用域。 如何实现JSP或Servlet的单线程模式对于JSP页面，可以通过page指令进行设置。&lt;%@page isThreadSafe=”false”%&gt; 对于Servlet，可以让自定义的Servlet实现SingleThreadModel标识接口。 说明：如果将JSP或Servlet设置成单线程工作模式，会导致每个请求创建一个Servlet实例，这种实践将导致严重的性能问题（服务器的内存压力很大，还会导致频繁的垃圾回收），所以通常情况下并不会这么做。 实现会话跟踪的技术有哪些 使用Cookie 向客户端发送Cookie 123Cookie c =new Cookie(&quot;name&quot;,&quot;value&quot;); //创建Cookie c.setMaxAge(60*60*24); //设置最大时效，此处设置的最大时效为一天response.addCookie(c); //把Cookie放入到HTTP响应中 其实Java的底层实现原理就是 （将设置的cookie 用Set-Cookie的方式返回给浏览器） 从客户端读取Cookie 12345678910111213String name =&quot;name&quot;; Cookie[]cookies =request.getCookies(); if(cookies !=null){ for(int i= 0;i&lt;cookies.length;i++){ Cookie cookie =cookies[i]; if(name.equals(cookis.getName())) //something is here. //you can get the value cookie.getValue(); } } 优点: 数据可以持久保存，不需要服务器资源，简单，基于文本的Key-Value 缺点: 大小受到限制，用户可以禁用Cookie功能，由于保存在本地，有一定的安全风险。 URL 重写 在URL中添加用户会话的信息作为请求的参数，或者将唯一的会话ID添加到URL结尾以标识一个会话。 优点： 在Cookie被禁用的时候依然可以使用 缺点： 必须对网站的URL进行编码，所有页面必须动态生成，不能用预先记录下来的URL进行访问。 3.隐藏的表单域 1&lt;input type=&quot;hidden&quot; name =&quot;session&quot; value=&quot;...&quot;/&gt; 优点： Cookie被禁时可以使用 缺点： 所有页面必须是表单提交之后的结果。 HttpSession 在所有会话跟踪技术中，HttpSession对象是最强大也是功能最多的。当一个用户第一次访问某个网站时会自动创建 HttpSession，每个用户可以访问他自己的HttpSession。可以通过HttpServletRequest对象的getSession方 法获得HttpSession，通过HttpSession的setAttribute方法可以将一个值放在HttpSession中，通过调用 HttpSession对象的getAttribute方法，同时传入属性名就可以获取保存在HttpSession中的对象。与上面三种方式不同的 是，HttpSession放在服务器的内存中，因此不要将过大的对象放在里面，即使目前的Servlet容器可以在内存将满时将HttpSession 中的对象移到其他存储设备中，但是这样势必影响性能。添加到HttpSession中的值可以是任意Java对象，这个对象最好实现了 Serializable接口，这样Servlet容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。 Cookie和Session的的区别Cookie 和 Session都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。 Cookie 一般用来保存用户信息 比如①我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了；②一般的网站都会有保持登录也就是说下次你再访问网站的时候就不需要重新登录了，这是因为用户登录的时候我们可以存放了一个 Token 在 Cookie 中，下次登录的时候只需要根据 Token 值来查找用户即可(为了安全考虑，重新登录一般要将 Token 重写)；③登录一次网站后访问网站其他页面不需要重新登录。Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。 Cookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。 Cookie 存储在客户端中，而Session存储在服务器上，相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。","link":"/2019/05/23/J2EE%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"title":"Java 运行时的内存划分","text":"这世上有三样东西是别人抢不走的：一是吃进胃里的食物，二是藏在心中的梦想，三是读进大脑的书 Java 运行时的内存划分 程序计数器记录当前线程所执行的字节码行号，用于获取下一条执行的字节码。 当多线程运行时，每个线程切换后需要知道上一次所运行的状态、位置。由此也可以看出程序计数器是每个线程私有的。 虚拟机栈虚拟机栈由一个一个的栈帧组成，栈帧是在每一个方法调用时产生的。 每一个栈帧由局部变量区、操作数栈等组成。每创建一个栈帧压栈，当一个方法执行完毕之后则出栈。 如果出现方法递归调用出现死循环的话就会造成栈帧过多，最终会抛出 StackOverflowError。 若线程执行过程中栈帧大小超出虚拟机栈限制，则会抛出 StackOverflowError。 若虚拟机栈允许动态扩展，但在尝试扩展时内存不足，或者在为一个新线程初始化新的虚拟机栈时申请不到足够的内存，则会抛出OutOfMemoryError。 这块内存区域也是线程私有的。 Java 堆Java 堆是整个虚拟机所管理的最大内存区域，所有的对象创建都是在这个区域进行内存分配。 可利用参数 -Xms -Xmx 进行堆内存控制。 这块区域也是垃圾回收器重点管理的区域，由于大多数垃圾回收器都采用分代回收算法，所有堆内存也分为 新生代、老年代，可以方便垃圾的准确回收。 这块内存属于线程共享区域。 方法区(JDK1.7)方法区主要用于存放已经被虚拟机加载的类信息，如常量，静态变量。这块区域也被称为永久代。 可利用参数 -XX:PermSize -XX:MaxPermSize 控制初始化方法区和最大方法区大小。 元数据区(JDK1.8)在 JDK1.8 中已经移除了方法区（永久代），并使用了一个元数据区域进行代替（Metaspace）。 默认情况下元数据区域会根据使用情况动态调整，避免了在 1.7 中由于加载类过多从而出现 java.lang.OutOfMemoryError: PermGen。 但也不能无限扩展，因此可以使用 -XX:MaxMetaspaceSize来控制最大内存。 运行时常量池运行时常量池是方法区的一部分，其中存放了一些符号引用。当 new 一个对象时，会检查这个区域是否有这个符号的引用。 直接内存直接内存又称为 Direct Memory（堆外内存），它并不是由 JVM 虚拟机所管理的一块内存区域。 有使用过 Netty 的朋友应该对这块并内存不陌生，在 Netty 中所有的 IO（nio） 操作都会通过 Native 函数直接分配堆外内存。 它是通过在堆内存中的 DirectByteBuffer 对象操作的堆外内存，避免了堆内存和堆外内存来回复制交换复制，这样的高效操作也称为零拷贝。 既然是内存，那也得是可以被回收的。但由于堆外内存不直接受 JVM 管理，所以常规 GC 操作并不能回收堆外内存。它是借助于老年代产生的 fullGC 顺便进行回收。同时也可以显式调用 System.gc() 方法进行回收（前提是没有使用 -XX:+DisableExplicitGC 参数来禁止该方法）。 值得注意的是：由于堆外内存也是内存，是由操作系统管理。如果应用有使用堆外内存则需要平衡虚拟机的堆内存和堆外内存的使用占比。避免出现堆外内存溢出。 常用参数 通过上图可以直观的查看各个区域的参数设置。 常见的如下： -Xms64m 最小堆内存 64m. -Xmx128m 最大堆内存 128m. -XX:NewSize=30m 新生代初始化大小为30m. -XX:MaxNewSize=40m 新生代最大大小为40m. -Xss=256k 线程栈大小。 -XX:+PrintHeapAtGC 当发生 GC 时打印内存布局。 -XX:+HeapDumpOnOutOfMemoryError 发送内存溢出时 dump 内存。 新生代和老年代的默认比例为 1:2，也就是说新生代占用 1/3的堆内存，而老年代占用 2/3 的堆内存。 可以通过参数 -XX:NewRatio=2 来设置老年代/新生代的比例。","link":"/2019/05/22/Java-%E8%BF%90%E8%A1%8C%E6%97%B6%E7%9A%84%E5%86%85%E5%AD%98%E5%88%92%E5%88%86/"},{"title":"Java 设计模式","text":"设计模式是对大家实际工作中写的各种代码进行高层次抽象的总结 设计模式分为 23 种经典的模式，根据用途我们又可以分为三大类。分别是创建型模式、结构型模式和行为型模式 列举几种设计原则，这几种设计原则将贯通全文： 面向接口编程，而不是面向实现。这个尤为重要，也是优雅的、可扩展的代码的第一步，这就不需要多说了吧 职责单一原则。每个类都应该只有一个单一的功能，并且该功能应该由这个类完全封装起来 对修改关闭，对扩展开放。对修改关闭是说，我们辛辛苦苦加班写出来的代码，该实现的功能和该修复的 bug 都完成了，别人可不能说改就改；对扩展开放就比较好理解了，也就是说在我们写好的代码基础上，很容易实现扩展。 创建型模式 创建型模式的作用就是创建对象，new 一个对象，然后 set 相关属性。但是，在很多场景下，我们需要给客户端提供更加友好的创建对象的方式，尤其是那种我们定义了类，但是需要提供给其他开发者用的时候。 简单工厂模式 和名字一样简单，非常简单，直接上代码吧： 12345678910111213141516public class FoodFactory { public static Food makeFood(String name) { if (name.equals(&quot;noodle&quot;)) { Food noodle = new LanZhouNoodle(); noodle.addSpicy(&quot;more&quot;); return noodle; } else if (name.equals(&quot;chicken&quot;)) { Food chicken = new HuangMenChicken(); chicken.addCondiment(&quot;potato&quot;); return chicken; } else { return null; } }} 其中，LanZhouNoodle 和 HuangMenChicken 都继承自 Food。 简单地说，简单工厂模式通常就是这样，一个工厂类 XxxFactory，里面有一个静态方法，根据我们不同的参数，返回不同的派生自同一个父类（或实现同一接口）的实例对象。 我们强调职责单一原则，一个类只提供一种功能，FoodFactory 的功能就是只要负责生产各种 Food。 工厂模式 简单工厂模式很简单，如果它能满足我们的需要，我觉得就不要折腾了。之所以需要引入工厂模式，是因为我们往往需要使用两个或两个以上的工厂。 1234567891011121314151617181920212223242526272829public interface FoodFactory { Food makeFood(String name);}public class ChineseFoodFactory implements FoodFactory { @Override public Food makeFood(String name) { if (name.equals(&quot;A&quot;)) { return new ChineseFoodA(); } else if (name.equals(&quot;B&quot;)) { return new ChineseFoodB(); } else { return null; } }}public class AmericanFoodFactory implements FoodFactory { @Override public Food makeFood(String name) { if (name.equals(&quot;A&quot;)) { return new AmericanFoodA(); } else if (name.equals(&quot;B&quot;)) { return new AmericanFoodB(); } else { return null; } }} 其中，ChineseFoodA、ChineseFoodB、AmericanFoodA、AmericanFoodB 都派生自 Food。 客户端调用： 123456789public class APP { public static void main(String[] args) { // 先选择一个具体的工厂 FoodFactory factory = new ChineseFoodFactory(); // 由第一步的工厂产生具体的对象，不同的工厂造出不一样的对象 Food food = factory.makeFood(&quot;A&quot;); }} 虽然都是调用 makeFood(“A”) 制作 A 类食物，但是，不同的工厂生产出来的完全不一样。 第一步，我们需要选取合适的工厂，然后第二步基本上和简单工厂一样。 核心在于，我们需要在第一步选好我们需要的工厂。比如，我们有 LogFactory 接口，实现类有 FileLogFactory 和 KafkaLogFactory，分别对应将日志写入文件和写入 Kafka 中，显然，我们客户端第一步就需要决定到底要实例化 FileLogFactory 还是 KafkaLogFactory，这将决定之后的所有的操作。 虽然简单，不过我也把所有的构件都画到一张图上，这样看着比较清晰： 抽象工厂模式 当涉及到产品族的时候，就需要引入抽象工厂模式了。 一个经典的例子是造一台电脑。我们先不引入抽象工厂模式，看看怎么实现。 因为电脑是由许多的构件组成的，我们将 CPU 和主板进行抽象，然后 CPU 由 CPUFactory 生产，主板由 MainBoardFactory 生产，然后，我们再将 CPU 和主板搭配起来组合在一起，如下图： 这个时候的客户端调用是这样的： 12345678910// 得到 Intel 的 CPUCPUFactory cpuFactory = new IntelCPUFactory();CPU cpu = intelCPUFactory.makeCPU();// 得到 AMD 的主板MainBoardFactory mainBoardFactory = new AmdMainBoardFactory();MainBoard mainBoard = mainBoardFactory.make();// 组装 CPU 和主板Computer computer = new Computer(cpu, mainBoard); 单独看 CPU 工厂和主板工厂，它们分别是前面我们说的工厂模式。这种方式也容易扩展，因为要给电脑加硬盘的话，只需要加一个 HardDiskFactory 和相应的实现即可，不需要修改现有的工厂。 但是，这种方式有一个问题，那就是如果** Intel 家产的 CPU 和 AMD 产的主板不能兼容使用**，那么这代码就容易出错，因为客户端并不知道它们不兼容，也就会错误地出现随意组合。 -下面就是我们要说的产品族的概念，它代表了组成某个产品的一系列附件的集合： 当涉及到这种产品族的问题的时候，就需要抽象工厂模式来支持了。我们不再定义 CPU 工厂、主板工厂、硬盘工厂、显示屏工厂等等，我们直接定义电脑工厂，每个电脑工厂负责生产所有的设备，这样能保证肯定不存在兼容问题。 这个时候，对于客户端来说，不再需要单独挑选 CPU厂商、主板厂商、硬盘厂商等，直接选择一家品牌工厂，品牌工厂会负责生产所有的东西，而且能保证肯定是兼容可用的。 12345678910111213public static void main(String[] args) { // 第一步就要选定一个“大厂” ComputerFactory cf = new AmdFactory(); // 从这个大厂造 CPU CPU cpu = cf.makeCPU(); // 从这个大厂造主板 MainBoard board = cf.makeMainBoard(); // 从这个大厂造硬盘 HardDisk hardDisk = cf.makeHardDisk(); // 将同一个厂子出来的 CPU、主板、硬盘组装在一起 Computer result = new Computer(cpu, board, hardDisk);} 当然，抽象工厂的问题也是显而易见的，比如我们要加个显示器，就需要修改所有的工厂，给所有的工厂都加上制造显示器的方法。这有点违反了对修改关闭，对扩展开放这个设计原则。 单例模式 单例模式用得最多，错得最多。 饿汉模式最简单： 12345678910111213public class Singleton { // 首先，将 new Singleton() 堵死 private Singleton() {}; // 创建私有静态实例，意味着这个类第一次使用的时候就会进行创建 private static Singleton instance = new Singleton(); public static Singleton getInstance() { return instance; } // 瞎写一个静态方法。这里想说的是，如果我们只是要调用 Singleton.getDate(...)， // 本来是不想要生成 Singleton 实例的，不过没办法，已经生成了 public static Date getDate(String mode) {return new Date();}} 很多人都能说出饿汉模式的缺点，可是我觉得生产过程中，很少碰到这种情况：你定义了一个单例的类，不需要其实例，可是你却把一个或几个你会用到的静态方法塞到这个类中。 饱汉模式最容易出错： 12345678910111213141516171819public class Singleton { // 首先，也是先堵死 new Singleton() 这条路 private Singleton() {} // 和饿汉模式相比，这边不需要先实例化出来，注意这里的 volatile，它是必须的 private static volatile Singleton instance = null; public static Singleton getInstance() { if (instance == null) { // 加锁 synchronized (Singleton.class) { // 这一次判断也是必须的，不然会有并发问题 if (instance == null) { instance = new Singleton(); } } } return instance; }} 双重检查，指的是两次检查 instance 是否为 null。volatile 在这里是需要的，希望能引起读者的关注。很多人不知道怎么写，直接就在 getInstance() 方法签名上加上 synchronized，这就不多说了，性能太差。 嵌套类最经典，以后大家就用它吧： 1234567891011public class Singleton3 { private Singleton3() {} // 主要是使用了 嵌套类可以访问外部类的静态属性和静态方法 的特性 private static class Holder { private static Singleton3 instance = new Singleton3(); } public static Singleton3 getInstance() { return Holder.instance; }} 注意，很多人都会把这个嵌套类说成是静态内部类，严格地说，内部类和嵌套类是不一样的，它们能访问的外部类权限也是不一样的。 最后，一定有人跳出来说用枚举实现单例，是的没错，枚举类很特殊，它在类加载的时候会初始化里面的所有的实例，而且 JVM 保证了它们不会再被实例化，所以它天生就是单例的。不说了，读者自己看着办吧，不建议使用。 建造者模式 经常碰见的 XxxBuilder 的类，通常都是建造者模式的产物。建造者模式其实有很多的变种，但是对于客户端来说，我们的使用通常都是一个模式的 12Food food = new FoodBuilder().a().b().c().build();Food food = Food.builder().a().b().c().build(); 套路就是先 new 一个 Builder，然后可以链式地调用一堆方法，最后再调用一次 build() 方法，我们需要的对象就有了。 来一个中规中矩的建造者模式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869class User { // 下面是“一堆”的属性 private String name; private String password; private String nickName; private int age; // 构造方法私有化，不然客户端就会直接调用构造方法了 private User(String name, String password, String nickName, int age) { this.name = name; this.password = password; this.nickName = nickName; this.age = age; } // 静态方法，用于生成一个 Builder，这个不一定要有，不过写这个方法是一个很好的习惯， // 有些代码要求别人写 new User.UserBuilder().a()...build() 看上去就没那么好 public static UserBuilder builder() { return new UserBuilder(); } public static class UserBuilder { // 下面是和 User 一模一样的一堆属性 private String name; private String password; private String nickName; private int age; private UserBuilder() { } // 链式调用设置各个属性值，返回 this，即 UserBuilder public UserBuilder name(String name) { this.name = name; return this; } public UserBuilder password(String password) { this.password = password; return this; } public UserBuilder nickName(String nickName) { this.nickName = nickName; return this; } public UserBuilder age(int age) { this.age = age; return this; } // build() 方法负责将 UserBuilder 中设置好的属性“复制”到 User 中。 // 当然，可以在 “复制” 之前做点检验 public User build() { if (name == null || password == null) { throw new RuntimeException(&quot;用户名和密码必填&quot;); } if (age &lt;= 0 || age &gt;= 150) { throw new RuntimeException(&quot;年龄不合法&quot;); } // 还可以做赋予”默认值“的功能 if (nickName == null) { nickName = name; } return new User(name, password, nickName, age); } }} 核心是：先把所有的属性都设置给 Builder，然后 build() 方法的时候，将这些属性复制给实际产生的对象。 看看客户端的调用： 12345678910public class APP { public static void main(String[] args) { User d = User.builder() .name(&quot;foo&quot;) .password(&quot;pAss12345&quot;) .age(25) .build(); }} 说实话，建造者模式的链式写法很吸引人，但是，多写了很多“无用”的 builder 的代码，感觉这个模式没什么用。不过，当属性很多，而且有些必填，有些选填的时候，这个模式会使代码清晰很多。我们可以在 Builder 的构造方法中强制让调用者提供必填字段，还有，在 build() 方法中校验各个参数比在 User 的构造方法中校验，代码要优雅一些。 题外话，强烈建议读者使用 lombok，用了 lombok 以后，上面的一大堆代码会变成如下这样: 12345678@Builderclass User { private String name; private String password; private String nickName; private int age;} 怎么样，省下来的时间是不是又可以干点别的了。 当然，如果你只是想要链式写法，不想要建造者模式，有个很简单的办法，User 的 getter 方法不变，所有的 setter 方法都让其 *return this 就可以了，然后就可以像下面这样调用： 12User user = new User().setName(&quot;&quot;).setPassword(&quot;&quot;).setAge(20); 原型模式 这是我要说的创建型模式的最后一个设计模式了。 原型模式很简单：有一个原型实例，基于这个原型实例产生新的实例，也就是“克隆”了。 Object 类中有一个 clone() 方法，它用于生成一个新的对象，当然，如果我们要调用这个方法，java 要求我们的类必须先实现 Cloneable 接口，此接口没有定义任何方法，但是不这么做的话，在 clone() 的时候，会抛出 CloneNotSupportedException 异常。 12protected native Object clone() throws CloneNotSupportedException; java 的克隆是浅克隆，碰到对象引用的时候，克隆出来的对象和原对象中的引用将指向同一个对象。通常实现深克隆的方法是将对象进行序列化，然后再进行反序列化。 原型模式了解到这里我觉得就够了，各种变着法子说这种代码或那种代码是原型模式，没什么意义。 创建型模式总结 创建型模式总体上比较简单，它们的作用就是为了产生实例对象，算是各种工作的第一步了，因为我们写的是面向对象的代码，所以我们第一步当然是需要创建一个对象了。 简单工厂模式最简单；工厂模式在简单工厂模式的基础上增加了选择工厂的维度，需要第一步选择合适的工厂；抽象工厂模式有产品族的概念，如果各个产品是存在兼容性问题的，就要用抽象工厂模式。单例模式就不说了，为了保证全局使用的是同一对象，一方面是安全性考虑，一方面是为了节省资源；建造者模式专门对付属性很多的那种类，为了让代码更优美；原型模式用得最少，了解和 Object 类中的 clone() 方法相关的知识即可。 结构型模式 前面创建型模式介绍了创建对象的一些设计模式，这节介绍的结构型模式旨在通过改变代码结构来达到解耦的目的，使得我们的代码容易维护和扩展。 代理模式 第一个要介绍的代理模式是最常使用的模式之一了，用一个代理来隐藏具体实现类的实现细节，通常还用于在真实的实现的前后添加一部分逻辑。 既然说是代理，那就要对客户端隐藏真实实现，由代理来负责客户端的所有请求。当然，代理只是个代理，它不会完成实际的业务逻辑，而是一层皮而已，但是对于客户端来说，它必须表现得就是客户端需要的真实实现。 理解代理这个词，这个模式其实就简单了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public interface FoodService { Food makeChicken(); Food makeNoodle();}public class FoodServiceImpl implements FoodService { public Food makeChicken() { Food f = new Chicken() f.setChicken(&quot;1kg&quot;); f.setSpicy(&quot;1g&quot;); f.setSalt(&quot;3g&quot;); return f; } public Food makeNoodle() { Food f = new Noodle(); f.setNoodle(&quot;500g&quot;); f.setSalt(&quot;5g&quot;); return f; }}// 代理要表现得“就像是”真实实现类，所以需要实现 FoodServicepublic class FoodServiceProxy implements FoodService { // 内部一定要有一个真实的实现类，当然也可以通过构造方法注入 private FoodService foodService = new FoodServiceImpl(); public Food makeChicken() { System.out.println(&quot;我们马上要开始制作鸡肉了&quot;); // 如果我们定义这句为核心代码的话，那么，核心代码是真实实现类做的， // 代理只是在核心代码前后做些“无足轻重”的事情 Food food = foodService.makeChicken(); System.out.println(&quot;鸡肉制作完成啦，加点胡椒粉&quot;); // 增强 food.addCondiment(&quot;pepper&quot;); return food; } public Food makeNoodle() { System.out.println(&quot;准备制作拉面~&quot;); Food food = foodService.makeNoodle(); System.out.println(&quot;制作完成啦&quot;) return food; }} 客户端调用，注意，我们要用代理来实例化接口： 1234// 这里用代理类来实例化FoodService foodService = new FoodServiceProxy();foodService.makeChicken(); 我们发现没有，代理模式说白了就是做 “方法包装” 或做 “方法增强”。在面向切面编程中，算了还是不要吹捧这个名词了，在 AOP 中，其实就是动态代理的过程。比如 Spring 中，我们自己不定义代理类，但是 Spring 会帮我们动态来定义代理，然后把我们定义在 @Before、@After、@Around 中的代码逻辑动态添加到代理中。 说到动态代理，又可以展开说 …… Spring 中实现动态代理有两种，一种是如果我们的类定义了接口，如 UserService 接口和 UserServiceImpl 实现，那么采用 JDK 的动态代理，感兴趣的读者可以去看看 java.lang.reflect.Proxy 类的源码；另一种是我们自己没有定义接口的，Spring 会采用 CGLIB 进行动态代理，它是一个 jar 包，性能还不错。 适配器模式 说完代理模式，说适配器模式，是因为它们很相似，这里可以做个比较。 适配器模式做的就是，有一个接口需要实现，但是我们现成的对象都不满足，需要加一层适配器来进行适配 适配器模式总体来说分三种：默认适配器模式、对象适配器模式、类适配器模式。先不急着分清楚这几个，先看看例子再说。 默认适配器模式 首先，我们先看看最简单的适配器模式**默认适配器模式(Default Adapter)**是怎么样的。 我们用 Appache commons-io 包中的 FileAlterationListener 做例子，此接口定义了很多的方法，用于对文件或文件夹进行监控，一旦发生了对应的操作，就会触发相应的方法。 1234567891011public interface FileAlterationListener { void onStart(final FileAlterationObserver observer); void onDirectoryCreate(final File directory); void onDirectoryChange(final File directory); void onDirectoryDelete(final File directory); void onFileCreate(final File file); void onFileChange(final File file); void onFileDelete(final File file); void onStop(final FileAlterationObserver observer);} 此接口的一大问题是抽象方法太多了，如果我们要用这个接口，意味着我们要实现每一个抽象方法，如果我们只是想要监控文件夹中的文件创建和文件删除事件，可是我们还是不得不实现所有的方法，很明显，这不是我们想要的。 所以，我们需要下面的一个适配器，它用于实现上面的接口，但是所有的方法都是空方法，这样，我们就可以转而定义自己的类来继承下面这个类即可。 123456789101112131415161718192021222324252627public class FileAlterationListenerAdaptor implements FileAlterationListener { public void onStart(final FileAlterationObserver observer) { } public void onDirectoryCreate(final File directory) { } public void onDirectoryChange(final File directory) { } public void onDirectoryDelete(final File directory) { } public void onFileCreate(final File file) { } public void onFileChange(final File file) { } public void onFileDelete(final File file) { } public void onStop(final FileAlterationObserver observer) { }} 比如我们可以定义以下类，我们仅仅需要实现我们想实现的方法就可以了： 1234567891011public class FileMonitor extends FileAlterationListenerAdaptor { public void onFileCreate(final File file) { // 文件创建 doSomething(); } public void onFileDelete(final File file) { // 文件删除 doSomething(); }} 当然，上面说的只是适配器模式的其中一种，也是最简单的一种，无需多言。下面，再介绍“正统的”适配器模式。 对象适配器模式 来看一个《Head First 设计模式》中的一个例子，我稍微修改了一下，看看怎么将鸡适配成鸭，这样鸡也能当鸭来用。因为，现在鸭这个接口，我们没有合适的实现类可以用，所以需要适配器。 12345678910111213141516171819public interface Duck { public void quack(); // 鸭的呱呱叫 public void fly(); // 飞}public interface Cock { public void gobble(); // 鸡的咕咕叫 public void fly(); // 飞}public class WildCock implements Cock { public void gobble() { System.out.println(&quot;咕咕叫&quot;); } public void fly() { System.out.println(&quot;鸡也会飞哦&quot;); }} 鸭接口有 fly() 和 quare() 两个方法，鸡 Cock 如果要冒充鸭，fly() 方法是现成的，但是鸡不会鸭的呱呱叫，没有 quack() 方法。这个时候就需要适配了： 12345678910111213141516171819202122// 毫无疑问，首先，这个适配器肯定需要 implements Duck，这样才能当做鸭来用public class CockAdapter implements Duck { Cock cock; // 构造方法中需要一个鸡的实例，此类就是将这只鸡适配成鸭来用 public CockAdapter(Cock cock) { this.cock = cock; } // 实现鸭的呱呱叫方法 @Override public void quack() { // 内部其实是一只鸡的咕咕叫 cock.gobble(); } @Override public void fly() { cock.fly(); }} 客户端调用很简单了： 12345678public static void main(String[] args) { // 有一只野鸡 Cock wildCock = new WildCock(); // 成功将野鸡适配成鸭 Duck duck = new CockAdapter(wildCock); ...} 到这里，大家也就知道了适配器模式是怎么回事了。无非是我们需要一只鸭，但是我们只有一只鸡，这个时候就需要定义一个适配器，由这个适配器来充当鸭，但是适配器里面的方法还是由鸡来实现的。 我们用一个图来简单说明下： 上图应该还是很容易理解的，我就不做更多的解释了。下面，我们看看类适配模式怎么样的。 类适配器模式 废话少说，直接上图： 看到这个图，大家应该很容易理解的吧，通过继承的方法，适配器自动获得了所需要的大部分方法。这个时候，客户端使用更加简单，直接 Target t = new SomeAdapter(); 就可以了。 适配器模式总结 类适配和对象适配的异同 一个采用继承，一个采用组合； 类适配属于静态实现，对象适配属于组合的动态实现，对象适配需要多实例化一个对象。 总体来说，对象适配用得比较多。 适配器模式和代理模式的异同 比较这两种模式，其实是比较对象适配器模式和代理模式，在代码结构上，它们很相似，都需要一个具体的实现类的实例。但是它们的目的不一样，代理模式做的是增强原方法的活；适配器做的是适配的活，为的是提供“把鸡包装成鸭，然后当做鸭来使用”，而鸡和鸭它们之间原本没有继承关系。 桥梁模式 理解桥梁模式，其实就是理解代码抽象和解耦。 我们首先需要一个桥梁，它是一个接口，定义提供的接口方法。 123public interface DrawAPI { public void draw(int radius, int x, int y);} 然后是一系列实现类： 123456789101112131415161718public class RedPen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class GreenPen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class BluePen implements DrawAPI { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }} 定义一个抽象类，此类的实现类都需要使用 DrawAPI： 12345678public abstract class Shape { protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI){ this.drawAPI = drawAPI; } public abstract void draw(); } 定义抽象类的子类：12345678910111213141516171819202122232425262728// 圆形public class Circle extends Shape { private int radius; public Circle(int radius, DrawAPI drawAPI) { super(drawAPI); this.radius = radius; } public void draw() { drawAPI.draw(radius, 0, 0); }}// 长方形public class Rectangle extends Shape { private int x; private int y; public Rectangle(int x, int y, DrawAPI drawAPI) { super(drawAPI); this.x = x; this.y = y; } public void draw() { drawAPI.draw(0, x, y); }} 最后，我们来看客户端演示： 12345678public static void main(String[] args) { Shape greenCircle = new Circle(10, new GreenPen()); Shape redRectangle = new Rectangle(4, 8, new RedPen()); greenCircle.draw(); redRectangle.draw();} 可能大家看上面一步步还不是特别清晰，我把所有的东西整合到一张图上： 这回大家应该就知道抽象在哪里，怎么解耦了吧。桥梁模式的优点也是显而易见的，就是非常容易进行扩展。 装饰模式 要把装饰模式说清楚明白，不是件容易的事情。也许读者知道 Java IO 中的几个类是典型的装饰模式的应用，但是读者不一定清楚其中的关系，也许看完就忘了，希望看完这节后，读者可以对其有更深的感悟。 首先，我们先看一个简单的图，看这个图的时候，了解下层次结构就可以了： 我们来说说装饰模式的出发点，从图中可以看到，接口 Component 其实已经有了 ConcreteComponentA 和 ConcreteComponentB 两个实现类了，但是，如果我们要增强这两个实现类的话，我们就可以采用装饰模式，用具体的装饰器来装饰实现类，以达到增强的目的。 从名字来简单解释下装饰器。既然说是装饰，那么往往就是添加小功能这种，而且，我们要满足可以添加多个小功能。最简单的，代理模式就可以实现功能的增强，但是代理不容易实现多个功能的增强，当然你可以说用代理包装代理的方式，但是那样的话代码就复杂了。 首先明白一些简单的概念，从图中我们看到，所有的具体装饰者们 ConcreteDecorator 都可以作为 Component 来使用，因为它们都实现了 Component 中的所有接口。它们和 Component 实现类 ConcreteComponent 的区别是，它们只是装饰者，起装饰作用，也就是即使它们看上去牛逼轰轰，但是它们都只是在具体的实现中加了层皮来装饰而已。 注意这段话中混杂在各个名词中的 Component 和 Decorator，别搞混了。 下面来看看一个例子，先把装饰模式弄清楚，然后再介绍下 java io 中的装饰模式的应用。 最近大街上流行起来了“快乐柠檬”，我们把快乐柠檬的饮料分为三类：红茶、绿茶、咖啡，在这三大类的基础上，又增加了许多的口味，什么金桔柠檬红茶、金桔柠檬珍珠绿茶、芒果红茶、芒果绿茶、芒果珍珠红茶、烤珍珠红茶、烤珍珠芒果绿茶、椰香胚芽咖啡、焦糖可可咖啡等等，每家店都有很长的菜单，但是仔细看下，其实原料也没几样，但是可以搭配出很多组合，如果顾客需要，很多没出现在菜单中的饮料他们也是可以做的。 在这个例子中，红茶、绿茶、咖啡是最基础的饮料，其他的像金桔柠檬、芒果、珍珠、椰果、焦糖等都属于装饰用的。当然，在开发中，我们确实可以像门店一样，开发这些类：LemonBlackTea、LemonGreenTea、MangoBlackTea、MangoLemonGreenTea……但是，很快我们就发现，这样子干肯定是不行的，这会导致我们需要组合出所有的可能，而且如果客人需要在红茶中加双份柠檬怎么办？三份柠檬怎么办？万一有个变态要四份柠檬，所以这种做法是给自己找加班的。 不说废话了，上代码。 首先，定义饮料抽象基类： 123456public abstract class Beverage { // 返回描述 public abstract String getDescription(); // 返回价格 public abstract double cost();} 然后是三个基础饮料实现类，红茶、绿茶和咖啡： 1234567891011121314151617public class BlackTea extends Beverage { public String getDescription() { return &quot;红茶&quot;; } public double cost() { return 10; }}public class GreenTea extends Beverage { public String getDescription() { return &quot;绿茶&quot;; } public double cost() { return 11; }}...// 咖啡省略 定义调料，也就是装饰者的基类，此类必须继承自 Beverage： 12345// 调料public abstract class Condiment extends Beverage {} 然后我们来定义柠檬、芒果等具体的调料，它们属于装饰者，毫无疑问，这些调料肯定都需要继承 Condiment 类： 1234567891011121314151617181920212223242526272829public class Lemon extends Condiment { private Beverage bevarage; // 这里很关键，需要传入具体的饮料，如需要传入没有被装饰的红茶或绿茶， // 当然也可以传入已经装饰好的芒果绿茶，这样可以做芒果柠檬绿茶 public Lemon(Beverage bevarage) { this.bevarage = bevarage; } public String getDescription() { // 装饰 return bevarage.getDescription() + &quot;, 加柠檬&quot;; } public double cost() { // 装饰 return beverage.cost() + 2; // 加柠檬需要 2 元 }}public class Mango extends Condiment { private Beverage bevarage; public Mango(Beverage bevarage) { this.bevarage = bevarage; } public String getDescription() { return bevarage.getDescription() + &quot;, 加芒果&quot;; } public double cost() { return beverage.cost() + 3; // 加芒果需要 3 元 }}...// 给每一种调料都加一个类 看客户端调用 12345678910public static void main(String[] args) { // 首先，我们需要一个基础饮料，红茶、绿茶或咖啡 Beverage beverage = new GreenTea(); // 开始装饰 beverage = new Lemon(beverage); // 先加一份柠檬 beverage = new Mongo(beverage); // 再加一份芒果 System.out.println(beverage.getDescription() + &quot; 价格：￥&quot; + beverage.cost()); //&quot;绿茶, 加柠檬, 加芒果 价格：￥16&quot;} 如果我们需要芒果珍珠双份柠檬红茶： 1Beverage beverage = new Mongo(new Pearl(new Lemon(new Lemon(new BlackTea())))); 是不是很变态？ 看看下图可能会清晰一些： 到这里，大家应该已经清楚装饰模式了吧。 下面，我们再来说说 java IO 中的装饰模式。看下图 InputStream 派生出来的部分类 我们知道 InputStream 代表了输入流，具体的输入来源可以是文件（FileInputStream）、管道（PipedInputStream）、数组（ByteArrayInputStream）等，这些就像前面奶茶的例子中的红茶、绿茶，属于基础输入流。 FilterInputStream 承接了装饰模式的关键节点，其实现类是一系列装饰器，比如 BufferedInputStream 代表用缓冲来装饰，也就使得输入流具有了缓冲的功能，LineNumberInputStream 代表用行号来装饰，在操作的时候就可以取得行号了，DataInputStream 的装饰，使得我们可以从输入流转换为 java 中的基本类型值。 当然，在 java IO 中，如果我们使用装饰器的话，就不太适合面向接口编程了，如： 1InputStream inputStream = new LineNumberInputStream(new BufferedInputStream(new FileInputStream(&quot;&quot;))); 这样的结果是，InputStream 还是不具有读取行号的功能，因为读取行号的方法定义在 LineNumberInputStream 类中。 我们应该像下面这样使用： 1234DataInputStream is = new DataInputStream( new BufferedInputStream( new FileInputStream(&quot;&quot;))); 所以说嘛，要找到纯的严格符合设计模式的代码还是比较难的 门面模式 门面模式（也叫外观模式，Facade Pattern）在许多源码中有使用，比如 slf4j 就可以理解为是门面模式的应用。这是一个简单的设计模式，我们直接上代码再说吧。 首先，我们定义一个接口： 1234public interface Shape { void draw();} 定义几个实现类：12345678910111213141516public class Circle implements Shape { @Override public void draw() { System.out.println(&quot;Circle::draw()&quot;); }}public class Rectangle implements Shape { @Override public void draw() { System.out.println(&quot;Rectangle::draw()&quot;); }} 客户端调用： 12345678910public static void main(String[] args) { // 画一个圆形 Shape circle = new Circle(); circle.draw(); // 画一个长方形 Shape rectangle = new Rectangle(); rectangle.draw();} 以上是我们常写的代码，我们需要画圆就要先实例化圆，画长方形就需要先实例化一个长方形，然后再调用相应的 draw() 方法。 下面，我们看看怎么用门面模式来让客户端调用更加友好一些。 我们先定义一个门面： 1234567891011121314151617181920212223242526public class ShapeMaker { private Shape circle; private Shape rectangle; private Shape square; public ShapeMaker() { circle = new Circle(); rectangle = new Rectangle(); square = new Square(); } /** * 下面定义一堆方法，具体应该调用什么方法，由这个门面来决定 */ public void drawCircle(){ circle.draw(); } public void drawRectangle(){ rectangle.draw(); } public void drawSquare(){ square.draw(); }} 看看现在客户端怎么调用： 123456789public static void main(String[] args) { ShapeMaker shapeMaker = new ShapeMaker(); // 客户端调用现在更加清晰了 shapeMaker.drawCircle(); shapeMaker.drawRectangle(); shapeMaker.drawSquare(); } 门面模式的优点显而易见，客户端不再需要关注实例化时应该使用哪个实现类，直接调用门面提供的方法就可以了，因为门面类提供的方法的方法名对于客户端来说已经很友好了。 组合模式 组合模式用于表示具有层次结构的数据，使得我们对单个对象和组合对象的访问具有一致性。 直接看一个例子吧，每个员工都有姓名、部门、薪水这些属性，同时还有下属员工集合（虽然可能集合为空），而下属员工和自己的结构是一样的，也有姓名、部门这些属性，同时也有他们的下属员工集合。 1234567891011121314151617181920212223242526272829public class Employee { private String name; private String dept; private int salary; private List&lt;Employee&gt; subordinates; // 下属 public Employee(String name,String dept, int sal) { this.name = name; this.dept = dept; this.salary = sal; subordinates = new ArrayList&lt;Employee&gt;(); } public void add(Employee e) { subordinates.add(e); } public void remove(Employee e) { subordinates.remove(e); } public List&lt;Employee&gt; getSubordinates(){ return subordinates; } public String toString(){ return (&quot;Employee :[ Name : &quot; + name + &quot;, dept : &quot; + dept + &quot;, salary :&quot; + salary+&quot; ]&quot;); } } 通常，这种类需要定义 add(node)、remove(node)、getChildren() 这些方法。 这说的其实就是组合模式，这种简单的模式我就不做过多介绍了，相信各位读者也不喜欢看我写废话。 享元模式 英文是 Flyweight Pattern，不知道是谁最先翻译的这个词，感觉这翻译真的不好理解，我们试着强行关联起来吧。Flyweight 是轻量级的意思，享元分开来说就是 共享 元器件，也就是复用已经生成的对象，这种做法当然也就是轻量级的了。 复用对象最简单的方式是，用一个 HashMap 来存放每次新生成的对象。每次需要一个对象的时候，先到 HashMap 中看看有没有，如果没有，再生成新的对象，然后将这个对象放入 HashMap 中。 这种简单的代码我就不演示了。 结构型模式总结 前面，我们说了代理模式、适配器模式、桥梁模式、装饰模式、门面模式、组合模式和享元模式。读者是否可以分别把这几个模式说清楚了呢？在说到这些模式的时候，心中是否有一个清晰的图或处理流程在脑海里呢？ 代理模式是做方法增强的，适配器模式是把鸡包装成鸭这种用来适配接口的，桥梁模式做到了很好的解耦，装饰模式从名字上就看得出来，适合于装饰类或者说是增强类的场景，门面模式的优点是客户端不需要关心实例化过程，只要调用需要的方法即可，组合模式用于描述具有层次结构的数据，享元模式是为了在特定的场景中缓存已经创建的对象，用于提高性能。 行为型模式 行为型模式关注的是各个类之间的相互作用，将职责划分清楚，使得我们的代码更加地清晰。 策略模式 策略模式太常用了，所以把它放到最前面进行介绍。它比较简单，我就不废话，直接用代码说事吧。 下面设计的场景是，我们需要画一个图形，可选的策略就是用红色笔来画，还是绿色笔来画，或者蓝色笔来画。 首先，先定义一个策略接口： 123public interface Strategy { public void draw(int radius, int x, int y);} 然后我们定义具体的几个策略： 12345678910111213141516171819public class RedPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用红色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class GreenPen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用绿色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }}public class BluePen implements Strategy { @Override public void draw(int radius, int x, int y) { System.out.println(&quot;用蓝色笔画图，radius:&quot; + radius + &quot;, x:&quot; + x + &quot;, y:&quot; + y); }} 使用策略的类： 1234567891011public class Context { private Strategy strategy; public Context(Strategy strategy){ this.strategy = strategy; } public int executeDraw(int radius, int x, int y){ return strategy.draw(radius, x, y); }} 客户端演示： 12345public static void main(String[] args) { Context context = new Context(new BluePen()); // 使用绿色笔来画 context.executeDraw(10, 0, 0);} 放到一张图上，让大家看得清晰些： 这个时候，大家有没有联想到结构型模式中的桥梁模式，它们其实非常相似，我把桥梁模式的图拿过来大家对比下： 要我说的话，它们非常相似，桥梁模式在左侧加了一层抽象而已。桥梁模式的耦合更低，结构更复杂一些。 观察者模式 观察者模式对于我们来说，真是再简单不过了。无外乎两个操作，观察者订阅自己关心的主题和主题有数据变化后通知观察者们。 首先，需要定义主题，每个主题需要持有观察者列表的引用，用于在数据变更的时候通知各个观察者： 123456789101112131415161718192021222324252627public class Subject { private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); private int state; public int getState() { return state; } public void setState(int state) { this.state = state; // 数据已变更，通知观察者们 notifyAllObservers(); } public void attach(Observer observer){ observers.add(observer); } // 通知观察者们 public void notifyAllObservers(){ for (Observer observer : observers) { observer.update(); } } } 定义观察者接口： 12345public abstract class Observer { protected Subject subject; public abstract void update();} 其实如果只有一个观察者类的话，接口都不用定义了，不过，通常场景下，既然用到了观察者模式，我们就是希望一个事件出来了，会有多个不同的类需要处理相应的信息。比如，订单修改成功事件，我们希望发短信的类得到通知、发邮件的类得到通知、处理物流信息的类得到通知等。 我们来定义具体的几个观察者类： 12345678910111213141516171819202122232425262728293031public class BinaryObserver extends Observer { // 在构造方法中进行订阅主题 public BinaryObserver(Subject subject) { this.subject = subject; // 通常在构造方法中将 this 发布出去的操作一定要小心 this.subject.attach(this); } // 该方法由主题类在数据变更的时候进行调用 @Override public void update() { String result = Integer.toBinaryString(subject.getState()); System.out.println(&quot;订阅的数据发生变化，新的数据处理为二进制值为：&quot; + result); }}public class HexaObserver extends Observer { public HexaObserver(Subject subject) { this.subject = subject; this.subject.attach(this); } @Override public void update() { String result = Integer.toHexString(subject.getState()).toUpperCase(); System.out.println(&quot;订阅的数据发生变化，新的数据处理为十六进制值为：&quot; + result); }} 客户端使用也非常简单： 1234567891011public static void main(String[] args) { // 先定义一个主题 Subject subject1 = new Subject(); // 定义观察者 new BinaryObserver(subject1); new HexaObserver(subject1); // 模拟数据变更，这个时候，观察者们的 update 方法将会被调用 subject.setState(11);} output: 123订阅的数据发生变化，新的数据处理为二进制值为：1011订阅的数据发生变化，新的数据处理为十六进制值为：B 当然，jdk 也提供了相似的支持，具体的大家可以参考 java.util.Observable 和 java.util.Observer 这两个类。 实际生产过程中，观察者模式往往用消息中间件来实现，如果要实现单机观察者模式，笔者建议读者使用 Guava 中的 EventBus，它有同步实现也有异步实现，本文主要介绍设计模式，就不展开说了。 责任链模式 责任链通常需要先建立一个单向链表，然后调用方只需要调用头部节点就可以了，后面会自动流转下去。比如流程审批就是一个很好的例子，只要终端用户提交申请，根据申请的内容信息，自动建立一条责任链，然后就可以开始流转了 有这么一个场景，用户参加一个活动可以领取奖品，但是活动需要进行很多的规则校验然后才能放行，比如首先需要校验用户是否是新用户、今日参与人数是否有限额、全场参与人数是否有限额等等。设定的规则都通过后，才能让用户领走奖品。 如果产品给你这个需求的话，我想大部分人一开始肯定想的就是，用一个 List 来存放所有的规则，然后 foreach 执行一下每个规则就好了。不过，读者也先别急，看看责任链模式和我们说的这个有什么不一样？ 首先，我们要定义流程上节点的基类： 123456789101112131415public abstract class RuleHandler { // 后继节点 protected RuleHandler successor; public abstract void apply(Context context); public void setSuccessor(RuleHandler successor) { this.successor = successor; } public RuleHandler getSuccessor() { return successor; }} 接下来，我们需要定义具体的每个节点了。 校验用户是否是新用户： 123456789101112131415public class NewUserRuleHandler extends RuleHandler { public void apply(Context context) { if (context.isNewUser()) { // 如果有后继节点的话，传递下去 if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(&quot;该活动仅限新用户参与&quot;); } }} 校验用户所在地区是否可以参与： 12345678910111213public class LocationRuleHandler extends RuleHandler { public void apply(Context context) { boolean allowed = activityService.isSupportedLocation(context.getLocation); if (allowed) { if (this.getSuccessor() != null) { this.getSuccessor().apply(context); } } else { throw new RuntimeException(&quot;非常抱歉，您所在的地区无法参与本次活动&quot;); } }} 校验奖品是否已领完： 12345678910111213public class LimitRuleHandler extends RuleHandler { public void apply(Context context) { int remainedTimes = activityService.queryRemainedTimes(context); // 查询剩余奖品 if (remainedTimes &gt; 0) { if (this.getSuccessor() != null) { this.getSuccessor().apply(userInfo); } } else { throw new RuntimeException(&quot;您来得太晚了，奖品被领完了&quot;); } }} 客户端：12345678910public static void main(String[] args) { RuleHandler newUserHandler = new NewUserRuleHandler(); RuleHandler locationHandler = new LocationRuleHandler(); RuleHandler limitHandler = new LimitRuleHandler(); // 假设本次活动仅校验地区和奖品数量，不校验新老用户 locationHandler.setSuccessor(limitHandler); locationHandler.apply(context);} 代码其实很简单，就是先定义好一个链表，然后在通过任意一节点后，如果此节点有后继节点，那么传递下去。 至于它和我们前面说的用一个 List 存放需要执行的规则的做法有什么异同，留给读者自己琢磨吧。 模板方法模式 在含有继承结构的代码中，模板方法模式是非常常用的，这也是在开源代码中大量被使用的。 通常会有一个抽象类： 12345678910111213141516public abstract class AbstractTemplate { // 这就是模板方法 public void templateMethod(){ init(); apply(); // 这个是重点 end(); // 可以作为钩子方法 } protected void init() { System.out.println(&quot;init 抽象层已经实现，子类也可以选择覆写&quot;); } // 留给子类实现 protected abstract void apply(); protected void end() { }} 模板方法中调用了 3 个方法，其中 apply() 是抽象方法，子类必须实现它，其实模板方法中有几个抽象方法完全是自由的，我们也可以将三个方法都设置为抽象方法，让子类来实现。也就是说，模板方法只负责定义第一步应该要做什么，第二步应该做什么，第三步应该做什么，至于怎么做，由子类来实现。 我们写一个实现类： 123456789public class ConcreteTemplate extends AbstractTemplate { public void apply() { System.out.println(&quot;子类实现抽象方法 apply&quot;); } public void end() { System.out.println(&quot;我们可以把 method3 当做钩子方法来使用，需要的时候覆写就可以了&quot;); }} 客户端调用演示： 123456public static void main(String[] args) { AbstractTemplate t = new ConcreteTemplate(); // 调用模板方法 t.templateMethod();} 代码其实很简单，基本上看到就懂了，关键是要学会用到自己的代码中。 状态模式 废话我就不说了，我们说一个简单的例子。商品库存中心有个最基本的需求是减库存和补库存，我们看看怎么用状态模式来写。 核心在于，我们的关注点不再是 Context 是该进行哪种操作，而是关注在这个 Context 会有哪些操作。 定义状态接口： 1234public interface State { public void doAction(Context context);} 定义减库存的状态： 1234567891011121314public class DeductState implements State { public void doAction(Context context) { System.out.println(&quot;商品卖出，准备减库存&quot;); context.setState(this); //... 执行减库存的具体操作 } public String toString(){ return &quot;Deduct State&quot;; }} 定义补库存状态： 123456789101112public class RevertState implements State { public void doAction(Context context) { System.out.println(&quot;给此商品补库存&quot;); context.setState(this); //... 执行加库存的具体操作 } public String toString() { return &quot;Revert State&quot;; }} 前面用到了 context.setState(this)，我们来看看怎么定义 Context 类： 123456789101112131415public class Context { private State state; private String name; public Context(String name) { this.name = name; } public void setState(State state) { this.state = state; } public void getState() { return this.state; }} 我们来看下客户端调用，大家就一清二楚了： 12345678910111213141516public static void main(String[] args) { // 我们需要操作的是 iPhone X Context context = new Context(&quot;iPhone X&quot;); // 看看怎么进行补库存操作 State revertState = new RevertState(); revertState.doAction(context); // 同样的，减库存操作也非常简单 State deductState = new DeductState(); deductState.doAction(context); // 如果需要我们可以获取当前的状态 // context.getState().toString();} 读者可能会发现，在上面这个例子中，如果我们不关心当前 context 处于什么状态，那么 Context 就可以不用维护 state 属性了，那样代码会简单很多。 不过，商品库存这个例子毕竟只是个例，我们还有很多实例是需要知道当前 context 处于什么状态的。 行为型模式总结 行为型模式部分介绍了策略模式、观察者模式、责任链模式、模板方法模式和状态模式，其实，经典的行为型模式还包括备忘录模式、命令模式等，但是它们的使用场景比较有限，而且本文篇幅也挺大了，我就不进行介绍了。 总结 学习设计模式的目的是为了让我们的代码更加的优雅、易维护、易扩展。这次整理这篇文章，让我重新审视了一下各个设计模式，对我自己而言收获还是挺大的。我想，文章的最大收益者一般都是作者本人，为了写一篇文章，需要巩固自己的知识，需要寻找各种资料，而且，自己写过的才最容易记住，也算是我给读者的建议吧。","link":"/2019/05/20/Java-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"JUC集合 ConcurrentHashMap详解","text":"JUC集合: ConcurrentHashMap详解 JDK1.7之前的ConcurrentHashMap使用分段锁机制实现，JDK1.8则使用数组+链表+红黑树数据结构和CAS原子操作实现ConcurrentHashMap；本文将分别介绍这两种方式的实现方案及其区别。@pdai JUC集合: ConcurrentHashMap详解 带着BAT大厂的面试问题去理解 为什么HashTable慢 ConcurrentHashMap - JDK 1.7 数据结构 初始化 put 过程分析 初始化槽: ensureSegment 获取写入锁: scanAndLockForPut 扩容: rehash get 过程分析 并发问题分析 ConcurrentHashMap - JDK 1.8 数据结构 初始化 put 过程分析 初始化数组: initTable 计数 addCount() 链表转红黑树: treeifyBin 扩容: tryPresize 数据迁移: transfer get 过程分析 对比总结 参考文章 ¶ 带着BAT大厂的面试问题去理解TIP 请带着这些问题继续后文，会很大程度上帮助你更好的理解相关知识点。@pdai 为什么HashTable慢? 它的并发度是什么? 那么ConcurrentHashMap并发度是什么? ConcurrentHashMap在JDK1.7和JDK1.8中实现有什么差别? JDK1.8解決了JDK1.7中什么问题 ConcurrentHashMap JDK1.7实现的原理是什么? 分段锁机制 ConcurrentHashMap JDK1.8实现的原理是什么? 数组+链表+红黑树，CAS ConcurrentHashMap JDK1.7中Segment数(concurrencyLevel)默认值是多少? 为何一旦初始化就不可再扩容? ConcurrentHashMap JDK1.7说说其put的机制? ConcurrentHashMap JDK1.7是如何扩容的? rehash(注：segment 数组不能扩容，扩容是 segment 数组某个位置内部的数组 HashEntry&lt;K,V&gt;[] 进行扩容) ConcurrentHashMap JDK1.8是如何扩容的? tryPresize ConcurrentHashMap JDK1.8链表转红黑树的时机是什么? 临界值为什么是8? ConcurrentHashMap JDK1.8是如何进行数据迁移的? transfer ¶ 为什么HashTable慢Hashtable之所以效率低下主要是因为其实现使用了synchronized关键字对put等操作进行加锁，而synchronized关键字加锁是对整个对象进行加锁，也就是说在进行put等修改Hash表的操作时，锁住了整个Hash表，从而使得其表现的效率低下。 ¶ ConcurrentHashMap - JDK 1.7在JDK1.5~1.7版本，Java使用了分段锁机制实现ConcurrentHashMap. 简而言之，ConcurrentHashMap在对象中保存了一个Segment数组，即将整个Hash表划分为多个分段；而每个Segment元素，即每个分段则类似于一个Hashtable；这样，在执行put操作时首先根据hash算法定位到元素属于哪个Segment，然后对该Segment加锁即可。因此，ConcurrentHashMap在多线程并发编程中可是实现多线程put操作。接下来分析JDK1.7版本中ConcurrentHashMap的实现原理。 ¶ 数据结构整个 ConcurrentHashMap 由一个个 Segment 组成，Segment 代表”部分“或”一段“的意思，所以很多地方都会将其描述为分段锁。注意，行文中，我很多地方用了“槽”来代表一个 segment。 简单理解就是，ConcurrentHashMap 是一个 Segment 数组，Segment 通过继承 ReentrantLock 来进行加锁，所以每次需要加锁的操作锁住的是一个 segment，这样只要保证每个 Segment 是线程安全的，也就实现了全局的线程安全。 concurrencyLevel: 并行级别、并发数、Segment 数，怎么翻译不重要，理解它。默认是 16，也就是说 ConcurrentHashMap 有 16 个 Segments，所以理论上，这个时候，最多可以同时支持 16 个线程并发写，只要它们的操作分别分布在不同的 Segment 上。这个值可以在初始化的时候设置为其他值，但是一旦初始化以后，它是不可以扩容的。 再具体到每个 Segment 内部，其实每个 Segment 很像之前介绍的 HashMap，不过它要保证线程安全，所以处理起来要麻烦些。 ¶ 初始化 initialCapacity: 初始容量，这个值指的是整个 ConcurrentHashMap 的初始容量，实际操作的时候需要平均分给每个 Segment。 loadFactor: 负载因子，之前我们说了，Segment 数组不可以扩容，所以这个负载因子是给每个 Segment 内部使用的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; // 计算并行级别 ssize，因为要保持并行级别是 2 的 n 次方 while (ssize &lt; concurrencyLevel) { ++sshift; ssize &lt;&lt;= 1; } // 我们这里先不要那么烧脑，用默认值，concurrencyLevel 为 16，sshift 为 4 // 那么计算出 segmentShift 为 28，segmentMask 为 15，后面会用到这两个值 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // initialCapacity 是设置整个 map 初始的大小， // 这里根据 initialCapacity 计算 Segment 数组中每个位置可以分到的大小 // 如 initialCapacity 为 64，那么每个 Segment 或称之为&quot;槽&quot;可以分到 4 个 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; // 默认 MIN_SEGMENT_TABLE_CAPACITY 是 2，这个值也是有讲究的，因为这样的话，对于具体的槽上， // 插入一个元素不至于扩容，插入第二个的时候才会扩容 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // 创建 Segment 数组， // 并创建数组的第一个元素 segment[0] Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; // 往数组写入 segment[0] UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss;} 初始化完成，我们得到了一个 Segment 数组。 我们就当是用 new ConcurrentHashMap() 无参构造函数进行初始化的，那么初始化完成后: Segment 数组长度为 16，不可以扩容 Segment[i] 的默认大小为 2，负载因子是 0.75，得出初始阈值为 1.5，也就是以后插入第一个元素不会触发扩容，插入第二个会进行第一次扩容 这里初始化了 segment[0]，其他位置还是 null，至于为什么要初始化 segment[0]，后面的代码会介绍 当前 segmentShift 的值为 32 - 4 = 28，segmentMask 为 16 - 1 = 15，姑且把它们简单翻译为移位数和掩码，这两个值马上就会用到 ¶ put 过程分析我们先看 put 的主流程，对于其中的一些关键细节操作，后面会进行详细介绍。 123456789101112131415161718192021public V put(K key, V value) { Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); // 1. 计算 key 的 hash 值 int hash = hash(key); // 2. 根据 hash 值找到 Segment 数组中的位置 j // hash 是 32 位，无符号右移 segmentShift(28) 位，剩下高 4 位， // 然后和 segmentMask(15) 做一次与操作，也就是说 j 是 hash 值的高 4 位，也就是槽的数组下标 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; // 刚刚说了，初始化的时候初始化了 segment[0]，但是其他位置还是 null， // ensureSegment(j) 对 segment[j] 进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); // 3. 插入新值到 槽 s 中 return s.put(key, hash, value, false);} @pdai: 代码已经复制到剪贴板 第一层皮很简单，根据 hash 值很快就能找到相应的 Segment，之后就是 Segment 内部的 put 操作了。 Segment 内部是由 数组+链表 组成的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162final V put(K key, int hash, V value, boolean onlyIfAbsent) { // 在往该 segment 写入前，需要先获取该 segment 的独占锁 // 先看主流程，后面还会具体介绍这部分内容 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try { // 这个是 segment 内部的数组 HashEntry&lt;K,V&gt;[] tab = table; // 再利用 hash 值，求应该放置的数组下标 int index = (tab.length - 1) &amp; hash; // first 是数组该位置处的链表的表头 HashEntry&lt;K,V&gt; first = entryAt(tab, index); // 下面这串 for 循环虽然很长，不过也很好理解，想想该位置没有任何元素和已经存在一个链表这两种情况 for (HashEntry&lt;K,V&gt; e = first;;) { if (e != null) { K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) { oldValue = e.value; if (!onlyIfAbsent) { // 覆盖旧值 e.value = value; ++modCount; } break; } // 继续顺着链表走 e = e.next; } else { // node 到底是不是 null，这个要看获取锁的过程，不过和这里都没有关系。 // 如果不为 null，那就直接将它设置为链表表头；如果是null，初始化并设置为链表表头。 if (node != null) node.setNext(first); else node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; // 如果超过了该 segment 的阈值，这个 segment 需要扩容 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); // 扩容后面也会具体分析 else // 没有达到阈值，将 node 放到数组 tab 的 index 位置， // 其实就是将新的节点设置成原链表的表头 setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; } } } finally { // 解锁 unlock(); } return oldValue;} @pdai: 代码已经复制到剪贴板 整体流程还是比较简单的，由于有独占锁的保护，所以 segment 内部的操作并不复杂。至于这里面的并发问题，我们稍后再进行介绍。 到这里 put 操作就结束了，接下来，我们说一说其中几步关键的操作。 ¶ 初始化槽: ensureSegmentConcurrentHashMap 初始化的时候会初始化第一个槽 segment[0]，对于其他槽来说，在插入第一个值的时候进行初始化。 这里需要考虑并发，因为很可能会有多个线程同时进来初始化同一个槽 segment[k]，不过只要有一个成功了就可以。 12345678910111213141516171819202122232425262728293031private Segment&lt;K,V&gt; ensureSegment(int k) { final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // 这里看到为什么之前要初始化 segment[0] 了， // 使用当前 segment[0] 处的数组长度和负载因子来初始化 segment[k] // 为什么要用“当前”，因为 segment[0] 可能早就扩容过了 Segment&lt;K,V&gt; proto = ss[0]; int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); // 初始化 segment[k] 内部的数组 HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { // 再次检查一遍该槽是否被其他线程初始化了。 Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); // 使用 while 循环，内部用 CAS，当前线程成功设值或其他线程成功设值后，退出 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) { if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; } } } return seg;} 总的来说，ensureSegment(int k) 比较简单，对于并发操作使用 CAS 进行控制。 ¶ 获取写入锁: scanAndLockForPut前面我们看到，在往某个 segment 中 put 的时候，首先会调用 node = tryLock() ? null : scanAndLockForPut(key, hash, value)，也就是说先进行一次 tryLock() 快速获取该 segment 的独占锁，如果失败，那么进入到 scanAndLockForPut 这个方法来获取锁。 下面我们来具体分析这个方法中是怎么控制加锁的。 123456789101112131415161718192021222324252627282930313233343536373839404142private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) { HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node // 循环获取锁 while (!tryLock()) { HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) { if (e == null) { if (node == null) // speculatively create node // 进到这里说明数组该位置的链表是空的，没有任何元素 // 当然，进到这里的另一个原因是 tryLock() 失败，所以该槽存在并发，不一定是该位置 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; } else if (key.equals(e.key)) retries = 0; else // 顺着链表往下走 e = e.next; } // 重试次数如果超过 MAX_SCAN_RETRIES(单核1多核64)，那么不抢了，进入到阻塞队列等待锁 // lock() 是阻塞方法，直到获取锁后返回 else if (++retries &gt; MAX_SCAN_RETRIES) { lock(); break; } else if ((retries &amp; 1) == 0 &amp;&amp; // 这个时候是有大问题了，那就是有新的元素进到了链表，成为了新的表头 // 所以这边的策略是，相当于重新走一遍这个 scanAndLockForPut 方法 (f = entryForHash(this, hash)) != first) { e = first = f; // re-traverse if entry changed retries = -1; } } return node;} @pdai: 代码已经复制到剪贴板 这个方法有两个出口，一个是 tryLock() 成功了，循环终止，另一个就是重试次数超过了 MAX_SCAN_RETRIES，进到 lock() 方法，此方法会阻塞等待，直到成功拿到独占锁。 这个方法就是看似复杂，但是其实就是做了一件事，那就是获取该 segment 的独占锁，如果需要的话顺便实例化了一下 node。 ¶ 扩容: rehash重复一下，segment 数组不能扩容，扩容是 segment 数组某个位置内部的数组 HashEntry&lt;K,V&gt;[] 进行扩容，扩容后，容量为原来的 2 倍。 首先，我们要回顾一下触发扩容的地方，put 的时候，如果判断该值的插入会导致该 segment 的元素个数超过阈值，那么先进行扩容，再插值，读者这个时候可以回去 put 方法看一眼。 该方法不需要考虑并发，因为到这里的时候，是持有该 segment 的独占锁的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 方法参数上的 node 是这次扩容后，需要添加到新的数组中的数据。private void rehash(HashEntry&lt;K,V&gt; node) { HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; // 2 倍 int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); // 创建新数组 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; // 新的掩码，如从 16 扩容到 32，那么 sizeMask 为 31，对应二进制 ‘000...00011111’ int sizeMask = newCapacity - 1; // 遍历原数组，老套路，将原数组位置 i 处的链表拆分到 新数组位置 i 和 i+oldCap 两个位置 for (int i = 0; i &lt; oldCapacity ; i++) { // e 是链表的第一个元素 HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) { HashEntry&lt;K,V&gt; next = e.next; // 计算应该放置在新数组中的位置， // 假设原数组长度为 16，e 在 oldTable[3] 处，那么 idx 只可能是 3 或者是 3 + 16 = 19 int idx = e.hash &amp; sizeMask; if (next == null) // 该位置处只有一个元素，那比较好办 newTable[idx] = e; else { // Reuse consecutive sequence at same slot // e 是链表表头 HashEntry&lt;K,V&gt; lastRun = e; // idx 是当前链表的头节点 e 的新位置 int lastIdx = idx; // 下面这个 for 循环会找到一个 lastRun 节点，这个节点之后的所有元素是将要放到一起的 for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; } } // 将 lastRun 及其之后的所有节点组成的这个链表放到 lastIdx 这个位置 newTable[lastIdx] = lastRun; // 下面的操作是处理 lastRun 之前的节点， // 这些节点可能分配在另一个链表中，也可能分配到上面的那个链表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) { V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); } } } } // 将新来的 node 放到新数组中刚刚的 两个链表之一 的 头部 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;} @pdai: 代码已经复制到剪贴板 这里的扩容比之前的 HashMap 要复杂一些，代码难懂一点。上面有两个挨着的 for 循环，第一个 for 有什么用呢? 仔细一看发现，如果没有第一个 for 循环，也是可以工作的，但是，这个 for 循环下来，如果 lastRun 的后面还有比较多的节点，那么这次就是值得的。因为我们只需要克隆 lastRun 前面的节点，后面的一串节点跟着 lastRun 走就是了，不需要做任何操作。 我觉得 Doug Lea 的这个想法也是挺有意思的，不过比较坏的情况就是每次 lastRun 都是链表的最后一个元素或者很靠后的元素，那么这次遍历就有点浪费了。不过 Doug Lea 也说了，根据统计，如果使用默认的阈值，大约只有 1/6 的节点需要克隆。 ¶ get 过程分析相对于 put 来说，get 就很简单了。 计算 hash 值，找到 segment 数组中的具体位置，或我们前面用的“槽” 槽中也是一个数组，根据 hash 找到数组中具体的位置 到这里是链表了，顺着链表进行查找即可 1234567891011121314151617181920212223public V get(Object key) { Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; // 1. hash 值 int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; // 2. 根据 hash 找到对应的 segment if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) { // 3. 找到segment 内部数组相应位置的链表，遍历 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) { K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; } } return null;} @pdai: 代码已经复制到剪贴板 ¶ 并发问题分析现在我们已经说完了 put 过程和 get 过程，我们可以看到 get 过程中是没有加锁的，那自然我们就需要去考虑并发问题。 添加节点的操作 put 和删除节点的操作 remove 都是要加 segment 上的独占锁的，所以它们之间自然不会有问题，我们需要考虑的问题就是 get 的时候在同一个 segment 中发生了 put 或 remove 操作。 put 操作的线程安全性。 初始化槽，这个我们之前就说过了，使用了 CAS 来初始化 Segment 中的数组。 添加节点到链表的操作是插入到表头的，所以，如果这个时候 get 操作在链表遍历的过程已经到了中间，是不会影响的。当然，另一个并发问题就是 get 操作在 put 之后，需要保证刚刚插入表头的节点被读取，这个依赖于 setEntryAt 方法中使用的 UNSAFE.putOrderedObject。 扩容。扩容是新创建了数组，然后进行迁移数据，最后面将 newTable 设置给属性 table。所以，如果 get 操作此时也在进行，那么也没关系，如果 get 先行，那么就是在旧的 table 上做查询操作；而 put 先行，那么 put 操作的可见性保证就是 table 使用了 volatile 关键字。 remove 操作的线程安全性。 remove 操作我们没有分析源码，所以这里说的读者感兴趣的话还是需要到源码中去求实一下的。 get 操作需要遍历链表，但是 remove 操作会”破坏”链表。 如果 remove 破坏的节点 get 操作已经过去了，那么这里不存在任何问题。 如果 remove 先破坏了一个节点，分两种情况考虑。 1、如果此节点是头节点，那么需要将头节点的 next 设置为数组该位置的元素，table 虽然使用了 volatile 修饰，但是 volatile 并不能提供数组内部操作的可见性保证，所以源码中使用了 UNSAFE 来操作数组，请看方法 setEntryAt。2、如果要删除的节点不是头节点，它会将要删除节点的后继节点接到前驱节点中，这里的并发保证就是 next 属性是 volatile 的。 ¶ ConcurrentHashMap - JDK 1.8在JDK1.7之前，ConcurrentHashMap是通过分段锁机制来实现的，所以其最大并发度受Segment的个数限制。因此，在JDK1.8中，ConcurrentHashMap的实现原理摒弃了这种设计，而是选择了与HashMap类似的数组+链表+红黑树的方式实现，而加锁则采用CAS和synchronized实现。 ¶ 数据结构 结构上和 Java8 的 HashMap 基本上一样，不过它要保证线程安全性，所以在源码上确实要复杂一些。 ¶ 初始化1234567891011121314// 这构造函数里，什么都不干public ConcurrentHashMap() {}public ConcurrentHashMap(int initialCapacity) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(); int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); this.sizeCtl = cap;} @pdai: 代码已经复制到剪贴板 这个初始化方法有点意思，通过提供初始容量，计算了 sizeCtl，sizeCtl = 【 (1.5 * initialCapacity + 1)，然后向上取最近的 2 的 n 次方】。如 initialCapacity 为 10，那么得到 sizeCtl 为 16，如果 initialCapacity 为 11，得到 sizeCtl 为 32。 sizeCtl 这个属性使用的场景很多，不过只要跟着文章的思路来，就不会被它搞晕了。 ¶ put 过程分析仔细地一行一行代码看下去: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public V put(K key, V value) { return putVal(key, value, false);}final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); // 得到 hash 值 int hash = spread(key.hashCode()); // 用于记录相应链表的长度 int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; // 如果数组&quot;空&quot;，进行数组初始化 if (tab == null || (n = tab.length) == 0) // 初始化数组，后面会详细介绍 tab = initTable(); // 找该 hash 值对应的数组下标，得到第一个节点 f else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 如果数组该位置为空， // 用一次 CAS 操作将这个新值放入其中即可，这个 put 操作差不多就结束了，可以拉到最后面了 // 如果 CAS 失败，那就是有并发操作，进到下一个循环就好了 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } // hash 居然可以等于 MOVED，这个需要到后面才能看明白，不过从名字上也能猜到，肯定是因为在扩容 else if ((fh = f.hash) == MOVED) // 帮助数据迁移，这个等到看完数据迁移部分的介绍后，再理解这个就很简单了 tab = helpTransfer(tab, f); else { // 到这里就是说，f 是该位置的头节点，而且不为空 V oldVal = null; // 获取数组该位置的头节点的监视器锁 synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { // 头节点的 hash 值大于 0，说明是链表 // 用于累加，记录链表的长度 binCount = 1; // 遍历链表 for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; // 如果发现了&quot;相等&quot;的 key，判断是否要进行值覆盖，然后也就可以 break 了 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } // 到了链表的最末端，将这个新值放到链表的最后面 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { // 红黑树 Node&lt;K,V&gt; p; binCount = 2; // 调用红黑树的插值方法插入新节点 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { // 判断是否要将链表转换为红黑树，临界值和 HashMap 一样，也是 8 if (binCount &gt;= TREEIFY_THRESHOLD) // 这个方法和 HashMap 中稍微有一点点不同，那就是它不是一定会进行红黑树转换， // 如果当前数组的长度小于 64，那么会选择进行数组扩容，而不是转换为红黑树 // 具体源码我们就不看了，扩容部分后面说 treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } // addCount(1L, binCount); return null;} ¶ 初始化数组: initTable这个比较简单，主要就是初始化一个合适大小的数组，然后会设置 sizeCtl。 初始化方法中的并发问题是通过对 sizeCtl 进行一个 CAS 操作来控制的。 123456789101112131415161718192021222324252627282930private final Node&lt;K,V&gt;[] initTable() { Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { // 初始化的&quot;功劳&quot;被其他线程&quot;抢去&quot;了 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // CAS 一下，将 sizeCtl 设置为 -1，代表抢到了锁 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { // DEFAULT_CAPACITY 默认初始容量是 16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; // 初始化数组，长度为 16 或初始化时提供的长度 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; // 将这个数组赋值给 table，table 是 volatile 的 table = tab = nt; // 如果 n 为 16 的话，那么这里 sc = 12 // 其实就是 0.75 * n sc = n - (n &gt;&gt;&gt; 2); } } finally { // 设置 sizeCtl 为 sc，我们就当是 12 吧 sizeCtl = sc; } break; } } return tab;} [¶](#计数 addCount()) 计数 addCount()1addCount(1L, binCount); 如何保证并发的size更新的安全性-&gt;原子性 1.cas =&gt; 加锁 性能下降，不断cas，自旋 2.分治 1234567891011121314151617181920212223242526private final void addCount(long x, int check) { CounterCell[] as; long b, s; // 判断 counterCells 是否为空， // 1. 如果为空，就通过 cas 操作尝试修改 baseCount 变量，对这个变量进行原子累加操作(做这个操作的意义是：如果在没有竞争的情况下，仍然采用 baseCount 来记录元素个数) // 2. 如果 cas 失败说明存在竞争，这个时候不能再采用 baseCount 来累加，而是通过CounterCell 来记录 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) { CounterCell a; long v; int m; boolean uncontended = true; // 这里有几个判断//1. 计数表为空则直接调用 fullAddCount//2. 从计数表中随机取出一个数组的位置为空，直接调用 fullAddCount//3. 通过 CAS 修改 CounterCell 随机位置的值，如果修改失败说明出现并发情况（这里又用到了一种巧妙的方法），调用 fullAndCountRandom 在线程并发的时候会有性能问题以及可能会产生相同的随机数,ThreadLocalRandom.getProbe 可以解决这个问题，并且性能要比 Random 高 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) { fullAddCount(x, uncontended); return; } if (check &lt;= 1) //链表长度小于等于 1，不需要考虑扩容 return; s = sumCount(); // 统计ConcurrentHashMap元素个数 } ... } baseCount=0：用来记录元素个数的成员属性 =&gt; 传入值baseCount = 1 ThreadLocalRandom =&gt; 线程安全的生成随机数 ¶ CounterCell解释1234567891011121314151617181920212223 private transient volatile int cellsBusy; // 标识当前 cell 数组是否在初始化或扩容中的CAS 标志位 /** * Table of counter cells. When non-null, size is a power of 2. */ private transient volatile CounterCell[] counterCells; // counterCells 数组，总数值的分值分别存在每个 cell 中 @sun.misc.Contended static final class CounterCell { volatile long value; CounterCell(long x) { value = x; } }//看到这段代码就能够明白了，CounterCell 数组的每个元素，都存储一个元素个数，而实际我们调用size 方法就是通过这个循环累加来得到的//又是一个设计精华，大家可以借鉴； 有了这个前提，再会过去看 addCount 这个方法，就容易理解一些了 final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } ¶fullAddCount源码第一次进来的位置： cellsBusy：1表示已经有其他线程在进行扩容了 第一次线程进来的时候：ThreadA进入 1234567891011121314151617181920212223242526 //cellsBusy=0表示没有在做初始化，通过cas更新cellsbusy的值标注当前线程正在做初始化操作 else if (cellsBusy == 0 &amp;&amp; counterCells == as &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean init = false; try { // Initialize table if (counterCells == as) { //初始化容量为2 CounterCell[] rs = new CounterCell[2]; //将x也就是元素的个数 放在指定的数组下标位置 rs[h &amp; 1] = new CounterCell(x); //赋值给counterCells counterCells = rs; //设置初始化完成标识 init = true; } } finally { //恢复标识 cellsBusy = 0; } if (init) break; } else if (U.compareAndSwapLong(this, BASECOUNT, v = baseCount, v + x)) //竞争激烈，其它线程占据cell 数组，直接累加在base变量中 break; // Fall back on using base} 第二次线程线程进来的时候：ThreadB进入线程 123456789101112131415161718192021222324if ((a = as[(n - 1) &amp; h]) == null) { if (cellsBusy == 0) { // Try to attach new Cell CounterCell r = new CounterCell(x); // Optimistic create if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean created = false; try { // Recheck under lock CounterCell[] rs; int m, j; if ((rs = counterCells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) { rs[j] = r; created = true; } } finally { cellsBusy = 0; } if (created) break; continue; // Slot is now non-empty } } collide = false;} 其他线程进入： 1U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x) 直接通过cas进行修改value值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126private final void fullAddCount(long x, boolean wasUncontended) { int h; //获取当前线程的probe的值，如果值为0，则初始化当前线程的probe的值,probe就是随机数 if ((h = ThreadLocalRandom.getProbe()) == 0) { ThreadLocalRandom.localInit(); // force initialization h = ThreadLocalRandom.getProbe(); // 由于重新生成了probe，未冲突标志位设置为true wasUncontended = true; } boolean collide = false; // True if last slot nonempty //自旋 for (;;) { CounterCell[] as; CounterCell a; int n; long v; //说明counterCells已经被初始化过了，我们先跳过这个代码，先看初始化部分 if ((as = counterCells) != null &amp;&amp; (n = as.length) &gt; 0) { // 通过该值与当前线程probe求与，获得cells的下标元素，和hash 表获取索引是一样的 if ((a = as[(n - 1) &amp; h]) == null) { //cellsBusy=0表示counterCells不在初始化或者扩容状态下 if (cellsBusy == 0) { // Try to attach new Cell //构造一个CounterCell的值，传入元素个数 CounterCell r = new CounterCell(x); // Optimistic create //通过cas设置cellsBusy标识，防止其他线程来对counterCells并发处理 if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean created = false; try { // Recheck under lock CounterCell[] rs; int m, j; //将初始化的r对象的元素个数放在对应下标的位置 if ((rs = counterCells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) { rs[j] = r; created = true; } } finally { //恢复标志位 cellsBusy = 0; } if (created) break; //说明指定cells下标位置的数据不为空，则进行下一次循环 continue; // Slot is now non-empty } } collide = false; } //说明在addCount方法中cas失败了，并且获取probe的值不为空 else if (!wasUncontended) // CAS already known to fail //设置为未冲突标识，进入下一次自旋 wasUncontended = true; // Continue after rehash //由于指定下标位置的cell值不为空，则直接通过cas进行原子累加，如果成功，则直接退出 else if (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) break; //如果已经有其他线程建立了新的counterCells或者CounterCells大于CPU核心数（很巧妙，线程的并发数不会超过cpu核心数） else if (counterCells != as || n &gt;= NCPU) //设置当前线程的循环失败不进行扩容 collide = false; // At max size or stale //恢复collide状态，标识下次循环会进行扩容 else if (!collide) collide = true; //进入这个步骤，说明CounterCell数组容量不够，线程竞争较大，所以先设置一个标识表示为正在扩容 else if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { try { if (counterCells == as) {// Expand table unless stale //扩容一倍 2变成4，这个扩容比较简单 CounterCell[] rs = new CounterCell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; counterCells = rs; } } finally { //恢复标识 cellsBusy = 0; } collide = false; //继续下一次自旋 continue; // Retry with expanded table } //继续下一次自旋 h = ThreadLocalRandom.advanceProbe(h); } //cellsBusy=0表示没有在做初始化，通过cas更新cellsbusy的值标注当前线程正在做初始化操作 else if (cellsBusy == 0 &amp;&amp; counterCells == as &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) { boolean init = false; try { // Initialize table if (counterCells == as) { //初始化容量为2 CounterCell[] rs = new CounterCell[2]; //将x也就是元素的个数 放在指定的数组下标位置 rs[h &amp; 1] = new CounterCell(x); //赋值给counterCells counterCells = rs; //设置初始化完成标识 init = true; } } finally { //恢复标识 cellsBusy = 0; } if (init) break; } else if (U.compareAndSwapLong(this, BASECOUNT, v = baseCount, v + x)) //竞争激烈，其它线程占据cell 数组，直接累加在base变量中 break; // Fall back on using base } } ¶CountCells图解 ¶transfer扩容阶段 ConcurrentHashMap的扩容是可以并行扩容的 判断是否需要扩容，也就是当更新后的键值对总数 baseCount &gt;= 阈值 sizeCtl 时，进行 rehash，这里面会有两个逻辑。 1.如果当前正在处于扩容阶段，则当前线程会加入并且协助扩容 2.如果当前没有在扩容，则直接触发扩容操作 12345678910111213141516171819202122232425262728if (check &gt;= 0) {//如果 binCount&gt;=0，标识需要检查扩容 Node&lt;K,V&gt;[] tab, nt; int n, sc; //s 标识集合大小，如果集合大小大于或等于扩容阈值（默认值的 0.75） //并且 table 不为空并且 table 的长度小于最大容量 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) { int rs = resizeStamp(n); //这里是生成一个唯一的扩容戳， if (sc &lt; 0) { //sc&lt;0，也就是 sizeCtl&lt;0，说明已经有别的线程正在扩容了 //这 5 个条件只要有一个条件为 true，说明当前线程不能帮助进行此次的扩容，直接跳出循环 //sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT!=rs 表示比较高 RESIZE_STAMP_BITS 位生成戳和 rs 是否相等，相同 //sc=rs+1 表示扩容结束 //sc==rs+MAX_RESIZERS 表示帮助线程线程已经达到最大值了 //nt=nextTable -&gt; 表示扩容已经结束 //transferIndex&lt;=0 表示所有的 transfer 任务都被领取完了，没有剩余的hash 桶来给自己自己好这个线程来做 transfer if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //当前线程尝试帮助此次扩容，如果成功，则调用 transfer transfer(tab, nt); } // 如果当前没有在扩容，那么 rs 肯定是一个正数，通过 rs&lt;&lt;RESIZE_STAMP_SHIFT 将 sc 设置为一个负数，+2 表示有一个线程在执行扩容 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); // 重新计数，判断是否需要开启下一轮扩容 }} ¶resizeStamp123static final int resizeStamp(int n) { return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));} Integer.numberOfLeadingZeros 这个方法是返回无符号整数 n 最高位非 0 位前面的 0 的个数 n = 16； Integer.numberOfLeadingZeros(16) = 5 32 - 5 = 27 resizeStamp(16) = 32795 1U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 执行如上代码 0000 0000 0000 0000 1000 0000 0001 1011 左移16位 =&gt; 1000 0000 0001 1011 0000 0000 0000 0000 + 2 =&gt; 1000 0000 0001 1011 0000 0000 0000 0010 扩容戳： 高16位代表扩容的标记 低16位代表扩容的线程数 -&gt; 有一个线程参与了扩容 1.需要保证每次扩容的扩容戳都是唯一的 2.可以支持并发扩容 ➢ 这样来存储有什么好处呢？ 1.首先在 CHM 中是支持并发扩容的，也就是说如果当前的数组需要进行扩容操作，可以 由多个线程来共同负责，这块后续会单独讲 2.可以保证每次扩容都生成唯一的生成戳，每次新的扩容，都有一个不同的 n，这个生成 戳就是根据 n 来计算出来的一个数字，n 不同，这个数字也不同 ➢ 第一个线程尝试扩容的时候，为什么是+2 因为 1 表示初始化，2 表示一个线程在执行扩容，而且对 sizeCtl 的操作都是基于位运算的， 所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而 sc + 1 会在 低 16 位上加 1。 ¶transfer1.扩大数组的长度 2.数组迁移 ConcurrentHashMap 并没有直接加锁，而是采用 CAS 实现无锁的并发同步策略，最精华 的部分是它可以利用多线程来进行协同扩容 1、fwd:这个类是个标识类，用于指向新表用的，其他线程遇到这个类会主动跳过这个类，因 为这个类要么就是扩容迁移正在进行，要么就是已经完成扩容迁移，也就是这个类要保证线 程安全，再进行操作。 2、advance:这个变量是用于提示代码是否进行推进处理，也就是当前桶处理完，处理下一个 桶的标识 3、finishing:这个变量用于提示扩容是否结束用的 12if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range 让每一个CPU执行一段数据的扩容，每一个CPU可以处理16个长度的数组 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175int nextIndex, nextBound;if (--i &gt;= bound || finishing) advance = false;else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false;}else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { // nextBound = 16 // bound = 16, i = 32-1 =31 // bound = nextBoud // i = nextIndex - 1; // advance = false bound = nextBound; i = nextIndex - 1; advance = false;} private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { int n = tab.length, stride; //将 (n&gt;&gt;&gt;3 相当于 n/8) 然后除以 CPU 核心数。如果得到的结果小于 16，那么就使用 16// 这里的目的是让每个 CPU 处理的桶一样多，避免出现转移任务不均匀的现象，如果桶较少的话，默认一个 CPU（一个线程）处理 16 个桶，也就是长度为 16 的时候，扩容的时候只会有一个线程来扩容 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) { // initiating try { //新建一个 n&lt;&lt;1 原始 table 大小的 nextTab,也就是 32 @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; //赋值给 nextTab } catch (Throwable ex) { // try to cope with OOME //扩容失败，sizeCtl 使用 int 的最大值 sizeCtl = Integer.MAX_VALUE; return; } nextTable = nextTab;//更新成员变量 transferIndex = n; //更新成员变量 } int nextn = nextTab.length; //新的 tab 的长度 // 创建一个 fwd 节点，表示一个正在被迁移的 Node，并且它的 hash 值为-1(MOVED)，也就是前面我们在讲 putval 方法的时候，会有一个判断 MOVED 的逻辑。它的作用是用来占位，表示原数组中位置 i 处的节点完成迁移以后，就会在 i 位置设置一个 fwd 来告诉其他线程这个位置已经处理过了，具体后续还会在讲 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 首次推进为 true，如果等于 true，说明需要再次推进一个下标（i--），反之，如果是false，那么就不能推进下标，需要将当前的下标处理完毕才能继续推进 boolean advance = true; //判断是否已经扩容完成，完成就 return，退出循环 boolean finishing = false; // to ensure sweep before committing nextTab //通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置transferIndex 属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界，先处理槽位 15 的节点； for (int i = 0, bound = 0;;) { // 这个循环使用 CAS 不断尝试为当前线程分配任务 // 直到分配成功或任务队列已经被全部分配完毕 // 如果当前线程已经被分配过 bucket 区域 // 那么会通过--i 指向下一个待处理 bucket 然后退出该循环 Node&lt;K,V&gt; f; int fh; while (advance) { int nextIndex, nextBound; //--i 表示下一个待处理的 bucket，如果它&gt;=bound,表示当前线程已经分配过bucket 区域 if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) {//表示所有 bucket 已经被分配完毕 i = -1; advance = false; } //通过 cas 来修改 TRANSFERINDEX,为当前线程分配任务，处理的节点区间为(nextBound,nextIndex)-&gt;(0,15) else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { bound = nextBound; i = nextIndex - 1; advance = false; } } //i&lt;0 说明已经遍历完旧的数组，也就是当前线程已经处理完所有负责的 bucket if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; if (finishing) { //如果完成了扩容 nextTable = null; //删除成员变量 table = nextTab; //删除成员变量 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //更新阈值(32*0.75=24) return; } // sizeCtl 在迁移前会设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2 // 然后，每增加一个线程参与迁移就会将 sizeCtl 加 1，// 这里使用 CAS 操作对 sizeCtl 的低 16 位进行减 1，代表做完了属于自己的任务 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // 第一个扩容的线程，执行 transfer 方法之前，会设置 //sizeCtl = (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2)后续帮其扩容的线程，执行 transfer 方法之前，会设置 sizeCtl = sizeCtl+1每一个退出 transfer 的方法的线程，退出之前，会设置 sizeCtl = sizeCtl-1那么最后一个线程退出时：必然有sc == (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2)，即 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT // 如果 sc - 2 不等于标识符左移 16 位。如果他们相等了，说明没有线程在帮助他们扩容了。也就是说，扩容结束了。 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 如果相等，扩容结束了，更新 finising 变量 finishing = advance = true; // 再次循环检查一下整张表 i = n; // recheck before commit } } // 如果位置 i 处是空的，没有任何节点，那么放入刚刚初始化的 ForwardingNode ”空节点“ else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //表示该位置已经完成了迁移，也就是如果线程 A 已经处理过这个节点，那么线程 B 处理这个节点时，hash 值一定为 MOVED else if ((fh = f.hash) == MOVED) advance = true; // already processed else { synchronized (f) { if (tabAt(tab, i) == f) { Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) { int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } else if (f instanceof TreeBin) { TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } } } } } } 1、通过数组的方式实现并发增加元素的个数 2、并发扩容，可以通过多个线程并行实现数据的迁移 3、采用高低链的方式来解决多次hash计算的问题，提升了效率 4、sizeCtl的设计，3钟状态表示 5、resizeStamp的设计，高低位的设计来实现唯一性以及多个线程的协助扩容记录 ¶扩容过程图解ConcurrentHashMap 支持并发扩容，实现方式是，把 Node 数组进行拆分，让每个线程处理 自己的区域，假设 table 数组总长度是 64，默认情况下，那么每个线程可以分到 16 个 bucket。 然后每个线程处理的范围，按照倒序来做迁移 通过 for 自循环处理每个槽位中的链表元素，默认 advace 为真，通过 CAS 设置 transferIndex 属性值，并初始化 i 和 bound 值，i 指当前处理的槽位序号，bound 指需要处理的槽位边界， 先处理槽位 31 的节点； （bound,i） =(16,31) 从 31 的位置往前推动。 假设这个时候 ThreadA 在进行 transfer 在当前假设条件下，槽位 15 中没有节点，则通过 CAS 插入在第二步中初始化的 ForwardingNode 节点，用于告诉其它线程该槽位已经处理过了； [¶](#sizeCtl 扩容退出机制)sizeCtl 扩容退出机制12if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) 每存在一个线程执行完扩容操作，就通过 cas 执行 sc-1。 接着判断(sc-2) !=resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT ; 如果相等，表示当前为整个扩 容操作的 最后一个线程，那么意味着整个扩容操作就结束了；如果不想等，说明还得继续 这么做的目的，一方面是防止不同扩容之间出现相同的 sizeCtl，另外一方面，还可以避免 sizeCtl 的 ABA 问题导致的扩容重叠的情况 ¶数据迁移的实现方案¶高低位原理 123456789101112131415161718192021222324252627282930if (fh &gt;= 0) { int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } setTabAt(nextTab, i, ln);// 低位不需要变 setTabAt(nextTab, i + n, hn); // 高位要变 setTabAt(tab, i, fwd); advance = true;} 12setTabAt(nextTab, i, ln);// 低位不需要变setTabAt(nextTab, i + n, hn); // 高位要变，需要增加n长度的位置 ¶为什么要做高低位的划分为什么这么做？ 1f = tabAt(tab, i = (n - 1) &amp; hash) 扩容以后对于同一个值，结果是不变的 ¶扩容结束以后的退出机制123456789101112131415if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; if (finishing) { nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; } if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit } } ¶put第三阶段如果对应的节点存在，判断这个节点的 hash 是不是等于 MOVED(-1)，说明当前节点是 ForwardingNode 节点， 意味着有其他线程正在进行扩容，那么当前现在直接帮助它进行扩容，因此调用 helpTransfer 方法 123456789101112131415161718192021222324252627else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f);final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) { Node&lt;K,V&gt;[] nextTab; int sc; // 判断此时是否仍然在执行扩容,nextTab=null 的时候说明扩容已经结束了 if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) { int rs = resizeStamp(tab.length); while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) { //说明扩容还未完成的情况下不断循环来尝试将当前线程加入到扩容操作中 //下面部分的整个代码表示扩容结束，直接退出循环 //transferIndex&lt;=0 表示所有的 Node 都已经分配了线程 //sc=rs+MAX_RESIZERS 表示扩容线程数达到最大扩容线程数 //sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT !=rs， 如果在同一轮扩容中，那么 sc 无符号右移比较高位和 rs 的值，那么应该是相等的。如果不相等，说明扩容结束了 //sc==rs+1 表示扩容结束 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break;//跳出循环 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) {//在低 16 位上增加扩容线程数 transfer(tab, nextTab); break; } } return nextTab; } return table; } ¶put第四阶段这个方法的主要作用是，如果被添加的节点的位置已经存在节点的时候，需要以链表的方式加入到节点中 如果当前节点已经是一颗红黑树，那么就会按照红黑树的规则将当前节点加入到红黑树中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849else { //进入到这个分支，说明 f 是当前 nodes 数组对应位置节点的头节点，并且不为空 V oldVal = null; synchronized (f) { //给对应的头结点加锁 if (tabAt(tab, i) == f) { {//再次判断对应下标位置是否为 f 节点 if (fh &gt;= 0) { //头结点的 hash 值大于 0，说明是链表 binCount = 1; //用来记录链表的长度 for (Node&lt;K,V&gt; e = f;; ++binCount) { //遍历链表 K ek; //如果发现相同的 key，则判断是否需要进行值的覆盖 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; //默认情况下，直接覆盖旧的值 if (!onlyIfAbsent) e.val = value; break; } //一直遍历到链表的最末端，直接把新的值加入到链表的最后面 Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } //如果当前的 f 节点是一颗红黑树 else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; //则调用红黑树的插入方法插入新的值 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val;//同样，如果值已经存在，则直接替换 if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; }} ¶put的第五阶段判断链表的长度是否已经达到临界值 8. 如果达到了临界值，这个时候会根据当前数组的长度 来决定是扩容还是将链表转化为红黑树。也就是说如果当前数组的长度小于 64，就会先扩容。 否则，会把当前链表转化为红黑树 12345678if (binCount != 0) { {//说明上面在做链表操作 //如果链表长度已经达到临界值 8 就需要把链表转换为树结构 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) //如果 val 是被替换的，则返回替换之前的值 return oldVal; break;} ¶treeifyBin在 putVal 的最后部分，有一个判断，如果链表长度大于 8，那么就会触发扩容或者红黑树的 转化操作。 1234567891011121314151617181920212223242526private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) { Node&lt;K,V&gt; b; int n, sc; if (tab != null) { if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //tab 的长度是不是小于 64，如果是，则执行扩容 tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) { //否则，将当前链表转化为红黑树结构存储 synchronized (b) { // 将链表转换成红黑树 if (tabAt(tab, index) == b) { TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) { TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); } } } }} ¶tryPresize1234567891011121314151617181920212223242526272829303132333435363738394041private final void tryPresize(int size) { //对 size 进 行修复 ,主 要目的是防止传入的值不是一个 2 次幂的 整数 ，然后通过tableSizeFor来讲入参转化为离该整数最近的 2 次幂 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) { Node&lt;K,V&gt;[] tab = table; int n; if (tab == null || (n = tab.length) == 0) { n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); } } finally { sizeCtl = sc; } } } else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) { int rs = resizeStamp(n); if (sc &lt; 0) { Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); } }} 总结如果链表的长度大于8，并且node数组长度&gt;64得时候，如果再添加数据，会把当前链表转为红黑树，当出现扩容的话，链表长度&lt;8,红黑树又会转为链表。 红黑树 情况一 情况二 情况三 ¶ 链表转红黑树: treeifyBin前面我们在 put 源码分析也说过，treeifyBin 不一定就会进行红黑树转换，也可能是仅仅做数组扩容。我们还是进行源码分析吧。 123456789101112131415161718192021222324252627282930313233343536private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) { Node&lt;K,V&gt; b; int n, sc; if (tab != null) { // MIN_TREEIFY_CAPACITY 为 64 // 所以，如果数组长度小于 64 的时候，其实也就是 32 或者 16 或者更小的时候，会进行数组扩容 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) // 后面我们再详细分析这个方法 tryPresize(n &lt;&lt; 1); // b 是头节点 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) { // 加锁 synchronized (b) { if (tabAt(tab, index) == b) { // 下面就是遍历链表，建立一颗红黑树 TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) { TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } // 将红黑树设置到数组相应位置中 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); } } } }} @pdai: 代码已经复制到剪贴板 ¶ 扩容: tryPresize如果说 Java8 ConcurrentHashMap 的源码不简单，那么说的就是扩容操作和迁移操作。 这个方法要完完全全看懂还需要看之后的 transfer 方法，读者应该提前知道这点。 这里的扩容也是做翻倍扩容的，扩容后数组容量为原来的 2 倍。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 首先要说明的是，方法参数 size 传进来的时候就已经翻了倍了private final void tryPresize(int size) { // c: size 的 1.5 倍，再加 1，再往上取最近的 2 的 n 次方。 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) { Node&lt;K,V&gt;[] tab = table; int n; // 这个 if 分支和之前说的初始化数组的代码基本上是一样的，在这里，我们可以不用管这块代码 if (tab == null || (n = tab.length) == 0) { n = (sc &gt; c) ? sc : c; if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if (table == tab) { @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; sc = n - (n &gt;&gt;&gt; 2); // 0.75 * n } } finally { sizeCtl = sc; } } } else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; else if (tab == table) { // 我没看懂 rs 的真正含义是什么，不过也关系不大 int rs = resizeStamp(n); if (sc &lt; 0) { Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 2. 用 CAS 将 sizeCtl 加 1，然后执行 transfer 方法 // 此时 nextTab 不为 null if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); } // 1. 将 sizeCtl 设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 我是没看懂这个值真正的意义是什么? 不过可以计算出来的是，结果是一个比较大的负数 // 调用 transfer 方法，此时 nextTab 参数为 null else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); } }} 这个方法的核心在于 sizeCtl 值的操作，首先将其设置为一个负数，然后执行 transfer(tab, null)，再下一个循环将 sizeCtl 加 1，并执行 transfer(tab, nt)，之后可能是继续 sizeCtl 加 1，并执行 transfer(tab, nt)。 所以，可能的操作就是执行 1 次 transfer(tab, null) + 多次 transfer(tab, nt)，这里怎么结束循环的需要看完 transfer 源码才清楚。 ¶ 数据迁移: transfer下面这个方法有点长，将原来的 tab 数组的元素迁移到新的 nextTab 数组中。 虽然我们之前说的 tryPresize 方法中多次调用 transfer 不涉及多线程，但是这个 transfer 方法可以在其他地方被调用，典型地，我们之前在说 put 方法的时候就说过了，请往上看 put 方法，是不是有个地方调用了 helpTransfer 方法，helpTransfer 方法会调用 transfer 方法的。 此方法支持多线程执行，外围调用此方法的时候，会保证第一个发起数据迁移的线程，nextTab 参数为 null，之后再调用此方法的时候，nextTab 不会为 null。 阅读源码之前，先要理解并发操作的机制。原数组长度为 n，所以我们有 n 个迁移任务，让每个线程每次负责一个小任务是最简单的，每做完一个任务再检测是否有其他没做完的任务，帮助迁移就可以了，而 Doug Lea 使用了一个 stride，简单理解就是步长，每个线程每次负责迁移其中的一部分，如每次迁移 16 个小任务。所以，我们就需要一个全局的调度者来安排哪个线程执行哪几个任务，这个就是属性 transferIndex 的作用。 第一个发起数据迁移的线程会将 transferIndex 指向原数组最后的位置，然后从后往前的 stride 个任务属于第一个线程，然后将 transferIndex 指向新的位置，再往前的 stride 个任务属于第二个线程，依此类推。当然，这里说的第二个线程不是真的一定指代了第二个线程，也可以是同一个线程，这个读者应该能理解吧。其实就是将一个大的迁移任务分为了一个个任务包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) { int n = tab.length, stride; // stride 在单核下直接等于 n，多核模式下为 (n&gt;&gt;&gt;3)/NCPU，最小值是 16 // stride 可以理解为”步长“，有 n 个位置是需要进行迁移的， // 将这 n 个任务分为多个任务包，每个任务包有 stride 个任务 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 如果 nextTab 为 null，先进行一次初始化 // 前面我们说了，外围会保证第一个发起迁移的线程调用此方法时，参数 nextTab 为 null // 之后参与迁移的线程调用此方法时，nextTab 不会为 null if (nextTab == null) { try { // 容量翻倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } // nextTable 是 ConcurrentHashMap 中的属性 nextTable = nextTab; // transferIndex 也是 ConcurrentHashMap 的属性，用于控制迁移的位置 transferIndex = n; } int nextn = nextTab.length; // ForwardingNode 翻译过来就是正在被迁移的 Node // 这个构造方法会生成一个Node，key、value 和 next 都为 null，关键是 hash 为 MOVED // 后面我们会看到，原数组中位置 i 处的节点完成迁移工作后， // 就会将位置 i 处设置为这个 ForwardingNode，用来告诉其他线程该位置已经处理过了 // 所以它其实相当于是一个标志。 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // advance 指的是做完了一个位置的迁移工作，可以准备做下一个位置的了 boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab /* * 下面这个 for 循环，最难理解的在前面，而要看懂它们，应该先看懂后面的，然后再倒回来看 * */ // i 是位置索引，bound 是边界，注意是从后往前 for (int i = 0, bound = 0;;) { Node&lt;K,V&gt; f; int fh; // 下面这个 while 真的是不好理解 // advance 为 true 表示可以进行下一个位置的迁移了 // 简单理解结局: i 指向了 transferIndex，bound 指向了 transferIndex-stride while (advance) { int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; // 将 transferIndex 值赋给 nextIndex // 这里 transferIndex 一旦小于等于 0，说明原数组的所有位置都有相应的线程去处理了 else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { // 看括号中的代码，nextBound 是这次迁移任务的边界，注意，是从后往前 bound = nextBound; i = nextIndex - 1; advance = false; } } if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; if (finishing) { // 所有的迁移操作已经完成 nextTable = null; // 将新的 nextTab 赋值给 table 属性，完成迁移 table = nextTab; // 重新计算 sizeCtl: n 是原数组长度，所以 sizeCtl 得出的值将是新数组长度的 0.75 倍 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; } // 之前我们说过，sizeCtl 在迁移前会设置为 (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2 // 然后，每有一个线程参与迁移就会将 sizeCtl 加 1， // 这里使用 CAS 操作对 sizeCtl 进行减 1，代表做完了属于自己的任务 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // 任务结束，方法退出 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 到这里，说明 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT， // 也就是说，所有的迁移任务都做完了，也就会进入到上面的 if(finishing){} 分支了 finishing = advance = true; i = n; // recheck before commit } } // 如果位置 i 处是空的，没有任何节点，那么放入刚刚初始化的 ForwardingNode ”空节点“ else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 该位置处是一个 ForwardingNode，代表该位置已经迁移过了 else if ((fh = f.hash) == MOVED) advance = true; // already processed else { // 对数组该位置处的结点加锁，开始处理数组该位置处的迁移工作 synchronized (f) { if (tabAt(tab, i) == f) { Node&lt;K,V&gt; ln, hn; // 头节点的 hash 大于 0，说明是链表的 Node 节点 if (fh &gt;= 0) { // 下面这一块和 Java7 中的 ConcurrentHashMap 迁移是差不多的， // 需要将链表一分为二， // 找到原链表中的 lastRun，然后 lastRun 及其之后的节点是一起进行迁移的 // lastRun 之前的节点需要进行克隆，然后分到两个链表中 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); } // 其中的一个链表放在新数组的位置 i setTabAt(nextTab, i, ln); // 另一个链表放在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; } else if (f instanceof TreeBin) { // 红黑树的迁移 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) { int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) { if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; } else { if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; } } // 如果一分为二后，节点数少于 8，那么将红黑树转换回链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; // 将 ln 放置在新数组的位置 i setTabAt(nextTab, i, ln); // 将 hn 放置在新数组的位置 i+n setTabAt(nextTab, i + n, hn); // 将原数组该位置处设置为 fwd，代表该位置已经处理完毕， // 其他线程一旦看到该位置的 hash 值为 MOVED，就不会进行迁移了 setTabAt(tab, i, fwd); // advance 设置为 true，代表该位置已经迁移完毕 advance = true; } } } } }} 说到底，transfer 这个方法并没有实现所有的迁移任务，每次调用这个方法只实现了 transferIndex 往前 stride 个位置的迁移工作，其他的需要由外围来控制。 这个时候，再回去仔细看 tryPresize 方法可能就会更加清晰一些了。 ¶ get 过程分析get 方法从来都是最简单的，这里也不例外: 计算 hash 值 根据 hash 值找到数组对应位置: (n - 1) &amp; h 根据该位置处结点性质进行相应查找 如果该位置为 null，那么直接返回 null 就可以了 如果该位置处的节点刚好就是我们需要的，返回该节点的值即可 如果该位置节点的 hash 值小于 0，说明正在扩容，或者是红黑树，后面我们再介绍 find 方法 如果以上 3 条都不满足，那就是链表，进行遍历比对即可 1234567891011121314151617181920212223242526public V get(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { // 判断头节点是否就是我们需要的节点 if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } // 如果头节点的 hash 小于 0，说明 正在扩容，或者该位置是红黑树 else if (eh &lt; 0) // 参考 ForwardingNode.find(int h, Object k) 和 TreeBin.find(int h, Object k) return (p = e.find(h, key)) != null ? p.val : null; // 遍历链表 while ((e = e.next) != null) { if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null;} 简单说一句，此方法的大部分内容都很简单，只有正好碰到扩容的情况，ForwardingNode.find(int h, Object k) 稍微复杂一些，不过在了解了数据迁移的过程后，这个也就不难了，所以限于篇幅这里也不展开说了。 ¶ 对比总结 HashTable : 使用了synchronized关键字对put等操作进行加锁; ConcurrentHashMap JDK1.7: 使用分段锁机制实现; ConcurrentHashMap JDK1.8: 则使用数组+链表+红黑树数据结构和CAS原子操作实现","link":"/2022/03/04/JUC%E9%9B%86%E5%90%88-ConcurrentHashMap%E8%AF%A6%E8%A7%A3/"},{"title":"Java内存区域与Java内存模型","text":"Java内存区域Java虚拟机在运行程序时把其自动管理的内存划分为以下几个区域。这个区域里的一些数据在JVM启动的时候创建，在JVM退出的时候销毁。而其他的数据依赖于每一个线程，在线程创建时创建，在线程退出时销毁。 1. 方法区（Method Area）：方法区又称Non-Heap（非堆）,主要用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。简单说方法区用来存储类型的元数据信息，一个.class文件是类被java虚拟机使用之前的表现形式，一旦这个类要被使用，java虚拟机就会对其进行装载、连接（验证、准备、解析）和初始化。而装载（后的结果就是由.class文件转变为方法区中的一段特定的数据结构。这个数据结构会存储如下信息： 类型信息 这个类型的全限定名 这个类型的直接超类的全限定名 这个类型是类类型还是接口类型 这个类型的访问修饰符 任何直接超接口的全限定名的有序列表 字段信息 字段名 字段类型 字段的修饰符 方法信息 方法名 方法返回类型 方法参数的数量和类型（按照顺序） 方法的修饰 其他信息 除了常量以外的所有类（静态）变量 一个指向ClassLoader的指针 一个指向Class对象的指针 常量池（常量数据以及对其他类型的符号引用） JVM为每个已加载的类型都维护一个常量池。常量池就是这个类型用到的常量的一个有序集合，包括实际的常量(string,integer,和floating point常量)和对类型，域和方法的符号引用。池中的数据项象数组项一样，是通过索引访问的。 每个类的这些元数据，无论是在构建这个类的实例还是调用这个类某个对象的方法，都会访问方法区的这些元数据。 构建一个对象时，JVM会在堆中给对象分配空间，这些空间用来存储当前对象实例属性以及其父类的实例属性（而这些属性信息都是从方法区获得），注意，这里并不是仅仅为当前对象的实例属性分配空间，还需要给父类的实例属性分配，到此其实我们就可以回答第一个问题了，即实例化父类的某个子类时，JVM也会同时构建父类的一个对象。从另外一个角度也可以印证这个问题：调用当前类的构造方法时，首先会调用其父类的构造方法直到Object，而构造方法的调用意味着实例的创建，所以子类实例化时，父类肯定也会被实例化。 类变量被类的所有实例共享，即使没有类实例时你也可以访问它。这些变量只与类相关，所以在方法区中，它们成为类数据在逻辑上的一部分。在JVM使用一个类之前，它必须在方法区中为每个non-final类变量分配空间。 方法区主要有以下几个特点： 1、方法区是线程安全的。由于所有的线程都共享方法区，所以，方法区里的数据访问必须被设计成线程安全的。例如，假如同时有两个线程都企图访问方法区中的同一个类，而这个类还没有被装入JVM，那么只允许一个线程去装载它，而其它线程必须等待 2、方法区的大小不必是固定的，JVM可根据应用需要动态调整。同时，方法区也不一定是连续的，方法区可以在一个堆(甚至是JVM自己的堆)中自由分配。 3、方法区也可被垃圾收集，当某个类不在被使用(不可触及)时，JVM将卸载这个类，进行垃圾收集 可以通过-XX:PermSize 和 -XX:MaxPermSize 参数限制方法区的大小。 对于习惯在HotSpot 虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（PermanentGeneration），本质上两者并不等价，仅仅是因为HotSpot 虚拟机的设计团队选择把GC 分代收集扩展至方法区，或者说使用永久代来实现方法区而已。对于其他虚拟机（如BEA JRockit、IBM J9 等）来说是不存在永久代的概念的。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载。当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 2. JVM堆（Java Heap）：Java 堆也是属于线程共享的内存区域，它在虚拟机启动时创建，是Java 虚拟机所管理的内存中最大的一块，主要用于存放对象实例，几乎所有的对象实例都在这里分配内存，注意Java 堆是垃圾收集器管理的主要区域，因此很多时候也被称做GC 堆，如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。 堆的大小可以通过-Xms(最小值)和-Xmx(最大值)参数设置，-Xms为JVM启动时申请的最小内存，默认为操作系统物理内存的1/64但小于1G，-Xmx为JVM可申请的最大内存，默认为物理内存的1/4但小于1G，默认当空余堆内存小于40%时，JVM会增大Heap到-Xmx指定的大小，可通过-XX:MinHeapFreeRation=来指定这个比列；当空余堆内存大于70%时，JVM会减小heap的大小到-Xms指定的大小，可通过XX:MaxHeapFreeRation=来指定这个比列，对于运行系统，为避免在运行时频繁调整Heap的大小，通常-Xms与-Xmx的值设成一样。 如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java 堆中还可以细分为：新生代和老年代； 新生代：程序新创建的对象都是从新生代分配内存，新生代由Eden Space和两块相同大小的Survivor Space(通常又称S0和S1或From和To)构成，可通过-Xmn参数来指定新生代的大小，也可以通过-XX:SurvivorRation来调整Eden Space及SurvivorSpace的大小。 老年代：用于存放经过多次新生代GC仍然存活的对象，例如缓存对象，新建的对象也有可能直接进入老年代，主要有两种情况：1、大对象，可通过启动参数设置-XX:PretenureSizeThreshold=1024(单位为字节，默认为0)来代表超过多大时就不在新生代分配，而是直接在老年代分配。2、大的数组对象，且数组中无引用外部对象。 老年代所占的内存大小为-Xmx对应的值减去-Xmn对应的值。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError 异常。 3. 虚拟机栈(Java Virtual Machine Stacks)：线程私有，它的生命周期与线程相同。虚拟机栈描述的是Java 方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。 动画是由一帧一帧图片连续切换结果的结果而产生的，其实虚拟机的运行和动画也类似，每个在虚拟机中运行的程序也是由许多的帧的切换产生的结果，只是这些帧里面存放的是方法的局部变量，操作数栈，动态链接，方法返回地址和一些额外的附加信息组成。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。对于执行引擎来说，活动线程中，只有栈顶的栈帧是有效的，称为当前栈帧，这个栈帧所关联的方法称为当前方法。执行引擎所运行的所有字节码指令都只针对当前栈帧进行操作。 4. 本地方法栈(Native Method Stacks)：本地方法栈（Native MethodStacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java 方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native 方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot 虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 5. 程序计数器(Program Counter Register)：程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java 虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java 方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie 方法，这个计数器值则为空（Undefined）。 此内存区域是唯一一个在Java 虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 Java内存模型Java内存模型(Java Memory Model，简称JMM)的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样底层细节。此处的变量与Java编程时所说的变量不一样，指包括了实例字段、静态字段和构成数组对象的元素，但是不包括局部变量与方法参数，后者是线程私有的，不会被共享。 Java内存模型中规定: 1. 线程对变量的所有操作（读取、赋值）都必须在工作内存中进行，而不能直接读写主内存中的变量 不同线程之间无法直接访问对方工作内存中的变量，线程间变量值的传递均需要在主内存来完成 这里的主内存、工作内存与Java内存区域的Java堆、栈、方法区不是同一层次内存划分,这两者基本上是没有关系的，如果两者一不定要勉强对就起来，那从变量，主内存，工作内存的定义来看，主内存对应Java堆中的对象实例数据部分，工作内存对应于虚拟机栈中的部分区域。 重排序在执行程序时为了提高性能，编译器和处理器经常会对指令进行重排序。重排序分成三种类型： 编译器优化的重排序。编译器在不改变单线程程序语义放入前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 as-if-serial语义as-if-serial语义的意思是，所有的操作均可以为了优化而被重排序，但是你必须要保证重排序后执行的结果不能被改变，编译器、runtime、处理器都必须遵守as-if-serial语义。注意as-if-serial只保证单线程环境，多线程环境下无效。重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。 原子性、可见性与有序性原子性：一个操作或者多个操作要么全部执行要么全部不执行； 可见性：当多个线程同时访问一个共享变量时，如果其中某个线程更改了该共享变量，其他线程应该可以立刻看到这个改变； 有序性：程序的执行要按照代码的先后顺序执行； happens-before原则Java内存模型中定义的两项操作之间的次序关系，如果说操作A先行发生于操作B，操作A产生的影响能被操作B观察到，“影响”包含了修改了内存中共享变量的值、发送了消息、调用了方法等。 下面是Java内存模型下一些”天然的“happens-before关系，这些happens-before关系无须任何同步器协助就已经存在，可以在编码中直接使用。如果两个操作之间的关系不在此列，并且无法从下列规则推导出来的话，它们就没有顺序性保障，虚拟机可以对它们进行随意地重排序。 a.程序次序规则(Pragram Order Rule)：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环结构。 b.管程锁定规则(Monitor Lock Rule)：一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，而”后面“是指时间上的先后顺序。 c.volatile变量规则(Volatile Variable Rule)：对一个volatile变量的写操作先行发生于后面对这个变量的读取操作，这里的”后面“同样指时间上的先后顺序。 d.线程启动规则(Thread Start Rule)：Thread对象的start()方法先行发生于此线程的每一个动作。 e.线程终于规则(Thread Termination Rule)：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join()方法结束，Thread.isAlive()的返回值等作段检测到线程已经终止执行。 f.线程中断规则(Thread Interruption Rule)：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupted()方法检测是否有中断发生。 g.对象终结规则(Finalizer Rule)：一个对象初始化完成(构造方法执行完成)先行发生于它的finalize()方法的开始。 g.传递性(Transitivity)：如果操作A先行发生于操作B，操作B先行发生于操作C，那就可以得出操作A先行发生于操作C的结论。 一个操作”时间上的先发生“不代表这个操作会是”先行发生“，那如果一个操作”先行发生“是否就能推导出这个操作必定是”时间上的先发生 “呢？也是不成立的，一个典型的例子就是指令重排序。所以时间上的先后顺序与happens-before原则之间基本没有什么关系，所以衡量并发安全问题一切必须以happens-before 原则为准。","link":"/2019/07/11/Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E4%B8%8EJava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"title":"Java 高级 --- 多线程快速入门","text":"多线程快速入门author: RolandLeetags: []categories: java 基础date: 2019-01-02 21:36:00 这世上有三样东西是别人抢不走的：一是吃进胃里的食物，二是藏在心中的梦想，三是读进大脑的书 多线程快速入门1、线程与进程区别 每个正在系统上运行的程序都是一个进程。每个进程包含一到多个线程。线程是一组指令的集合，或者是程序的特殊段，它可以在程序里独立执行。 所以线程基本上是轻量级的进程，它负责在单个程序里执行多任务。通常由操作系统负责多个线程的调度和执行。 使用线程可以把占据时间长的程序中的任务放到后台去处理，程序的运行速度可能加快，在一些等待的任务实现上如用户输入、文件读写和网络收发数据等，线程就比较有用了。在这种情况下可以释放一些珍贵的资源如内存占用等等。 如果有大量的线程,会影响性能，因为操作系统需要在它们之间切换，更多的线程需要更多的内存空间，线程的中止需要考虑其对程序运行的影响。通常块模型数据是在多个线程间共享的，需要防止线程死锁情况的发生。 总结:进程是所有线程的集合，每一个线程是进程中的一条执行路径。 2、为什么要使用多线程？ （1）、使用多线程可以减少程序的响应时间。单线程如果遇到等待或阻塞，将会导致程序不响应鼠标键盘等操作，使用多线程可以解决此问题，增强程序的交互性。 （2）、与进程相比，线程的创建和切换开销更小，因为线程共享代码段、数据段等内存空间。 （3）、多核CPU，多核计算机本身就具有执行多线程的能力，如果使用单个线程，将无法重复利用计算资源，造成资源的巨大浪费。 （4）、多线程可以简化程序的结构，使程序便于维护，一个非常复杂的进程可以分为多个线程执行。 3、多线程应用场景？ 答:主要能体现到多线程提高程序效率。 举例: 迅雷多线程下载、数据库连接池、分批发送短信等。 4、多线程创建方式第一种、 继承Thread类 重写run方法123456789101112131415161718192021class CreateThread extends Thread { // run方法中编写 多线程需要执行的代码 publicvoid run() { for (inti = 0; i&lt; 10; i++) { System.out.println(&quot;i:&quot; + i); } }}publicclass ThreadDemo { publicstaticvoid main(String[] args) { System.out.println(&quot;-----多线程创建开始-----&quot;); // 1.创建一个线程 CreateThread createThread = new CreateThread(); // 2.开始执行线程 注意 开启线程不是调用run方法，而是start方法 System.out.println(&quot;-----多线程创建启动-----&quot;); createThread.start(); System.out.println(&quot;-----多线程创建结束-----&quot;); }} 调用start方法后，代码并没有从上往下执行，而是有一条新的执行分支 注意：画图演示多线程不同执行路径。 第二种、实现Runnable接口,重写run方法12345678910111213141516171819202122class CreateRunnable implements Runnable { @Override publicvoid run() { for (inti = 0; i&lt; 10; i++) { System.out.println(&quot;i:&quot; + i); } }}publicclass ThreadDemo2 { publicstaticvoid main(String[] args) { System.out.println(&quot;-----多线程创建开始-----&quot;); // 1.创建一个线程 CreateRunnable createThread = new CreateRunnable(); // 2.开始执行线程 注意 开启线程不是调用run方法，而是start方法 System.out.println(&quot;-----多线程创建启动-----&quot;); Thread thread = new Thread(createThread); thread.start(); System.out.println(&quot;-----多线程创建结束-----&quot;); }} 第三种、使用匿名内部类方式12345678910System.out.println(&quot;-----多线程创建开始-----&quot;); Thread thread = new Thread(new Runnable() { public void run() { for (int i = 0; i&lt; 10; i++) { System.out.println(&quot;i:&quot; + i); } } }); thread.start(); System.out.println(&quot;-----多线程创建结束-----&quot;); 5、使用继承Thread类还是使用实现Runnable接口好？ 使用实现实现Runnable接口好，原因实现了接口还可以继续继承，继承了类不能再继承。 6、启动线程是使用调用start方法还是run方法？ 开始执行线程 注意 开启线程不是调用run方法，而是start方法调用run知识使用实例调用方法。 7、获取线程对象以及名称| 常用线程api方法 || ——– | :—– || start() | 启动线程 || currentThread() | 获取当前线程对象| getID()| 获取当前线程ID Thread-编号 该编号从0开始| getName()| 获取当前线程名称| sleep(long mill) | 休眠线程| Stop（） | 停止线程,| 常用线程构造函数 || Thread（） | 分配一个新的 Thread 对象| Thread（String name）| 分配一个新的 Thread对象，具有指定的 name正如其名。| Thread（Runable r）| 分配一个新的 Thread对象| Thread（Runable r, String name） | 分配一个新的 Thread对象 8、守护线程 Java中有两种线程，一种是用户线程，另一种是守护线程。 用户线程是指用户自定义创建的线程，主线程停止，用户线程不会停止 守护线程当进程不存在或主线程停止，守护线程也会被停止。 使用setDaemon(true)方法设置为守护线程 123456789101112131415161718192021222324252627282930public class DaemonThread { public static void main(String[] args) { Thread thread = new Thread(new Runnable() { @Override public void run() { while (true) { try { Thread.sleep(100); } catch (Exception e) { // TODO: handle exception } System.out.println(&quot;我是子线程...&quot;); } } }); thread.setDaemon(true); thread.start(); for (int i = 0; i &lt; 10; i++) { try { Thread.sleep(100); } catch (Exception e) { } System.out.println(&quot;我是主线程&quot;); } System.out.println(&quot;主线程执行完毕!&quot;); }} 9、多线程运行状态 线程从创建、运行到结束总是处于下面五个状态之一：新建状态、就绪状态、运行状态、阻塞状态及死亡状态 新建状态 当用new操作符创建一个线程时， 例如new Thread(r)，线程还没有开始运行，此时线程处在新建状态。 当一个线程处于新生状态时，程序还没有开始运行线程中的代码 就绪状态 一个新创建的线程并不自动开始运行，要执行线程，必须调用线程的start()方法。当线程对象调用start()方法即启动了线程，start()方法创建线程运行的系统资源，并调度线程运行run()方法。当start()方法返回后，线程就处于就绪状态。 处于就绪状态的线程并不一定立即运行run()方法，线程还必须同其他线程竞争CPU时间，只有获得CPU时间才可以运行线程。因为在单CPU的计算机系统中，不可能同时运行多个线程，一个时刻仅有一个线程处于运行状态。因此此时可能有多个线程处于就绪状态。对多个处于就绪状态的线程是由Java运行时系统的线程调度程序(thread scheduler)来调度的。 运行状态 当线程获得CPU时间后，它才进入运行状态，真正开始执行run()方法.阻塞状态线程运行过程中，可能由于各种原因进入阻塞状态: 1&gt;线程通过调用sleep方法进入睡眠状态； 2&gt;线程调用一个在I/O上被阻塞的操作，即该操作在输入输出操作完成之前不会返回到它的调用者； 3&gt;线程试图得到一个锁，而该锁正被其他线程持有； 4&gt;线程在等待某个触发条件； 死亡状态 有两个原因会导致线程死亡： - - 1) run方法正常退出而自然死亡， - - 2) 一个未捕获的异常终止了run方法而使线程猝死。 为了确定线程在当前是否存活着（就是要么是可运行的，要么是被阻塞了），需要使用isAlive方法。如果是可运行或被阻塞，这个方法返回true； 如果线程仍旧是new状态且不是可运行的， 或者线程死亡了，则返回false. join()方法作用 当在主线程当中执行到t1.join()方法时，就认为主线程应该把执行权让给t1 创建一个线程，子线程执行完毕后，主线程才能执行。 1234567891011121314151617181920212223242526Thread t1 = new Thread(new Runnable() { @Override public void run() { for (int i = 0; i &lt; 10; i++) { try { Thread.sleep(10); } catch (Exception e) { } System.out.println(Thread.currentThread().getName() + &quot;i:&quot; + i); } } }); t1.start(); // 当在主线程当中执行到t1.join()方法时，就认为主线程应该把执行权让给t1 t1.join(); for (int i = 0; i &lt; 10; i++) { try { Thread.sleep(10); } catch (Exception e) { } System.out.println(&quot;main&quot; + &quot;i:&quot; + i); } 优先级 现代操作系统基本采用时分的形式调度运行的线程，线程分配得到的时间片的多少决定了线程使用处理器资源的多少，也对应了线程优先级这个概念。在JAVA线程中，通过一个int priority来控制优先级，范围为1-10，其中10最高，默认值为5。下面是源码（基于1.8）中关于priority的一些量和方法。 12345678910111213141516171819202122class PrioritytThread implements Runnable { public void run() { for (int i = 0; i &lt; 100; i++) { System.out.println(Thread.currentThread().toString() + &quot;---i:&quot; + i); } }}public class ThreadDemo4 { public static void main(String[] args) { PrioritytThread prioritytThread = new PrioritytThread(); Thread t1 = new Thread(prioritytThread); Thread t2 = new Thread(prioritytThread); t1.start(); // 注意设置了优先级， 不代表每次都一定会被执行。 只是CPU调度会有限分配 t1.setPriority(10); t2.start(); }} Yield方法Thread.yield()方法的作用：暂停当前正在执行的线程，并执行其他线程。（可能没有效果）yield()让当前正在运行的线程回到可运行状态，以允许具有相同优先级的其他线程获得运行的机会。因此，使用yield()的目的是让具有相同优先级的线程之间能够适当的轮换执行。但是，实际中无法保证yield()达到让步的目的，因为，让步的线程可能被线程调度程序再次选中。结论：大多数情况下，yield()将导致线程从运行状态转到可运行状态，但有可能没有效果。 总结 1.进程与线程的区别？ 答:进程是所有线程的集合，每一个线程是进程中的一条执行路径，线程只是一条执行路径。 2.为什么要用多线程？ 答:提高程序效率 3.多线程创建方式？ 答:继承Thread或Runnable 接口。 4.是继承Thread类好还是实现Runnable接口好？ 答:Runnable接口好，因为实现了接口还可以继续继承。继承Thread类不能再继承。 5.你在哪里用到了多线程？ 答:主要能体现到多线程提高程序效率。 举例:分批发送短信、迅雷多线程下载等。 总结不易，给个关注吧 https://github.com/yunlongn","link":"/2019/01/02/Java-%E9%AB%98%E7%BA%A7-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8-1/"},{"title":"Java基础知识","text":"1. 面向对象和面向过程的区别面向过程优点： 性能比面向对象高。因为类调用时需要实例化，开销比较大，比较消耗资源，所以当性能是最重要的考量因素的时候，比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发 缺点： 没有面向对象易维护、易复用、易扩展 面向对象优点： 易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统更加灵活、更加易于维护 缺点： 性能比面向过程低 2. Java 语言有哪些特点? 简单易学； 面向对象（封装，继承，多态）； 平台无关性（ Java 虚拟机实现平台无关性）； 可靠性； 安全性； 支持多线程（ C++ 语言没有内置的多线程机制，因此必须调用操作系统的多线程功能来进行多线程程序设计，而 Java 语言却提供了多线程支持）； 支持网络编程并且很方便（ Java 语言诞生本身就是为简化网络编程设计的，因此 Java 语言不仅支持网络编程而且很方便）； 编译与解释并存； 3. 关于 JVM JDK 和 JRE 最详细通俗的解答JVMJava虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。 什么是字节码?采用字节码的好处是什么? 在 Java 中，JVM可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以 Java 程序运行时比较高效，而且，由于字节码并不针对一种特定的机器，因此，Java程序无须重新编译便可在多种不同操作系统的计算机上运行。 Java 程序从源代码到运行一般有下面3步： 我们需要格外注意的是 .class-&gt;机器码 这一步。在这一步 JVM 类加载器首先加载字节码文件，然后通过解释器逐行解释执行，这种方式的执行速度会相对比较慢。而且，有些方法和代码块是经常需要被调用的(也就是所谓的热点代码)，所以后面引进了 JIT 编译器，而JIT 属于运行时编译。当 JIT 编译器完成第一次编译后，其会将字节码对应的机器码保存下来，下次可以直接使用。而我们知道，机器码的运行效率肯定是高于 Java 解释器的。这也解释了我们为什么经常会说 Java 是编译与解释共存的语言。 HotSpot采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是JIT所需要编译的部分。JVM会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。JDK 9引入了一种新的编译模式AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了JIT预热等各方面的开销。JDK支持分层编译和AOT协作使用。但是 ，AOT 编译器的编译质量是肯定比不上 JIT 编译器的。 总结：Java虚拟机（JVM）是运行 Java 字节码的虚拟机。JVM有针对不同系统的特定实现（Windows，Linux，macOS），目的是使用相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语言“一次编译，随处可以运行”的关键所在。 JDK 和 JREJDK是Java Development Kit，它是功能齐全的Java SDK。它拥有JRE所拥有的一切，还有编译器（javac）和工具（如javadoc和jdb）。它能够创建和编译程序。 JRE 是 Java运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，包括 Java虚拟机（JVM），Java类库，java命令和其他的一些基础构件。但是，它不能用于创建新程序。 如果你只是为了运行一下 Java 程序的话，那么你只需要安装 JRE 就可以了。如果你需要进行一些 Java 编程方面的工作，那么你就需要安装JDK了。但是，这不是绝对的。有时，即使您不打算在计算机上进行任何Java开发，仍然需要安装JDK。例如，如果要使用JSP部署Web应用程序，那么从技术上讲，您只是在应用程序服务器中运行Java程序。那你为什么需要JDK呢？因为应用程序服务器会将 JSP 转换为 Java servlet，并且需要使用 JDK 来编译 servlet。 4. Oracle JDK 和 OpenJDK 的对比可能在看这个问题之前很多人和我一样并没有接触和使用过 OpenJDK 。那么Oracle和OpenJDK之间是否存在重大差异？下面我通过收集到的一些资料，为你解答这个被很多人忽视的问题。 对于Java 7，没什么关键的地方。OpenJDK项目主要基于Sun捐赠的HotSpot源代码。此外，OpenJDK被选为Java 7的参考实现，由Oracle工程师维护。关于JVM，JDK，JRE和OpenJDK之间的区别，Oracle博客帖子在2012年有一个更详细的答案： 问：OpenJDK存储库中的源代码与用于构建Oracle JDK的代码之间有什么区别？ 答：非常接近 - 我们的Oracle JDK版本构建过程基于OpenJDK 7构建，只添加了几个部分，例如部署代码，其中包括Oracle的Java插件和Java WebStart的实现，以及一些封闭的源代码派对组件，如图形光栅化器，一些开源的第三方组件，如Rhino，以及一些零碎的东西，如附加文档或第三方字体。展望未来，我们的目的是开源Oracle JDK的所有部分，除了我们考虑商业功能的部分。 总结： Oracle JDK版本将每三年发布一次，而OpenJDK版本每三个月发布一次； OpenJDK 是一个参考模型并且是完全开源的，而Oracle JDK是OpenJDK的一个实现，并不是完全开源的； Oracle JDK 比 OpenJDK 更稳定。OpenJDK和Oracle JDK的代码几乎相同，但Oracle JDK有更多的类和一些错误修复。因此，如果您想开发企业/商业软件，我建议您选择Oracle JDK，因为它经过了彻底的测试和稳定。某些情况下，有些人提到在使用OpenJDK 可能会遇到了许多应用程序崩溃的问题，但是，只需切换到Oracle JDK就可以解决问题； 在响应性和JVM性能方面，Oracle JDK与OpenJDK相比提供了更好的性能； Oracle JDK不会为即将发布的版本提供长期支持，用户每次都必须通过更新到最新版本获得支持来获取最新版本； Oracle JDK根据二进制代码许可协议获得许可，而OpenJDK根据GPL v2许可获得许可。 5. Java和C++的区别?我知道很多人没学过 C++，但是面试官就是没事喜欢拿咱们 Java 和 C++ 比呀！没办法！！！就算没学过C++，也要记下来！ 都是面向对象的语言，都支持封装、继承和多态 Java 不提供指针来直接访问内存，程序内存更加安全 Java 的类是单继承的，C++ 支持多重继承；虽然 Java 的类不可以多继承，但是接口可以多继承。 Java 有自动内存管理机制，不需要程序员手动释放无用内存 6. 什么是 Java 程序的主类 应用程序和小程序的主类有何不同?一个程序中可以有多个类，但只能有一个类是主类。在 Java 应用程序中，这个主类是指包含 main（）方法的类。而在 Java 小程序中，这个主类是一个继承自系统类 JApplet 或 Applet 的子类。应用程序的主类不一定要求是 public 类，但小程序的主类要求必须是 public 类。主类是 Java 程序执行的入口点。 7. Java 应用程序与小程序之间有那些差别?简单说应用程序是从主线程启动(也就是 main() 方法)。applet 小程序没有main方法，主要是嵌在浏览器页面上运行(调用init()线程或者run()来启动)，嵌入浏览器这点跟 flash 的小游戏类似。 8. 字符型常量和字符串常量的区别? 形式上: 字符常量是单引号引起的一个字符; 字符串常量是双引号引起的若干个字符 含义上: 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置) 占内存大小 字符常量只占2个字节; 字符串常量占若干个字节(至少一个字符结束标志) (注意： char在Java中占两个字节) java编程思想第四版：2.2.2节 9. 构造器 Constructor 是否可被 override?在讲继承的时候我们就知道父类的私有属性和构造方法并不能被继承，所以 Constructor 也就不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。 10. 重载和重写的区别重载： 发生在同一个类中，方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同，发生在编译时。 重写： 发生在父子类中，方法名、参数列表必须相同，返回值范围小于等于父类，抛出的异常范围小于等于父类，访问修饰符范围大于等于父类；如果父类方法访问修饰符为 private 则子类就不能重写该方法。 11. Java 面向对象编程三大特性: 封装 继承 多态封装封装把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果属性不想被外界访问，我们大可不必提供方法给外界访问。但是如果一个类没有提供给外界访问的方法，那么这个类也没有什么意义了。 继承继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承我们能够非常方便地复用以前的代码。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。（以后介绍）。 多态所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。 在Java中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。 12. String StringBuffer 和 StringBuilder 的区别是什么? String 为什么是不可变的?可变性 简单的来说：String 类中使用 final 关键字修饰字符数组来保存字符串，private final char value[]，所以 String 对象是不可变的。而StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串char[]value 但是没有用 final 关键字修饰，所以这两种对象都是可变的。 StringBuilder 与 StringBuffer 的构造方法都是调用父类构造方法也就是 AbstractStringBuilder 实现的，大家可以自行查阅源码。 AbstractStringBuilder.java 12345678abstract class AbstractStringBuilder implements Appendable, CharSequence { char[] value; int count; AbstractStringBuilder() { } AbstractStringBuilder(int capacity) { value = new char[capacity]; } 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用String 单线程操作字符串缓冲区下操作大量数据: 适用StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用StringBuffer 13. 自动装箱与拆箱装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 14. 在一个静态方法内调用一个非静态成员为什么是非法的?由于静态方法可以不通过对象进行调用，因此在静态方法里，不能调用其他非静态变量，也不可以访问非静态变量成员。 15. 在 Java 中定义一个不做事且没有参数的构造方法的作用Java 程序在执行子类的构造方法之前，如果没有用 super() 来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。因此，如果父类中只定义了有参数的构造方法，而在子类的构造方法中又没有用 super() 来调用父类中特定的构造方法，则编译时将发生错误，因为 Java 程序在父类中找不到没有参数的构造方法可供执行。解决办法是在父类里加上一个不做事且没有参数的构造方法。 16. import java和javax有什么区别？刚开始的时候 JavaAPI 所必需的包是 java 开头的包，javax 当时只是扩展 API 包来使用。然而随着时间的推移，javax 逐渐地扩展成为 Java API 的组成部分。但是，将扩展从 javax 包移动到 java 包确实太麻烦了，最终会破坏一堆现有的代码。因此，最终决定 javax 包将成为标准API的一部分。 所以，实际上java和javax没有区别。这都是一个名字。 17. 接口和抽象类的区别是什么？ 接口的方法默认是 public，所有方法在接口中不能有实现(Java 8 开始接口方法可以有默认实现），而抽象类可以有非抽象的方法。 接口中除了static、final变量，不能有其他变量，而抽象类中则不一定。 一个类可以实现多个接口，但只能实现一个抽象类。接口自己本身可以通过extends关键字扩展多个接口。 接口方法默认修饰符是public，抽象方法可以有public、protected和default这些修饰符（抽象方法就是为了被重写所以不能使用private关键字修饰！）。 从设计层面来说，抽象是对类的抽象，是一种模板设计，而接口是对行为的抽象，是一种行为的规范。 备注：在JDK8中，接口也可以定义静态方法，可以直接用接口名调用。实现类和实现是不可以调用的。如果同时实现两个接口，接口中定义了一样的默认方法，则必须重写，不然会报错。(详见issue:https://github.com/Snailclimb/JavaGuide/issues/146) 18. 成员变量与局部变量的区别有那些？ 从语法形式上看:成员变量是属于类的，而局部变量是在方法中定义的变量或是方法的参数；成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 从变量在内存中的存储方式来看:如果成员变量是使用static修饰的，那么这个成员变量是属于类的，如果没有使用static修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 从变量在内存中的生存时间上看:成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动消失。 成员变量如果没有被赋初值:则会自动以类型的默认值而赋值（一种情况例外被 final 修饰的成员变量也必须显示地赋值），而局部变量则不会自动赋值。 19. 创建一个对象用什么运算符?对象实体与对象引用有何不同?new运算符，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。一个对象引用可以指向0个或1个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有n个引用指向它（可以用n条绳子系住一个气球）。 20. 什么是方法的返回值?返回值在类的方法里的作用是什么?方法的返回值是指我们获取到的某个方法体中的代码执行后产生的结果！（前提是该方法可能产生结果）。返回值的作用:接收出结果，使得它可以用于其他的操作！ 21. 一个类的构造方法的作用是什么? 若一个类没有声明构造方法，该程序能正确执行吗? 为什么?主要作用是完成对类对象的初始化工作。可以执行。因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。 22. 构造方法有哪些特性？ 名字与类名相同。 没有返回值，但不能用void声明构造函数。 生成类的对象时自动执行，无需调用。 23. 静态方法和实例方法有何不同 在外部调用静态方法时，可以使用”类名.方法名”的方式，也可以使用”对象名.方法名”的方式。而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象。 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），而不允许访问实例成员变量和实例方法；实例方法则无此限制。 24. 对象的相等与指向他们的引用相等,两者有什么不同?对象的相等，比的是内存中存放的内容是否相等。而引用相等，比较的是他们指向的内存地址是否相等。 25. 在调用子类构造方法之前会先调用父类没有参数的构造方法,其目的是?帮助子类做初始化工作。 26. == 与 equals(重要)== : 它的作用是判断两个对象的地址是不是相等。即，判断两个对象是不是同一个对象(基本数据类型==比较的是值，引用数据类型==比较的是内存地址)。 equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况： 情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。 情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来比较两个对象的内容是否相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。 举个例子： 1234567891011121314151617public class test1 { public static void main(String[] args) { String a = new String(&quot;ab&quot;); // a 为一个引用 String b = new String(&quot;ab&quot;); // b为另一个引用,对象的内容一样 String aa = &quot;ab&quot;; // 放在常量池中 String bb = &quot;ab&quot;; // 从常量池中查找 if (aa == bb) // true System.out.println(&quot;aa==bb&quot;); if (a == b) // false，非同一对象 System.out.println(&quot;a==b&quot;); if (a.equals(b)) // true System.out.println(&quot;aEQb&quot;); if (42 == 42.0) { // true System.out.println(&quot;true&quot;); } }} 说明： String 中的 equals 方法是被重写过的，因为 object 的 equals 方法是比较的对象的内存地址，而 String 的 equals 方法比较的是对象的值。 当创建 String 类型的对象时，虚拟机会在常量池中查找有没有已经存在的值和要创建的值相同的对象，如果有就把它赋给当前引用。如果没有就在常量池中重新创建一个 String 对象。 27. hashCode 与 equals (重要)面试官可能会问你：“你重写过 hashcode 和 equals 么，为什么重写equals时必须重写hashCode方法？” hashCode（）介绍hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在JDK的Object.java中，这就意味着Java中的任何类都包含有hashCode() 函数。 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 为什么要有 hashCode我们先以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode： 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals（）方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自我的Java启蒙书《Head first java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 通过我们可以看出：hashCode() 的作用就是获取哈希码，也称为散列码；它实际上是返回一个int整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。**hashCode() 在散列表中才有用，在其它情况下没用。**在散列表中hashCode() 的作用是获取对象的散列码，进而确定该对象在散列表中的位置。 hashCode（）与equals（）的相关规定 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 推荐阅读：Java hashCode() 和 equals()的若干问题解答 28. 为什么Java中只有值传递？ 为什么Java中只有值传递？ 29. 简述线程、程序、进程的基本概念。以及他们之间关系是什么?线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 程序是含有指令和数据的文件，被存储在磁盘或其他的数据存储设备中，也就是说程序是静态的代码。 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。从另一角度来说，进程属于操作系统的范畴，主要是同一段时间内，可以同时执行一个以上的程序，而线程则是在同一程序内几乎同时执行一个以上的程序段。 30. 线程有哪些基本状态?Java 线程在运行的生命周期中的指定时刻只可能处于下面6种不同状态的其中一个状态（图源《Java 并发编程艺术》4.1.4节）。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示（图源《Java 并发编程艺术》4.1.4节）： 由上图可以看出： 线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 cpu 时间片（timeslice）后就处于 RUNNING（运行） 状态。 操作系统隐藏 Java虚拟机（JVM）中的 RUNNABLE 和 RUNNING 状态，它只能看到 RUNNABLE 状态（图源：HowToDoInJava：Java Thread Life Cycle and Thread States），所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 当线程执行 wait()方法之后，线程进入 WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME_WAITING(超时等待) 状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（long millis）方法或 wait（long millis）方法可以将 Java 线程置于 TIMED WAITING 状态。当超时时间到达后 Java 线程将会返回到 RUNNABLE 状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞） 状态。线程在执行 Runnable 的run()方法之后将会进入到 TERMINATED（终止） 状态。 31 关于 final 关键字的一些总结final关键字主要用在三个地方：变量、方法、类。 对于一个final变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。 当用final修饰一个类时，表明这个类不能被继承。final类中的所有成员方法都会被隐式地指定为final方法。 使用final方法的原因有两个。第一个原因是把方法锁定，以防任何继承类修改它的含义；第二个原因是效率。在早期的Java实现版本中，会将final方法转为内嵌调用。但是如果方法过于庞大，可能看不到内嵌调用带来的任何性能提升（现在的Java版本已经不需要使用final方法进行这些优化了）。类中所有的private方法都隐式地指定为final。 32 Java 中的异常处理Java异常类层次结构图 在 Java 中，所有的异常都有一个共同的祖先java.lang包中的 Throwable类。Throwable： 有两个重要的子类：Exception（异常） 和 Error（错误） ，二者都是 Java 异常处理的重要子类，各自都包含大量子类。 Error（错误）:是程序无法处理的错误，表示运行应用程序中较严重问题。大多数错误与代码编写者执行的操作无关，而表示代码运行时 JVM（Java 虚拟机）出现的问题。例如，Java虚拟机运行错误（Virtual MachineError），当 JVM 不再有继续执行操作所需的内存资源时，将出现 OutOfMemoryError。这些异常发生时，Java虚拟机（JVM）一般会选择线程终止。 这些错误表示故障发生于虚拟机自身、或者发生在虚拟机试图执行应用时，如Java虚拟机运行错误（Virtual MachineError）、类定义错误（NoClassDefFoundError）等。这些错误是不可查的，因为它们在应用程序的控制和处理能力之 外，而且绝大多数是程序运行时不允许出现的状况。对于设计合理的应用程序来说，即使确实发生了错误，本质上也不应该试图去处理它所引起的异常状况。在 Java中，错误通过Error的子类描述。 Exception（异常）:是程序本身可以处理的异常。Exception 类有一个重要的子类 RuntimeException。RuntimeException 异常由Java虚拟机抛出。NullPointerException（要访问的变量没有引用任何对象时，抛出该异常）、ArithmeticException（算术运算异常，一个整数除以0时，抛出该异常）和 ArrayIndexOutOfBoundsException （下标越界异常）。 注意：异常和错误的区别：异常能被程序本身可以处理，错误是无法处理。 Throwable类常用方法 public string getMessage():返回异常发生时的详细信息 public string toString():返回异常发生时的简要描述 public string getLocalizedMessage():返回异常对象的本地化信息。使用Throwable的子类覆盖这个方法，可以声称本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与getMessage（）返回的结果相同 public void printStackTrace():在控制台上打印Throwable对象封装的异常信息 异常处理总结 try 块：用于捕获异常。其后可接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块：用于处理try捕获到的异常。 finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。 在以下4种特殊情况下，finally块不会被执行： 在finally语句块第一行发生了异常。 因为在其他行，finally块还是会得到执行 在前面的代码中用了System.exit(int)已退出程序。 exit是带参函数 ；若该语句在异常语句之后，finally会执行 程序所在的线程死亡。 关闭CPU。 下面这部分内容来自issue:https://github.com/Snailclimb/JavaGuide/issues/190。 关于返回值： 如果try语句里有return，返回的是try语句块中变量值。详细执行过程如下： 如果有返回值，就把返回值保存到局部变量中； 执行jsr指令跳到finally语句里执行； 执行完finally语句后，返回之前保存在局部变量表里的值。 如果try，finally语句里均有return，忽略try的return，而使用finally的return. 33 Java序列化中如果有些字段不想进行序列化，怎么办？对于不想进行序列化的变量，使用transient关键字修饰。 transient关键字的作用是：阻止实例中那些用此关键字修饰的的变量序列化；当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复。transient只能修饰变量，不能修饰类和方法。 34 获取用键盘输入常用的的两种方法方法1：通过 Scanner 123Scanner input = new Scanner(System.in);String s = input.nextLine();input.close(); 方法2：通过 BufferedReader 12BufferedReader input = new BufferedReader(new InputStreamReader(System.in)); String s = input.readLine(); 35 Java 中 IO 流分为几种?BIO,NIO,AIO 有什么区别?java 中 IO 流分为几种? 按照流的流向分，可以分为输入流和输出流； 按照操作单元划分，可以划分为字节流和字符流； 按照流的角色划分为节点流和处理流。 Java Io流共涉及40多个类，这些类看上去很杂乱，但实际上很有规则，而且彼此之间存在非常紧密的联系， Java I0流的40多个类都是从如下4个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 按操作方式分类结构图： 按操作对象分类结构图 BIO,NIO,AIO 有什么区别? BIO (Blocking I/O): 同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。在活动连接数不是特别高（小于单机1000）的情况下，这种模型是比较不错的，可以让每一个连接专注于自己的 I/O 并且编程模型简单，也不用过多考虑系统的过载、限流等问题。线程池本身就是一个天然的漏斗，可以缓冲一些系统处理不了的连接或请求。但是，当面对十万甚至百万级连接的时候，传统的 BIO 模型是无能为力的。因此，我们需要一种更高效的 I/O 处理模型来应对更高的并发量。 NIO (New I/O): NIO是一种同步非阻塞的I/O模型，在Java 1.4 中引入了NIO框架，对应 java.nio 包，提供了 Channel , Selector，Buffer等抽象。NIO中的N可以理解为Non-blocking，不单纯是New。它支持面向缓冲的，基于通道的I/O操作方法。 NIO提供了与传统BIO模型中的 Socket 和 ServerSocket 相对应的 SocketChannel 和 ServerSocketChannel 两种不同的套接字通道实现,两种通道都支持阻塞和非阻塞两种模式。阻塞模式使用就像传统中的支持一样，比较简单，但是性能和可靠性都不好；非阻塞模式正好与之相反。对于低负载、低并发的应用程序，可以使用同步阻塞I/O来提升开发速率和更好的维护性；对于高负载、高并发的（网络）应用，应使用 NIO 的非阻塞模式来开发 AIO (Asynchronous I/O): AIO 也就是 NIO 2。在 Java 7 中引入了 NIO 的改进版 NIO 2,它是异步非阻塞的IO模型。异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。AIO 是异步IO的缩写，虽然 NIO 在网络操作中，提供了非阻塞的方法，但是 NIO 的 IO 行为还是同步的。对于 NIO 来说，我们的业务线程是在 IO 操作准备好时，得到通知，接着就由这个线程自行进行 IO 操作，IO操作本身是同步的。查阅网上相关资料，我发现就目前来说 AIO 的应用还不是很广泛，Netty 之前也尝试使用过 AIO，不过又放弃了。 36. 常见关键字总结:static,final,this,super详见笔主的这篇文章: https://gitee.com/SnailClimb/JavaGuide/blob/master/docs/java/Basis/final、static、this、super.md 37. Collections 工具类和 Arrays 工具类常见方法总结详见笔主的这篇文章: https://gitee.com/SnailClimb/JavaGuide/blob/master/docs/java/Basis/Arrays,CollectionsCommonMethods.md 参考 https://stackoverflow.com/questions/1906445/what-is-the-difference-between-jdk-and-jre https://www.educba.com/oracle-vs-openjdk/ https://stackoverflow.com/questions/22358071/differences-between-oracle-jdk-and-openjdk?answertab=active#tab-top","link":"/2019/05/22/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"title":"Java源码之List,Set,Map","text":"Java原理之List,Set,Map 1、Java8对Java7的HashMap做了修改，最大的区别就是利用了红黑树。 2、Java7的结构中，查找数据的时候，我们会根据hash值快速定位到数组的具体下标。但是后面是需要通过链表去遍历数据，所以查询的速度就依赖于链表的长度，时间复杂度也自然是O(n) 3、为了减少2中出现的问题，在Java8中，当链表的个数大于8的时候，就会把链表转化为红黑树。那么在红黑树查找数据的时候，时间复杂度就变味了O(logN) 说说List,Set,Map三者的区别？ List(对付顺序的好帮手)： List接口存储一组不唯一（可以有多个元素引用相同的对象），有序的对象 Set(注重独一无二的性质): 不允许重复的集合。不会有多个元素引用相同的对象。 Map(用Key来搜索的专家): 使用键值对存储。Map会维护与Key有关联的值。两个Key可以引用相同的对象，但Key不能重复，典型的Key是String类型，但也可以是任何对象。 Arraylist 与 LinkedList 区别? 1. 是否保证线程安全： ArrayList 和 LinkedList 都是不同步的，也就是不保证线程安全； 2. 底层数据结构： Arraylist 底层使用的是Object数组；LinkedList 底层使用的是双向链表数据结构（JDK1.6之前为循环链表，JDK1.7取消了循环。注意双向链表和双向循环链表的区别，下面有介绍到！） 3. 插入和删除是否受元素位置的影响： ① ArrayList 采用数组存储，所以插入和删除元素的时间复杂度受元素位置的影响。 比如：执行add(E e) 方法的时候， ArrayList 会默认在将指定的元素追加到此列表的末尾，这种情况时间复杂度就是O(1)。但是如果要在指定位置 i 插入和删除元素的话（add(int index, E element) ）时间复杂度就为 O(n-i)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。 ② LinkedList 采用链表存储，所以插入，删除元素时间复杂度不受元素位置的影响，都是近似 O（1）而数组为近似 O（n）。 4. 是否支持快速随机访问： LinkedList 不支持高效的随机元素访问，而 ArrayList 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index) 方法)。 5. 内存空间占用： ArrayList的空 间浪费主要体现在在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（因为要存放直接后继和直接前驱以及数据）。 补充内容:RandomAccess接口12public interface RandomAccess {} 查看源码我们发现实际上 RandomAccess 接口中什么都没有定义。所以，在我看来 RandomAccess 接口不过是一个标识罢了。标识什么？ 标识实现这个接口的类具有随机访问功能。 在binarySearch（）方法中，它要判断传入的list 是否RamdomAccess的实例，如果是，调用indexedBinarySearch（）方法，如果不是，那么调用iteratorBinarySearch（）方法 1234567public static &lt;T&gt;int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) { if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key);} ArrayList 实现了 RandomAccess 接口， 而 LinkedList 没有实现。为什么呢？我觉得还是和底层数据结构有关！ArrayList 底层是数组，而 LinkedList 底层是链表。数组天然支持随机访问，时间复杂度为 O（1），所以称为快速随机访问。链表需要遍历到特定位置才能访问特定位置的元素，时间复杂度为 O（n），所以不支持快速随机访问。，ArrayList 实现了 RandomAccess 接口，就表明了他具有快速随机访问功能。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！ 下面再总结一下 list 的遍历方式选择： 实现了RandomAccess接口的list，优先选择普通for循环 ，其次foreach, 未实现RandomAccess接口的ist， 优先选择iterator遍历（foreach遍历底层也是通过iterator实现的），大size的数据，千万不要使用普通for循环 补充内容:双向链表和双向循环链表双向链表： 包含两个指针，一个prev指向前一个节点，一个next指向后一个节点。 双向循环链表： 最后一个节点的 next 指向head，而 head 的prev指向最后一个节点，构成一个环。 ArrayList 与 Vector 区别呢?为什么要用Arraylist取代Vector呢？Vector类的所有方法都是同步的。可以由两个线程安全地访问一个Vector对象、但是一个线程访问Vector的话代码要在同步操作上耗费大量的时间。 Arraylist不是同步的，所以在不需要保证线程安全时时建议使用Arraylist。 说一说 ArrayList 的扩容机制吧详见笔主的这篇文章:通过源码一步一步分析ArrayList 扩容机制 HashMap 和 Hashtable 的区别 线程是否安全： HashMap 是非线程安全的，HashTable 是线程安全的；HashTable 内部的方法基本都经过synchronized 修饰。（如果你要保证线程安全的话就使用 ConcurrentHashMap 吧！）； 效率： 因为线程安全的问题，HashMap 要比 HashTable 效率高一点。另外，HashTable 基本被淘汰，不要在代码中使用它； 对Null key 和Null value的支持： HashMap 中，null 可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为 null。。但是在 HashTable 中 put 进的键值只要有一个 null，直接抛出 NullPointerException。 初始容量大小和每次扩充容量大小的不同 ： ①创建时如果不指定容量初始值，Hashtable 默认的初始大小为11，之后每次扩充，容量变为原来的2n+1。HashMap 默认的初始化大小为16。之后每次扩充，容量变为原来的2倍。②创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为2的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用2的幂作为哈希表的大小,后面会介绍到为什么是2的幂次方。 底层数据结构： JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。Hashtable 没有这样的机制。 HasMap 中带有初始容量的构造函数： 123456789101112131415public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);} public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR);} 下面这个方法保证了 HashMap 总是使用2的幂作为哈希表的大小。 123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;} HashMap 和 HashSet区别如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone() 、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 HashMap HashSet 实现了Map接口 实现Set接口 存储键值对 仅存储对象 调用 put（）向map中添加元素 调用 add（）方法向Set中添加元素 HashMap使用键（Key）计算Hashcode HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性， HashSet如何检查重复当你把对象加入HashSet时，HashSet会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的hashcode值作比较，如果没有相符的hashcode，HashSet会假设对象没有重复出现。但是如果发现有相同hashcode值的对象，这时会调用equals（）方法来检查hashcode相等的对象是否真的相同。如果两者相同，HashSet就不会让加入操作成功。（摘自我的Java启蒙书《Head fist java》第二版） hashCode（）与equals（）的相关规定： 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个equals方法返回true 两个对象有相同的hashcode值，它们也不一定是相等的 综上，equals方法被覆盖过，则hashCode方法也必须被覆盖 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写hashCode()，则该class的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）。 ==与equals的区别 ==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同 ==是指对内存地址进行比较 equals()是对字符串的内容进行比较 ==指引用是否相同 equals()指的是值是否相同 HashMap的底层实现JDK1.8之前 推荐一个很不错的源码分析文章 https://www.javadoop.com/post/hashmap JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 1234567 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 对比一下 JDK1.7的 HashMap 的 hash 方法源码. 12345678static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);} 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 [ JDK1.8之后相比于之前的版本， JDK1.8之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。 [ TreeMap、TreeSet以及JDK1.8之后的HashMap底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 推荐阅读： 《Java 8系列之重新认识HashMap》 ：https://zhuanlan.zhihu.com/p/21673805 HashMap 的长度为什么是2的幂次方为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648到2147483647，前后加起来大概40亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个40亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的。用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) &amp; hash”。（n代表数组长度）。这也就解释了 HashMap 的长度为什么是2的幂次方。 这个算法应该如何设计呢？ 我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是2的幂次则等价于与其除数减一的与(&amp;)操作（也就是说 hash%length==hash&amp;(length-1)的前提是 length 是2的 n 次方；）。” 并且 采用二进制位操作 &amp;，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是2的幂次方。 HashMap 多线程操作导致死循环问题主要原因在于 并发下的Rehash 会造成元素之间会形成一个循环链表。不过，jdk 1.8 后解决了这个问题，但是还是不建议在多线程下使用 HashMap,因为多线程下使用 HashMap 还是会存在其他问题比如数据丢失。并发环境下推荐使用 ConcurrentHashMap 。 详情请查看：https://coolshell.cn/articles/9606.html ConcurrentHashMap 和 Hashtable 的区别ConcurrentHashMap 和 Hashtable 的区别主要体现在实现线程安全的方式上不同。 底层数据结构： JDK1.7的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要）： ① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。 两者的对比图： 图片来源：http://www.cnblogs.com/chengxiao/p/6842045.html HashTable: [ JDK1.7的ConcurrentHashMap： [JDK1.8的ConcurrentHashMap（TreeBin: 红黑二叉树节点 Node: 链表节点）：[ ConcurrentHashMap线程安全的具体实现方式/底层具体实现JDK1.7（上面有示意图）首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据时，其他段的数据也能被其他线程访问。 ConcurrentHashMap 是由 Segment 数组结构和 HashEntry 数组结构组成。 Segment 实现了 ReentrantLock,所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。 12static class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable {} 一个 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。 JDK1.8 （上面有示意图）ConcurrentHashMap取消了Segment分段锁，采用CAS和synchronized来保证并发安全。数据结构跟HashMap1.8的结构类似，数组+链表/红黑二叉树。Java 8在链表长度超过一定阈值（8）时将链表（寻址时间复杂度为O(N)）转换为红黑树（寻址时间复杂度为O(long(N))） synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发，效率又提升N倍。 comparable 和 Comparator的区别 comparable接口实际上是出自java.lang包 它有一个 compareTo(Object obj)方法用来排序 comparator接口实际上是出自 java.util 包它有一个compare(Object obj1, Object obj2)方法用来排序 一般我们需要对一个集合使用自定义排序时，我们就要重写compareTo()方法或compare()方法，当我们需要对某一个集合实现两种排序方式，比如一个song对象中的歌名和歌手名分别采用一种排序方法的话，我们可以重写compareTo()方法和使用自制的Comparator方法或者以两个Comparator来实现歌名排序和歌星名排序，第二种代表我们只能使用两个参数版的 Collections.sort(). Comparator定制排序123456789101112131415161718192021222324252627282930ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;();arrayList.add(-1);arrayList.add(3);arrayList.add(3);arrayList.add(-5);arrayList.add(7);arrayList.add(4);arrayList.add(-9);arrayList.add(-7);System.out.println(&quot;原始数组:&quot;);System.out.println(arrayList);// void reverse(List list)：反转Collections.reverse(arrayList);System.out.println(&quot;Collections.reverse(arrayList):&quot;);System.out.println(arrayList);// void sort(List list),按自然排序的升序排序Collections.sort(arrayList);System.out.println(&quot;Collections.sort(arrayList):&quot;);System.out.println(arrayList);// 定制排序的用法Collections.sort(arrayList, new Comparator&lt;Integer&gt;() { @Override public int compare(Integer o1, Integer o2) { return o2.compareTo(o1); }});System.out.println(&quot;定制排序后：&quot;);System.out.println(arrayList); Output: 12345678原始数组:[-1, 3, 3, -5, 7, 4, -9, -7]Collections.reverse(arrayList):[-7, -9, 4, 7, -5, 3, 3, -1]Collections.sort(arrayList):[-9, -7, -5, -1, 3, 3, 4, 7]定制排序后：[7, 4, 3, 3, -1, -5, -7, -9] 重写compareTo方法实现按年龄来排序123456789101112131415161718192021222324252627282930313233343536373839404142434445// person对象没有实现Comparable接口，所以必须实现，这样才不会出错，才可以使treemap中的数据按顺序排列// 前面一个例子的String类已经默认实现了Comparable接口，详细可以查看String类的API文档，另外其他// 像Integer类等都已经实现了Comparable接口，所以不需要另外实现了public class Person implements Comparable&lt;Person&gt; { private String name; private int age; public Person(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } /** * TODO重写compareTo方法实现按年龄来排序 */ @Override public int compareTo(Person o) { // TODO Auto-generated method stub if (this.age &gt; o.getAge()) { return 1; } else if (this.age &lt; o.getAge()) { return -1; } return age; }} 12345678910111213public static void main(String[] args) { TreeMap&lt;Person, String&gt; pdata = new TreeMap&lt;Person, String&gt;(); pdata.put(new Person(&quot;张三&quot;, 30), &quot;zhangsan&quot;); pdata.put(new Person(&quot;李四&quot;, 20), &quot;lisi&quot;); pdata.put(new Person(&quot;王五&quot;, 10), &quot;wangwu&quot;); pdata.put(new Person(&quot;小红&quot;, 5), &quot;xiaohong&quot;); // 得到key的值的同时得到key所对应的值 Set&lt;Person&gt; keys = pdata.keySet(); for (Person key : keys) { System.out.println(key.getAge() + &quot;-&quot; + key.getName()); }} Output： 12345-小红10-王五20-李四30-张三 集合框架底层数据结构总结Collection1. List Arraylist： Object数组 Vector： Object数组 LinkedList： 双向链表(JDK1.6之前为循环链表，JDK1.7取消了循环) 详细可阅读JDK1.7-LinkedList循环链表优化 2. Set HashSet（无序，唯一）: 基于 HashMap 实现的，底层采用 HashMap 来保存元素 LinkedHashSet： LinkedHashSet 继承与 HashSet，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的LinkedHashMap 其内部是基于 Hashmap 实现一样，不过还是有一点点区别的。 TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树。) Map HashMap： JDK1.8之前HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间 LinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：《LinkedHashMap 源码详细分析（JDK1.8）》 Hashtable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的 TreeMap： 红黑树（自平衡的排序二叉树） 如何选用集合?集合的选用主要根据集合的特点来选用，比如我们需要根据键值获取到元素值时就选用Map接口下的集合，需要排序时选择TreeMap,不需要排序时就选择HashMap,需要保证线程安全就选用ConcurrentHashMap.当我们只需要存放元素值时，就选择实现Collection接口的集合，需要保证元素唯一时选择实现Set接口的集合比如TreeSet或HashSet，不需要就选择实现List接口的比如ArrayList或LinkedList，然后再根据实现这些接口的集合的特点来选用。","link":"/2019/05/24/Java%E6%BA%90%E7%A0%81%E4%B9%8BList-Set-Map/"},{"title":"Java编程技巧","text":"这世上有三样东西是别人抢不走的：一是吃进胃里的食物，二是藏在心中的梦想，三是读进大脑的书 如何在整数左填充0问题如何在整数左填充0举例 1 = “0001” 答案一，String.format1String.format(&quot;%05d&quot;, yournumber); 用0填充，总长度为5https://docs.oracle.com/javase/8/docs/api/java/util/Formatter.html 答案二，ApacheCommonsLanguage如果需要在Java 1.5前使用，可以利用 Apache Commons Language 方法 1org.apache.commons.lang.StringUtils.leftPad(String str, int size, '0') 答案三，DecimalFormat12345678910import java.text.DecimalFormat;class TestingAndQualityAssuranceDepartment{ public static void main(String [] args) { int x=1; DecimalFormat df = new DecimalFormat(&quot;00&quot;); System.out.println(df.format(x)); }} 答案四，自己实现如果效率很重要的话，相比于 String.format 函数的可以自己实现 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @param in The integer value * @param fill The number of digits to fill * @return The given value left padded with the given number of digits */public static String lPadZero(int in, int fill){ boolean negative = false; int value, len = 0; if(in &gt;= 0){ value = in; } else { negative = true; value = - in; in = - in; len ++; } if(value == 0){ len = 1; } else{ for(; value != 0; len ++){ value /= 10; } } StringBuilder sb = new StringBuilder(); if(negative){ sb.append('-'); } for(int i = fill; i &gt; len; i--){ sb.append('0'); } sb.append(in); return sb.toString(); } 效率对比 12345678910111213141516171819202122public static void main(String[] args) { Random rdm; long start; // Using own function rdm = new Random(0); start = System.nanoTime(); for(int i = 10000000; i != 0; i--){ lPadZero(rdm.nextInt(20000) - 10000, 4); } System.out.println(&quot;Own function: &quot; + ((System.nanoTime() - start) / 1000000) + &quot;ms&quot;); // Using String.format rdm = new Random(0); start = System.nanoTime(); for(int i = 10000000; i != 0; i--){ String.format(&quot;%04d&quot;, rdm.nextInt(20000) - 10000); } System.out.println(&quot;String.format: &quot; + ((System.nanoTime() - start) / 1000000) + &quot;ms&quot;);} 结果 自己的实现：1697ms String.format：38134ms 答案，Google GuavaMaven： 12345&lt;dependency&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;version&gt;14.0.1&lt;/version&gt;&lt;/dependency&gt; 样例： 12Strings.padStart(&quot;7&quot;, 3, '0') returns &quot;007&quot;Strings.padStart(&quot;2020&quot;, 3, '0') returns &quot;2020&quot; 注意：Guava 是非常有用的库，它提供了很多有用的功能，包括了Collections, Caches, Functional idioms, Concurrency, Strings, Primitives, Ranges, IO, Hashing, EventBus等 如何用一行代码初始化一个ArrayList问题为了测试，我需要临时快速创建一个list。一开始我这样做： 1234ArrayList&lt;String&gt; places = new ArrayList&lt;String&gt;();places.add(&quot;Buenos Aires&quot;);places.add(&quot;Córdoba&quot;);places.add(&quot;La Plata&quot;); 之后我重构了下 12ArrayList&lt;String&gt; places = new ArrayList&lt;String&gt;( Arrays.asList(&quot;Buenos Aires&quot;, &quot;Córdoba&quot;, &quot;La Plata&quot;)); 是否有更加简便的方法呢？ 回答常见方式实际上，也许“最好”的方式，就是你写的这个方式，因为它不用再创建新的List: 1234ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();list.add(&quot;A&quot;);list.add(&quot;B&quot;);list.add(&quot;C&quot;); 只是这个方式看上去要多写些代码，让人郁闷 匿名内部类当然，还有其他方式，例如,写一个匿名内部类，然后在其中做初始化（也被称为 brace initialization）： 12345ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;() {{ add(&quot;A&quot;); add(&quot;B&quot;); add(&quot;C&quot;);}}; 但是，我不喜欢这个方式。只是为了做个初始化，却要在ArrayList的同一行后面加这么一坨代码。 Arrays.asList1List&lt;String&gt; places = Arrays.asList(&quot;Buenos Aires&quot;, &quot;Córdoba&quot;, &quot;La Plata&quot;); Collections.singletonList1List&lt;String&gt; places = Collections.singletonList(&quot;Buenos Aires&quot;); 注意：后面的这两种方式，得到的是一个定长的List(如果add操作会抛异常）。如果你需要一个不定长的List,可以这样做： 12ArrayList&lt;String&gt; places = new ArrayList&lt;&gt;(Arrays.asList(&quot;Buenos Aires&quot;, &quot;Córdoba&quot;, &quot;La Plata&quot;)); 为什么在java中存放密码更倾向于char[]而不是String问题在Swing中，password字段有一个getPassword()方法（返回char[]），而不是通常的getText()方法(返回String字符串)。同样的，我看到一个建议说不要使用字符串处理密码。为什么在涉及passwords时，都说字符串会对安全构成威胁？感觉使用char[]不是那么的方便。 回答String是不可变的。虽然String加载密码之后可以把这个变量扔掉，但是字符串并不会马上被GC回收，一但进程在GC执行到这个字符串之前被dump，dump出的的转储中就会含有这个明文的字符串。那如果我去“修改”这个字符串，比如把它赋一个新值，那么是不是就没有这个问题了？答案是否定的，因为String本身是不可修改的，任何基于String的修改函数都是返回一个新的字符串，原有的还会在内存里。 然而对于数组，你可以在抛弃它之前直接修改掉它里面的内容或者置为乱码，密码就不会存在了。但是如果你什么也不做直接交给gc的话，也会存在上面一样的问题。 所以，这是一个安全性的问题–但是，即使使用char[]也仅仅是降低了攻击者攻击的机会，而且仅仅对这种特定的攻击有效。 初始化静态map问题怎么在Java中初始化一个静态的map 我想到的两种方法如下，大家是否有更好的建议呢？ 方法一：static初始化器 方法二：实例初始化（匿名子类） 下面是描述上面两种方法的例子 12345678910111213141516import java.util.HashMap;import java.util.Map;public class Test{ private static final Map&lt;Integer, String&gt; myMap = new HashMap&lt;Integer, String&gt;(); static { myMap.put(1, &quot;one&quot;); myMap.put(2, &quot;two&quot;); } private static final Map&lt;Integer, String&gt; myMap2 = new HashMap&lt;Integer, String&gt;(){ { put(1, &quot;one&quot;); put(2, &quot;two&quot;); } };} 答案答案1匿名子类初始化器是java的语法糖，我搞不明白为什么要用匿名子类来初始化，而且，如果类是final的话，它将不起作用 我使用static初始化器来创建一个固定长度的静态map 123456789public class Test{ private static final Map&lt;Integer, String&gt; myMap; static{ Map&lt;Integer, String&gt; aMap = ...; aMap.put(1,&quot;one&quot;); aMap.put(2,&quot;two&quot;); myMap = Collections.unmodifiableMap(aMap); }} 答案2我喜欢用Guava（是 Collection 框架的增强）的方法初始化一个静态的，不可改变的map 12345static final Map&lt;Integer, String&gt; MY_MAP = ImmutableMap.of( 1, &quot;one&quot;, 2, &quot;two&quot;) 当map的 entry个数超过5个时，你就不能使用ImmutableMap.of。可以试试ImmutableMap.bulider() 1234567static final Map&lt;Integer, String&gt; MY_MAP = ImmutableMap.&lt;Integer, String&gt;builder() .put(1, &quot;one&quot;) .put(2, &quot;two&quot;) // ... .put(15, &quot;fifteen&quot;) .build(); 给3个布尔变量，当其中有2个或者2个以上为true才返回true问题给3个boolean变量，a,b,c，当其中有2个或2个以上为true时才返回true？ 最笨的方法： 1234567891011boolean atLeastTwo(boolean a, boolean b, boolean c) { if ((a &amp;&amp; b) || (b &amp;&amp; c) || (a &amp;&amp; c)) { return true; } else { return false; }} 优雅解法1 1return a ? (b || c) : (b &amp;&amp; c); 优雅解法2 1return (a==b) ? a : c; 优雅解法3 1return a ^ b ? c : a 优雅解法4 1return a ? (b || c) : (b &amp;&amp; c); 获取完整的堆栈信息问题捕获了异常后，如何获取完整的堆栈轨迹（stack trace） 回答1String fullStackTrace = org.apache.commons.lang.exception.ExceptionUtils.getFullStackTrace(e) 1Thread.currentThread().getStackTrace();","link":"/2019/05/22/Java%E7%BC%96%E7%A8%8B%E6%8A%80%E5%B7%A7/"},{"title":"MyBatis二级缓存使用","text":"注意点： 在最新的3.x版本，实现二级缓存的配置也有了一些改变。 官方建议在service使用缓存，但是你也可以直接在mapper层缓存，这里的二级缓存就是直接在Mapper层进行缓存操作 Mybatis的二级缓存实现也十分简单，只要在springboot的配置文件打开二级缓存，即123mybatis-plus: configuration: cache-enabled: true 缓存接口的实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class MybatisRedisCache implements Cache { // 读写锁 private final ReadWriteLock readWriteLock = new ReentrantReadWriteLock(true); //这里使用了redis缓存，使用springboot自动注入 @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; private String id; public MybatisRedisCache(final String id) { if (id == null) { throw new IllegalArgumentException(&quot;Cache instances require an ID&quot;); } this.id = id; } @Override public String getId() { return this.id; } @Override public void putObject(Object key, Object value) { if (redisTemplate == null) { //由于启动期间注入失败，只能运行期间注入，这段代码可以删除 redisTemplate = (RedisTemplate&lt;String, Object&gt;) ApplicationContextRegister.getApplicationContext().getBean(&quot;RedisTemplate&quot;); } if (value != null) { redisTemplate.opsForValue().set(key.toString(), value); } } @Override public Object getObject(Object key) { try { if (key != null) { return redisTemplate.opsForValue().get(key.toString()); } } catch (Exception e) { log.error(&quot;缓存出错 &quot;); } return null; } @Override public Object removeObject(Object key) { if (key != null) { redisTemplate.delete(key.toString()); } return null; } @Override public void clear() { log.debug(&quot;清空缓存&quot;); if (redisTemplate == null) { redisTemplate = (RedisTemplate&lt;String, Object&gt;) ApplicationContextRegister.getApplicationContext().getBean(&quot;functionDomainRedisTemplate&quot;); } Set&lt;String&gt; keys = redisTemplate.keys(&quot;*:&quot; + this.id + &quot;*&quot;); if (!CollectionUtils.isEmpty(keys)) { redisTemplate.delete(keys); } } @Override public int getSize() { Long size = redisTemplate.execute((RedisCallback&lt;Long&gt;) RedisServerCommands::dbSize); return size.intValue(); } @Override public ReadWriteLock getReadWriteLock() { return this.readWriteLock; }} mapper.xml文件声明缓存，这里3.x只需要这样配置1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.zsy.mapper.CarouselMapper&quot;&gt; &lt;cache-ref namespace=&quot;com.zsy.mapper.CarouselMapper&quot;/&gt;&lt;/mapper&gt; Mapper接口使用注解123@Repository@CacheNamespace(implementation=MybatisRedisCache.class,eviction=MybatisRedisCache.class)public interface CarouselMapper extends BaseMapper&lt;Carousel&gt; {}","link":"/2019/03/27/MyBatis-plus%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%E4%BD%BF%E7%94%A8/"},{"title":"MySQL一：架构体系","text":"我们一般都不会去操作数据库本身，「而是通过SQL语句调用MySQL，由MySQL处理并返回执行结果」。那么SQL语句是如何执行sql语句的呢？ Sql语句执行过程图解： 从图中可以看出sql的执行基本上分为五步: 「建立连接（Connectors&amp;Connection Pool）」 「查询缓存（Cache&amp;Buffer）」 「解析器（Parser）」 「预处理器（preprocessor）」 「查询优化器（Optimizer）」 「操作引擎执行 SQL 语句」 一、建立连接（Connectors&amp;Connection Pool）当客户端发送请求时，「客户端与服务器首先通过通信协议与MySQL的connectors建立连接。\\「之后MySQL将收到请求」*暂存在连接池（connection pool)中，由处理器（Management Serveices &amp; Utilities）管理。*「当该」**请求从等待队列进入到处理队列，管理器会将该请求丢给SQL接口（SQL Interface）。」** 「MySQL客户端与服务端的通信方式是【半双工】」。所以对于每一个 MySQL 的连接，时刻都有一个【线程状态】来标识这个连接正在做什么。 「通讯机制」 「全双工」：能同时发送和接收数据。例如平时打电话。 全双工通信允许数据同时进行双向传输，即有两个信道。 全双工通信是两个单工通信方式的结合，要求收发双方都有独立的接收和发送能力。 「半双工」：指的某一时刻，要么发送数据，要么接收数据，不能同时。例如早期对讲机 半双工通信允许信号在两个方向上传输，但某一时刻只允许信号在一个信道上单向传输。 半双工通信实际上是一种可切换方向的单工通信。 「单工」：只能发送数据或只能接收数据。例如单行道 单工通信只支持信号在一个方向上传输（正向或反向），任何时候不能改变信号的传输方向。 「线程状态」 线程状态可以查看用户正在运行的线程信息，查询命令： 1show processlist; #root用户能查看所有线程，其他用户只能看自己的 如图可以获取线程的详细信息： Id：线程 ID ，结束线程可以使用以下命令 1kill id User：启动这个线程的用户； Host：发送请求的客户端的 IP 和端口号； db：当前命令在哪个库执行； Command：该线程正在执行的操作命令 Create DB：正在创建库操作； Drop DB：正在删除库操作； Execute：正在执行一个 PreparedStatement； Close Stmt：正在关闭一个 PreparedStatement； Query：正在执行一个语句； Sleep：正在等待客户端发送语句； Quit：正在退出； Shutdown：正在关闭服务器 Time：表示该线程处于当前状态的时间，单位是秒。 State：线程状态 Updating：正在搜索匹配记录，进行修改； Sleeping：正在等待客户端发送新请求； Starting：正在执行请求处理； Checking table：正在检查数据表； Closing table：正在将表中数据刷新到磁盘中； Locked：被其他线程锁住了记录； Sending Data：正在处理 Select 查询，同时将结果返回发送给客户端； Info：一般记录线程执行的语句，默认显示前100个字符。想看完整语句可以使用 1show full processlist; 二、查询缓存（Cache&amp;Buffer）在sql接口接收到请求后，首先会在查询缓存中查找结果 开启查询缓存后，在查询缓存中找到完全相同的SQL语句，则将查询结果直接返回给客户端。 没有开启查询缓存或者没有找到完全相同的 SQL 语句。则会由解析器进行语法语义解析，并生成【解析树】。 2.1 开启查询缓存 「连接mysql服务」 12#mysql -h ip -u 用户 -P 端口 -p mysql -h 127.0.0.1 -u root -P 3306 -p 「查看是否开启查询缓存」 1show variables like &quot;%query_cache%&quot;; 上图中【query_cache_type】代表是否开启查询缓存 「0代表关闭查询缓存（显示OFF）」 「1表示始终开启查询缓存（显示ON）」 【当sql语句中有sql_no_cache关键词时不使用缓存】 如：select sql_no_cache user_name from users where user_id = ‘100’; 「2表示按需开启查询缓存（显示DEMAND）」 【代表当sql语句中有SQL_CACHE关键词时才缓存】 如：select SQL_CACHE user_name from users where user_id = ‘100’; 「开启查询缓存」 12345678#第一步：查询my.cnf的所在目录[root@localhost www]# find / -name my.cnf/etc/my.cnf#第二步：编辑my.cnf 添加query_cache_type=1[root@localhost www]# vim /etc/my.cnfquery_cache_type=1#第三步：重启mysql 生效[root@localhost www]# systemctl restart mysqld 修改配置文件my.cnf开启查询缓存 设置变量开启查询缓存 1set global query_cache_type = 1; 2.2 查询缓存失效当sql语句满足下面条件时，即使开启查询缓存，以下SQL也不能缓存： 查询语句使用 SQL_NO_CACHE 查询的结果大于 query_cache_limit 设置 查询中有一些不确定的参数，比如 now() 三、解析器（Parser）「当查询缓存没有命中时，客户端发送的SQL会被解析器进行语法解析，生成【解析树】，对sqlj进行语法分析」，生成解析树如下： 四、预处理器（preprocessor）「当解析器的语法分析通过之后，预处理器会检查生成的解析树，解决解析器无法解析的语义」。它会检查表和列名是否存在，检查名字和别名，判断解析树的语义是否正确。预处理之后得到一个新的解析树。 这里留下一个问题，留待以后： mysql预处理器和解析器生成的解析树有什么区别？ 解析器只是语法解析 会生成一个解析树 预处理会检查表名 字段 权限啥的 然后生成一个新的解析树， 预处理会检查表名 字段 权限如果有问题 就直接报错了，那还生成解析树干啥？要是没报错，那他生成的解析树跟解析器的又有啥区别呢？ 五、查询优化器「当预处理器生成新的解析树之后，会进入优化器阶段，一条SQL的执行方式有很多种，数据库最终执行的SQL也不是客户端发送的SQL，但是最终返回的结果是相同的」。优化器会帮助我们选择一个最优的方式去执行sql. 比如： 表里面有多个索引的时候，决定使用哪个索引； 在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 5.1 查询优化器的作用「查询优化器根据【解析树】生成不同的执行计划，然后选择一种最优的执行计划。MySQL是基于开销（cost）的优化器，选择使用开销最小的执行计划」。 12#查看查询的开销show status like 'Last_query_cost'; 「执行计划」 「mysql不会生成查询字节码来执行查询，而是生成查询的一棵指令树，然后通过存储引擎执行完成这棵指令树并返回结果。最终的执行计划包含了重构查询的全部信息」。 5.2 生成优执行计划策略MySQL生成最优执行计划的有很多种策略，可以分为两类：静态优化（编译时优化）、动态优化（运行时优化） 5.2.1 静态优化（编译时优化）「静态优化可以直接对解析树进行分析,并且完成优化」。优化器可以通过简单的代数变换将where 条件转换成另一种等价形式,「静态优化不依赖于特别的数值」 如where 条件中带入的一些常数等.静态优化再第一次完成后就一直有效,即使使用不同的参数重复执行查询也不会发生变化 「等价变换策略」 1=1 and a&gt;1 优化后：a &gt; 1 a &lt; b and a=5 优化后 b&gt;5 and a=5 基于联合索引，调整条件位置 5.2.2 动态优化（运行时优化） 「优化count、min、max等函数」 InnoDB引擎min函数只需要找索引最左边 InnoDB引擎max函数只需要找索引最右边 MyISAM引擎count(*)，不需要计算，直接返回 「提前终止查询」 使用了limit查询，获取limit所需的数据，就不在继续遍历后面数据 「in优化」 MySQL对in查询，会先进行排序，再采用二分法查找数据。比如where id in (2,1,3)，变 成 in (1,2,3) 六、操作引擎执行 SQL 语句「在优化器生成最优的执行计划之后，会生成指令集，mysql会根据执行计划给出的指令逐步执行，并存储引擎的类型，调用对应的API接口与底层存储引擎缓存或者物理文件的交互，直到完成所有的数据查询。」 若开启用查询缓存 会将SQL语句和结果完整地保存到查询缓存（Cache&amp;Buffer）中，以后若有相同的SQL语句执行则直接返回结果。 返回结果采用增量模式返回 服务器端不需要存储太多的结果，不会消耗过多内存。 客户端可以更快的获得返回的结果。 当查询不需要返回结果给客户端时，mysql仍然会返回这个查询的其他信息，如行数等 七、总结回顾一下mysql的运行机制的整体流程 首先客户端的请求会通过mysql的connectors与其进行连接，连接后请求会暂存在连接池（connection pool)中，由处理器（Management Serveices &amp; Utilities）管理。 当请求从「等待队列进入到处理队列」，管理器会将该请求丢给SQL接口（SQL Interface）。 SQL接口接收到请求后，会将请求进行hash处理并与缓存中的结果进行对比。如果匹配则返回缓存中的结果，否则解释器处理。 解释器接收SQL接口的请求，判断SQL语句语法是否正确，生成解析树。 解释器处理完，由预处理器校验权限，表名，字段名等信息。 优化器对针对最终的解析树产生多种执行计划，并选择最优的执行计划。 确定最优执行计划后，SQL语句交由存储引擎处理，存储引擎会在存储设备中取得相应的数据，并原路返回给客户端。","link":"/2021/08/06/MySQL%E4%B8%80%EF%BC%9A%E6%9E%B6%E6%9E%84%E4%BD%93%E7%B3%BB/"},{"title":"MySQL三：存储引擎","text":"一、MySQL存储引擎概述「数据库存储引擎是数据库底层软件组织，数据库管理系统（DBMS）使用数据引擎进行创建、查询、更新和删除数据」。不同的存储引擎提供不同的存储机制、索引、锁等功能。许多数据库管理系统都支持多种不同的数据引擎。 在关系数据库中数据的存储是以表的形式存储的，所以「存储引擎也可以称为表类型(Table Type，即存储和操作此表的类型)」。 「MySQL的存储引擎」 「MySQL的存储引擎在体系架构中位于第三层，负责MySQL中的数据的存储和提取，是与文件打交道的子系统，它是根据MySQL提供的文件访问层抽象接口定制的一种文件访问机制。」 从架构图中可以看出「mysql支持多种存储引擎， 不同版本的mysql支持的引擎会有细微差别」 InnoDB：支持事务，具有提交，回滚和崩溃恢复能力，事务安全 MyISAM：不支持事务和外键，访问速度快 Memory：利用内存创建表，访问速度非常快，因为数据在内存，而且默认使用Hash索引，但是 一旦关闭，数据就会丢失 Archive：归档类型引擎，仅能支持insert和select语句 Csv：以CSV文件进行数据存储，由于文件限制，所有列必须强制指定not null，另外CSV引擎也不 支持索引和分区，适合做数据交换的中间表 BlackHole: 黑洞，只进不出，进来消失，所有插入数据都不会保存 Federated：可以访问远端MySQL数据库中的表。一个本地表，不保存数据，访问远程表内容。 MRG_MyISAM：一组MyISAM表的组合，这些MyISAM表必须结构相同， Merge表本身没有数据， 对Merge操作可以对一组MyISAM表进行操作。 我本地使用的5.7.24版本，使用以下命令可以查看当前数据库支持的引擎信息： 1show engines 二 MySQL常用存储引擎 「MySQL5.7支持的存储引擎包含」 InnoDB 、MyISAM 、BDB、MEMORY、MERGE、EXAMPLE、NDB Cluster、ARCHIVE、CSV、BLACKHOLE、FEDERATED等。 「其中InnoDB和BDB提供事务安全表，其他存储引擎是非事务安全表。」 「MySQL默认存储引擎」 「Mysql5.5之前的默认存储引擎是MyISAM，5.5之后改为InnoDB」。通过以下命令可以查看默认的存储引擎 1show variables like '%default_storage_engine%'; 「可以在配置文件中设置default_storage_engine修改默认的存储引擎」 「常用存储引擎的特性」 在MySQL中常用的存储引擎：【InnoDB】【MyISAM】【MEMORY】【 MERGE】【NDB】，它们之间的一些特细如下表： 特点 InnoDB MyISAM MEMORY MERGE NDB 存储限制 64TB 265TB RAM 没有 384 EB 事务安全 支持 锁机制 行锁(适合高并发) 表锁 表锁 表锁 行锁 B树索引 支持 支持 支持 支持 支持 哈希索引 支持 全文索引 支持(5.6版本之后) 支持 集群索引 支持 数据索引 支持 支持 支持 索引缓存 支持 支持 支持 支持 支持 数据可压缩 支持 空间使用 高 低 N/A 低 低 内存使用 高 低 中等 低 高 批量插入速度 低 高 高 高 高 支持外键 支持 虽然mysql支持的存储引擎多种多样，但是「基本上在一般的企业和应用中大多是使用的【InnoDB】【MyISAM】两种」。因此我们在提到存储引擎的时候也都是默认描述的这两种，那么到底什么时候使用InnoDB？什么时候使用MyISAM呢？ 三、InnoDB和MyISAM对比InnoDB和MyISAM是使用MySQL时最常用的两种引擎类型，我们重点来看下两者区别。 3.1 事务和外键 InnoDB支持事务和外键，具有安全性和完整性，适合大量insert或update操作 MyISAM不支持事务和外键，它提供高速存储和检索，适合大量的select查询操作 3.2 锁机制 InnoDB支持行级锁，锁定指定记录。基于索引来加锁实现。 MyISAM支持表级锁，锁定整张表。 3.3 索引结构 InnoDB使用聚集索引（聚簇索引），索引和记录在一起存储，既缓存索引，也缓存记录。 InnoDB中叶子结点中直接存储的是索引对应的数据，如下图： MyISAM使用非聚集索引（非聚簇索引），索引和记录分开。 MyISAM中叶节点的data域存放的是数据记录的地址，如下图： 3.4 并发处理能力 MyISAM使用表锁，会导致写操作并发率低，读之间并不阻塞，读写阻塞。 InnoDB读写阻塞可以与隔离级别有关，可以采用多版本并发控制（MVCC）来支持高并发 3.5 存储文件 InnoDB表对应两个文件，一个.frm表结构文件，一个.ibd数据文件。InnoDB表最大支持64TB； MyISAM表对应三个文件，一个.frm表结构文件，一个MYD表数据文件，一个.MYI索引文件。从MySQL5.0开始默认限制是256TB。 3.6 适用场景 「MyISAM」 不需要事务支持（不支持） 并发相对较低（锁定机制问题） 数据修改相对较少，以读为主 数据一致性要求不高 「InnoDB」 需要事务支持（具有较好的事务特性） 行级锁定对高并发有很好的适应能力 数据更新较为频繁的场景 数据一致性要求较高 硬件设备内存较大，可以利用InnoDB较好的缓存能力来提高内存利用率，减少磁盘IO 四、InnoDB与MyISAM如何选择 需要事务选择InnoDB 存在并发修改选择InnoDB 追求快速查询，且数据修改少，选择MyISAM 在绝大多数情况下，推荐使用InnoDB","link":"/2021/08/06/MySQL%E4%B8%89%EF%BC%9A%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"},{"title":"MySQL(七)：一文详解六大日志","text":"日志一般分为逻辑日志与物理日志两类 「逻辑日志」：即执行过的事务中的sql语句，执行的sql语句（增删改）「反向」的信息 「物理日志」：mysql 数据最终是保存在数据页中的，物理日志记录的就是数据页变更 。 「mysql数据库中日志是重要组成部分，记录着数据库运行期间各种状态信息」。主要有6类： 二进制日志 重做日志 撤销日志 错误日志 查询日志 中继日志 「而我们一般比较关注的是二进制日志( binlog )和事务日志(包括重做日志redo log 和撤销日志 undo log )」。 一、二进制日志(binlog)1.1 什么是binlog「记录对MySQL数据库执行的更改操作，包括语句的发生时间、执行时长，主要用于数据库恢复和主从复制」。但不记录select、show等不修改数据库的SQL。 「binlog是逻辑日志，在 mysql Server 层进行记录，属于mysql全局日志，无论使用什么存储引擎都会记录 binlog 日志」。 「binlog产生的时机」 「事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）按照一定的格式记录到binlog中」。 「binlog记录方式」 「binlog 是通过追加的方式记录数据库执行的写操作(不包括读操作)信息，以二进制保存在磁盘中。」 「开启binlog」 在my.inf主配置文件中添加如下配置 12345678910#打开binlog日志 默认是OFFlog_bin=ON#binlog日志的基本文件名，后面会追加标识来表示每一个文件 例如：mysql-bin.000094log_bin_basename=/var/lib/mysql/mysql-bin#指定的是binlog文件的索引文件 管理所有的binlog文件的目录 记录有多少个binlog文件等log_bin_index=/var/lib/mysql/mysql-bin.index#或者将以上三个配置合二为一log-bin=/var/lib/mysql/mysql-bin#mysql会根据这个配置自动设置log_bin为on状态，自动设置log_bin_index文件为指定文件名后跟.index 「binlog大小设置与过期时间」 「binlog 是通过追加的方式进行写入的，有两个参数需要控制binlog大小设置与过期时间」 12345678[mysqld]expire_logs_days = 7max_binlog_size = 104857600set global max_binlog_size=104857600;#100Mshow variables like '%max_binlog_size%';show variables like '%expire_logs_days%';expire_logs_days 1234set global max_binlog_size=104857600;#100Mset global expire_logs_days = 7;show variables like '%max_binlog_size%';show variables like '%expire_logs_days%'; 「查看配置」 12show variables like '%max_binlog_size%';show variables like '%expire_logs_days%'; 「命令配置」 及时生效，重启失效， 「max_binlog_size」 「max_binlog_size 代表每个 binlog文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志，默认值是1GB」。 「expire_logs_days」 「mysql自动清除过期binlog日志的天数。默认值为0,表示没有自动删除」。 「配置文件配置」 永久性，需要重启，修改my.conf： 1.2 binlog使用场景在实际应用中， binlog 的主要使用场景有两个，分别是 「主从复制」 和 「数据恢复」 。 「数据复制」 ： 在 Master 端开启 binlog ，然后将 binlog发送到各个 Slave 端， Slave 端执行 binlog 从而达到主从数据一致。 「数据恢复」 通过使用 mysqlbinlog 工具来恢复数据。 1.3 binlog刷盘时机「对于 InnoDB 存储引擎而言，只有在事务提交时才会记录biglog 日志」，此时记录还在内存中，那么 biglog是什么时候刷到磁盘中的呢？ 「mysql 通过 sync_binlog 参数控制 biglog 的刷盘时机，取值范围是 0-N：」 0：不去强制要求，由系统自行判断何时写入磁盘； 1：每次 commit 的时候都要将 binlog 写入磁盘； N：每N个事务，才会将 binlog 写入磁盘。 1show variables like '%sync_binlog%'; 「MySQL 5.7.7之后版本的 sync_binlog 默认值是 1 ，配置为1也是数据最不容易丢失的，但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。」 1.4 binlog日志格式binlog 日志格式通过 binlog_format 指定，有三种格式，分别为 STATMENT 、 ROW 和 MIXED。日志。 在 MySQL 5.7.7 之前，默认的格式是 STATEMENT ， MySQL 5.7.7 之后，默认值是 ROW。 1234567mysql&gt; show variables like '%binlog_format%';+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+1 row in set, 1 warning (0.00 sec) 「STATMENT」 基于SQL 语句的复制( statement-based replication, SBR )，每一条修改数据的sql语句会记录到binlog 中 。 缺点：在某些情况下会导致主从数据不一致，比如执行sysdate() 、 slepp() 等 。 优点：不需要记录每一行数据的变化，减少了 binlog 日志量，节约 IO , 从而提高了性能； 「ROW」 基于行的复制(row-based replication, RBR )，不记录每条sql语句的上下文信息，仅需记录哪条数据被修改了 。 缺点：会产生大量的日志，尤其是alter table 的时候会让日志暴涨 优点：能清楚记录每一个行数据的修改细节，能完全实现主从数据同步和数据的恢复； 「MIXED」 基于STATMENT 和 ROW 两种模式的混合复制(mixed-based replication, MBR )，一般的复制使用STATEMENT 模式保存 binlog ，对于 STATEMENT 模式无法复制的操作使用 ROW 模式保存 binlog 二、重做日志（Redo Log）「重做日志（Redo Log）是InnoDB存储引擎所特有的日志」。了解Redo Log从以下几个方向来看： Redo Log解决了什么问题 为什么需要redo log redo log效率为什么快 redo log file刷盘 Redo Log和BinLog的比较 2.1Redo Log解决了什么问题「InnoDB作为MySQL的存储引擎，数据是存放在磁盘中的，如果每次读写数据都需要磁盘I/O，效率会很低。」 为提高读写效率，InnoDB添加了缓存池（Buffer Pool）作为访问数据库的缓冲，其包含了磁盘中部分数据页的映射。 「当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则需要从磁盘中读取然后放入 Buffer Pool；」 「当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中（即：刷脏）」。 Buffer Pool的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。而Redo Log 解决了这个问题。 2.1为什么需要redo log 事务的四大特性ACID中有一个是 「持久性」 「只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态」 。 「mysql是如何保证一致性」 最简单的做法是在每次事务提交的时候，将该事务涉及修改的数据页全部刷新到磁盘中。但是这么做会有性能瓶颈，主要体现在两个方面： 因为 Innodb 是以 页 为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！ 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机I/O写入性能太差！ 因此 mysql 设计了 redo log ， 「redo log是物理日志，只记录事务对数据页做了哪些修改，而不是某一行或某几行修改成什么样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)，相对而言文件更小并且是顺序IO」。 2.3 redo log效率为什么快Redo Log 默认是在事务提交的时候将日志写入磁盘，为什么它比直接将 Buffer Pool中修改的数据写入磁盘要快呢？ 「磁盘刷脏操作是随机I/O，因为每次修改的数据的位置都是随机的，但是写redo log是追加操作，顺序I/O」。 「磁盘刷脏是以数据页为单位的（Page），MySQL默认页的大小是16KB，一个page上一个小修改都要整页写入；而 redo log中只包含真正修改的部分，不会有无效I/O」。 2.4 redo log基本概念 「重做日志是一种基于磁盘的数据结构，用于在崩溃恢复期间修正不完整事务写入的数据」。 MySQL以循环方式写入重做日志文件，记录InnoDB中所有对Buffer Pool修改的日志。 当出现实例故障，导致数据未能更新到数据文件，则数据库重启时须redo，重新把数据更新到数据文件。读写事务在执行的过程中，都会不断的产生redo log。默认情况下，重做日志在磁盘上由两个名为ib_logfile0和ib_logfile1的文件物理表示。 redo log 包括两部分 一个是内存中的日志缓冲( redo log buffer )，临时性 一个是磁盘上的日志文件( redo log file)，永久性。 2.5 redo log写磁盘的方式mysql 每执行一条 DML 语句，先将记录写入 redo log buffer，后续「某个时间点再一次性将多个操作记录写到 redo log file」。这种 「先写日志，再写磁盘」 的技术就是 「「预写式日志」」（Write-Ahead Logging ，缩写 WAL）。 「在计算机操作系统中，用户空间( user space )下的缓冲区数据是无法直接写入磁盘的，中间必须经过操作系统内核空间( kernel space )的缓冲区( OS Buffer )」。因此， redo log buffer 写入 redo log file 实际上是「先写入 OS Buffer ，然后再通过系统调用 fsync() 将其刷到 redo log file中」。 「redo log file刷盘时机」 「mysql 支持三种将 redo log buffer 写入 redo log file 的时机」，可以通过 innodb_flush_log_at_trx_commit 参数配置： 1234567mysql&gt; show variables like '%innodb_flush_log_at_trx_commit%';+--------------------------------+-------+| Variable_name | Value |+--------------------------------+-------+| innodb_flush_log_at_trx_commit | 1 |+--------------------------------+-------+1 row in set, 1 warning (0.01 sec) 速度较快，比0安全，只有在「操作系统崩溃或者系统断电」的情况下，上一秒钟所有事务数据才可能丢失。 最安全，但也是最慢，即使mysql挂掉也不会丢失数据，但是IO频繁，性能下降。 速度最快，不太安全，在mysql挂掉的时候，会丢失一秒钟的数据。 当innodb_flush_log_at_trx_commit = 0 【「延迟写」】：「提交事务时不会立即将数据redo log buffer 写入到 OS Buffer ，而是通过 InnoDB 的主线程每秒写入 OS Buffer并调用 fsync() 将其刷到 redo log file中」。 当innodb_flush_log_at_trx_commit = 1 【「实时写实时刷」】：「每次提交事务时都会将数据redo log buffer 写入到 OS Buffer 并立即调用 fsync() 将其刷到 redo log file中」。 当innodb_flush_log_at_trx_commit = 2 【「实时写延迟刷」】：「每次提交事务时都会将数据redo log buffer 写入到 OS Buffer 中，然后每间隔一秒调用 fsync() 将其刷到 redo log file中」 2.6 redo log写入形式「在innodb中，既有redo log 需要刷盘，还有 数据页 也需要刷盘， redo log存在的意义主要就是降低对 数据页 刷盘的要求」 。 「redo log 实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此 redo log实现上采用了【大小固定，循环写入】的方式，当写到结尾时，会回到开头循环写日志」。如下图： write pos ：表示 redo log 当前记录的 LSN (逻辑序列号)位置 check point ：表示 「数据页更改记录」 刷盘后对应 redo log 所处的 LSN(逻辑序列号)位置。 write pos 到 check point 之间的部分是 redo log 空着的部分，用于记录新的记录； check point 到 write pos 之间是 redo log 待落盘的数据页更改记录。 当 write pos追上check point 时，会先推动 check point 向前移动，空出位置再记录新的日志。 启动 innodb 的时候，无论上次是正常关闭或异常关闭，都会进行恢复操作。 重启innodb 时，首先会检查磁盘中数据页的 LSN ，如果数据页的LSN 小于日志中的 LSN ，则会从 checkpoint 开始恢复。 如果在宕机前正处于check point 的刷盘过程，且「数据页的刷盘进度超过了日志页的刷盘进度」，此时会出现数据页中记录的 LSN 大于日志中的 LSN，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。 2.7 Redo Log和BinLog的比较redo log 主要用来。binlog是用来进行归档的。 「一个更新的sql先执行到redo log内为预提交状态，binlog写入，写入之后通知redo log改提交状态」 「作用不同」 redo log是用于【崩溃恢复】的，保证MySQL宕机也不会影响持久性。 redo log 主要用来数据库宕机恢复数据的 binlog是【用于时间点恢复】的，保证服务器可以基于时间点恢复数据或者用于主从复制。 binlog是用来进行归档的 「层次不同」 redo log是InnoDB存储引擎实现的，innodb独享 binlog是MySQL的服务器层实现的服务，全局引擎共享 「内容不同」 redo log是物理日志，内容基于磁盘的数据Page，「记录该数据页更新状态内容」 比如：「将第6页、第8行、第7个位置改成aaaa」这种 bin log是逻辑日志，内容是二进制的，根据binlog＿format参数的不同，分为不同的模式，可能基于SQL 语句、基于数据本身、或者二者的混合，「记录更新过程」。 比如：insert into t values (null, 4, ‘2022-03-24’); 也跟bin log日志格式有关 「写入时机不同」 bin log是在事务提交完成后进行一次写入 redo log的写入时机有三种（具体在上文中有介绍） 「写入方式不同」 binlog在写满或者重启之后，会生成新的binlog文件，旧的日志数据会一直保留。 redo log是循环使用，会清理旧的日志数据。 三 撤销日志（Undo Logs） 世间没有后悔药，但是在MySQL中实现了重头开始，撤销日志（Undo Logs）就是MySQL的后悔药。 「撤消日志是在事务开始之前保存的被修改数据的备份，用于回滚事务」。 撤消日志属于逻辑日志，根据每行记录进行记录。 撤消日志存在于系统表空间、撤消表空间和临时表空间中。 3.1 什么是Undo logUndo：意为撤销或取消，undo即返回指定某个状态的操作 「undo log」 「一种用于撤销回退的日志，在事务开始之前，会先记录存放到 Undo 日志文件里，备份起来，当事务回滚时或者数据库崩溃时用于回滚事务」。 「undo log记录的是什么」 「undo log中记录的是当前事务操作中的相反操作」。 undo日志属于逻辑日志，记录的是一个操作过程，sql执行delete或者update操作都会记录一条undo日志 123一条insert语句在undo log中会对应一条delete语句，一条delete语句在undo log中会对应一条insert语句，update语句会在undo log中对应相反的update语句， 3.2 Undo log存储方式「Undo log的存储由InnoDB存储引擎实现」，数据保存在InnoDB的数据文件中，innodb存储引擎对undo的管理采用回滚段（rollback segment）的数据结构。 回滚段（rollback segment）中有1024个undo log segment， MySQL5.5版本之前 只支持1个rollback segment，即只能存储1024个undo log segment MySQL5.5版本之后 支持128个rollback segment（innodb_undo_logs配置项），即能存储128*1024个undo log segment 3.3 Undo log相关的变量12345678910mysql&gt; show variables like '%innodb_undo%';+--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| innodb_undo_directory | .\\ || innodb_undo_log_truncate | OFF || innodb_undo_logs | 128 || innodb_undo_tablespaces | 0 |+--------------------------+-------+4 rows in set, 1 warning (0.03 sec) innodb_undo_directory 定义存储的目录路径，默认值.\\，表示datadir， datadir参数在my.in中配置 123[mysql]# 设置mysql数据库的数据的存放目录datadir=&quot;D:\\mysql-8.0.13-winx64\\data&quot; innodb_undo_log_truncate 开启1(ON)/关闭0(OFF)在线回收（收缩）undo log日志文件，支持动态设置，默认关闭 innodb_undo_logs 回滚段rollback segment的数量，Mysql5.5版本后默认设置为128 innodb_undo_tablespaces 默认值为0，表示undo log全部写入一个表空间文件，可以设置这个变量，平均分配到多少个文件中。 3.4 Undo log的工作原理「undo log在事务开启之前产生，当事务提交后，InnoDB会将事务对应的undo日志保存在删除list中，后台通过清除线程进行回收处理」。 以一条sql执行update、select过程，如图： 执行update操作，事务A提交时候（事务还没提交），会将数据进行备份，备份到对应的undo buffer， Undo Log保存了未提交之前的操作日志，User表数据肯定就是持久保存到InnoDB的数据文件IBD，默认情况。 此时事务B进行查询操作，直接读undo buffer缓存，事务A还没提交事务，如需要回滚，不读磁盘，先直接从undo buffer缓存读取 3.5 Undo log作用说明 「实现事务的原子性」undo log可以用于实现事务的原子性， 如果事务处理过程中要执行回滚（rollback）操作，可以利用undo log将数据恢复到事务开始之前 「实现多版本并发控制（MVCC）」Undo Log 在 MySQL InnoDB 存储引擎中用来实现多版本并发控制，事务没提交之前，undo日志可以作为高并发情况下其它并发事务进行快照读。 3.6 Bin log、redo log、Undo 如何协同「先来回顾一下几个日志的主要作用」 Buffer Pool 是MySQL的一个非常重要的组件，因为针对数据库的增删改操作都是在Buffer Pool完成的 Undo log 记录的是数据操作前的样子 Redo log 记录的是数据被操作后的样子 Bin log 记录的是整个操作记录 「准备更新一条数据到事务的提交的流程描述：」 首先执行器根据MySQL的执行计划来查询数据，先是从缓存池(Buffer Pool)中查询数据，如果没有就会去 数据库中查询，如果查询到了就将其放到缓存池中。 在数据被缓存到缓存池的同时，会写入 undo log 日志文件 更新的动作是在BufferPool中完成的，同时会将更新后的数据添加到redo log buffer中 完成以后就可以提交事务，在提交的同时会做以下三件事 将redo log buffer中的数据刷入到redo log 文件中 将本次操作记录写入到bin log文件中 将bin log 文件名字和更新内容在bin log中的位置记录到redo log中，同时在redo log 最后添加 commit 标记 3.7 Undo log的清理3.7.1 Undo log类型「在回滚段中，每个 undo log 段都有一个类型字段，共有两种类型」： 「insert undo log」**「代表事务在insert新记录时产生的undo log, 其回滚段类型为 insert undo logs，仅用于事务回滚，并且在事务提交后可以被立即丢弃」**。 「update undo log」**「事务在进行update或delete时产生的undo log，其回滚段类型为 update undo logs; 不仅在事务回滚时需要，在实现MVCC快照读时也需要」** 3.7.2 Undo log清理类型Undo log的清理也分为两种情况 「事务 rollback」 如果事务rollback，innodb 通过执行 undo log中的所有反向操作，实现事务回滚，随后就会删除该事务关联的所有 undo log 段。 「事务 commit」 对于 insert undo logs，事务回滚后，innodb会直接清除该事务关联的所有 undo log 段。 对于 update undo logs，只有当前没有任何活跃事务存在时，innodb 的 purge 线程才会清理这些 undo log 段 3.7.3 purge 线程上述提到的 「purge 线程，是一个周期运行的垃圾收集线程，主要用来收集 undo log 段」。 「innodb 会将所有需要清理的任务添加到 purge 队列中，」 可以通过 innodb_max_purge_lag 配置项设定 purge 队列的大小 「purge 线程会在周期执行时，对 purge 队列中的任务进行清理，」 可以通过innodb_max_purge_lag_delay 配置项设定 purge 线程的执行周期间隔 「尽量缩短使用中每个事务的持续时间，可以让 purge 线程有更大概率回收已经没有存在必要的 undo log 段，从而尽量释放磁盘空间的占用」 四、错误日志(error log)「错误日志(error log)：记录mysql服务的启停时正确和错误的信息，还记录启动、停止、运行过程中的错误信息。」 「指定错误日志文件」 「在my.ini的[mysqld]下：添加代码：log-error=file_name.txt。」 1log-error=E:\\log-error.txt。 「如果没有指定file_name，则默认的错误日志文件为datadir目录下的 hostname.err ，hostname表示当前的主机名。」 「查看错误日志位置」 1show variables like 'log_error'; 「错误日志的产生」 「MySQL 5.5.7之前」 刷新日志操作(如flush logs)会备份旧的错误日志(以_old结尾)，并创建一个新的错误日志文件并打开。 「在MySQL 5.5.7之后」 执行刷新日志的操作时，错误日志「会关闭并重新打开」，如果错误日志不存在，则会先创建。 「MySQL正在运行状态下」 「在运行状态下删除错误日志后，不会自动创建错误日志，只有在刷新日志的时候才会创建一个新的错误日志文件」。 五、查询日志「查询日志分为一般查询日志和慢查询日志」。通过查询是否超出变量「long_query_time」指定时间的值来判定的。 「在超时时间内完成的查询是一般查询，可以将其记录到一般查询日志中，超出时间的查询是慢查询，可以将其记录到慢查询日志中。」 「long_query_time」 1234# 指定慢查询超时时长，超出此时长的属于慢查询，会记录到慢查询日志中long_query_time = 10 # 定义一般查询日志和慢查询日志的输出格式，不指定时默认为filelog_output={TABLE|FILE|NONE} 「TABLE：表示记录日志到表中」 「FILE：表示记录日志到文件中」 「NONE：表示不记录日志」 5.1 一般查询日志(general log )「记录了服务器接收到的每一个查询或是命令」，无论这些查询或是命令是否正确甚至是否包含语法错误，general log 都会将其记录下来 。 开启General logmysql服务器需要不断地记录日志，会产生一定的系统开销。 所有Mysql默认关闭一般查询日志。 「开启general log」 1set global general_log=on;#为全局变量 「开启general log」 1set global general_log=off; 「查看general log是否开启」 1show global variables like 'general_log'; 「设置日志文件路径」 默认是库文件路径下主机名加上.log 1set global general_log_file='/tmp/general.log'; 「查看日志输出格式」 1show variables like 'log_output'; 5.2 慢查询日志(slow log)「慢查询日志记录执行时间超过long_query_time和没有使用索引的查询语句，并且只会记录执行成功的语句。」 「查询超出变量 long_query_time 指定时间值的为慢查询。不包含查询获取锁(包括锁等待)的时间」。 「查看慢查询指定时间」 「默认10s」 1234567mysql&gt; show variables like 'long_query_time';+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+1 row in set, 1 warning (0.00 sec) 「查看慢查询的条数」 1234567mysql&gt; show status like &quot;%slow_queries%&quot;;+---------------+-------+| Variable_name | Value |+---------------+-------+| Slow_queries | 4 |+---------------+-------+1 row in set (0.00 sec) 以上Slow_queries = 4 说明查询超过10秒的查询有4个 「启用慢查询日志」 1与ON等价，0与OFF等价。 123mysql&gt; set @@global.slow_query_log=on;#或者 mysql&gt; set @@global.slow_query_log=1; 「查看是否启用慢查询日志」 1show variables like 'slow_query%'; 「slow_query_log」 1slow_query_log={1|ON|0|OFF} 与 log_slow_queries={``yes``|no} 都是表示是否启用慢查询日志，两个同时变化。 「slow_query_log_file」 日志文件位置，默认路径为库文件目录下主机名加上-slow.log 「【MySQL记录慢查询日志是在查询执行完毕且已经完全释放锁之后记录的】，因此慢查询日志记录的顺序和执行的SQL查询语句顺序可能会不一致（先执行完先记录）」。 六、中继日志(relay log)「主要作用：主从复制」 「【从服务器I/O线程】将主服务器的【二进制日志】读取过来记录到从服务器本地文件，然后【从服务器SQL线程】会读取relay-log日志的内容并应用到从服务器，从而使从服务器和主服务器的数据保持一致」 「查看relay log配置」 1show variables like '%relay%';","link":"/2021/08/06/MySQL%E4%B8%83%EF%BC%9A%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3%E5%85%AD%E5%A4%A7%E6%97%A5%E5%BF%97/"},{"title":"Java集合框架","text":"前言 一、概述 集合框架图 Collection Map 工具类 通用实现 二、深入源码分析 ArrayList 1. 概览 2. 序列化 3. 扩容 4. 删除元素 5. Fail-Fast Vector 1. 同步 2. ArrayList 与 Vector 3. Vector 替代方案 synchronizedList CopyOnWriteArrayList LinkedList 1. 概览 2. add() 3. remove() 4. get() 5. 总结 6. ArrayList 与 LinkedList HashMap 1. 存储结构 JDK1.7 的存储结构 JDK1.8 的存储结构 2. 重要参数 3. 确定哈希桶数组索引位置 4. 分析HashMap的put方法 5. 扩容机制 6. 线程安全性 7. JDK1.8与JDK1.7的性能对比 8. Hash较均匀的情况 9. Hash极不均匀的情况 10. HashMap与HashTable 11. 小结 ConcurrentHashMap 1. 概述 2. 存储结构 2. size 操作 3. 同步方式 4. JDK 1.8 的改动 HashSet 1. 成员变量 2. 构造函数 3. add() 4. 总结 LinkedHashSet and LinkedHashMap 1. 概览 2. get() 3. put() 4. remove() 5. LinkedHashSet 6. LinkedHashMap经典用法 三、容器中的设计模式 迭代器模式 适配器模式 四、面试指南 1. ArrayList和LinkedList区别 2. HashMap和HashTable区别，HashMap的key类型 3. HashMap和ConcurrentHashMap 4. Hashtable的原理 5. Hash冲突的解决办法 6. 什么是迭代器 7. 构造相同hash的字符串进行攻击，这种情况应该怎么处理？JDK7如何处理 8. Hashmap为什么大小是2的幂次 更新日志 前言 Java集合框架 (Java Collections Framework, JCF) 也称容器，这里可以类比 C++ 中的 STL，在市面上似乎还没能找到一本详细介绍的书籍。在这里主要对如下部分进行源码分析，及在面试中常见的问题。 例如，在阿里面试常问到的 HashMap 和 ConcurrentHashMap 原理等等。深入源码分析是面试中必备的技能，通过本文的阅读会对集合框架有更深一步的了解。 本文参考： CarpenterLee/JCFInternals: 深入理解Java集合框架 crossoverJie/Java-Interview: 👨‍🎓 Java related : basic, concurrent, algorithm Interview-Notebook/Java 容器.md at master · CyC2018/Interview-Notebook 一、概述 Java集合框架提供了数据持有对象的方式，提供了对数据集合的操作。Java 集合框架位于 java.util 包下，主要有三个大类：Collection(接口)**、Map(接口)、集合工具类**。 集合框架图 Collection ArrayList：线程不同步。默认初始容量为 10，当数组大小不足时容量扩大为 1.5 倍。为追求效率，ArrayList 没有实现同步（synchronized），如果需要多个线程并发访问，用户可以手动同步，也可使用 Vector 替代。 LinkedList：线程不同步。双向链接实现。LinkedList 同时实现了 List 接口和 Deque 接口，也就是说它既可以看作一个顺序容器，又可以看作一个队列（Queue），同时又可以看作一个栈（Stack）。这样看来，LinkedList 简直就是个全能冠军。当你需要使用栈或者队列时，可以考虑使用 LinkedList，一方面是因为 Java 官方已经声明不建议使用 Stack 类，更遗憾的是，Java 里根本没有一个叫做 Queue 的类（它是个接口名字）。关于栈或队列，现在的首选是 ArrayDeque，它有着比 LinkedList（当作栈或队列使用时）有着更好的性能。 Stack and Queue：Java 里有一个叫做 Stack 的类，却没有叫做 Queue 的类（它是个接口名字）。当需要使用栈时，Java 已不推荐使用 Stack，而是推荐使用更高效的 ArrayDeque；既然 Queue 只是一个接口，当需要使用队列时也就首选 ArrayDeque 了（次选是 LinkedList ）。 Vector：线程同步。默认初始容量为 10，当数组大小不足时容量扩大为 2 倍。它的同步是通过 Iterator 方法加 synchronized 实现的。 Stack：线程同步。继承自 Vector，添加了几个方法来完成栈的功能。现在已经不推荐使用 Stack，在栈和队列中有限使用 ArrayDeque，其次是 LinkedList。 TreeSet：线程不同步，内部使用 NavigableMap 操作。默认元素 “自然顺序” 排列，可以通过 Comparator 改变排序。TreeSet 里面有一个 TreeMap（适配器模式） HashSet：线程不同步，内部使用 HashMap 进行数据存储，提供的方法基本都是调用 HashMap 的方法，所以两者本质是一样的。集合元素可以为 NULL。 Set：Set 是一种不包含重复元素的 Collection，Set 最多只有一个 null 元素。Set 集合通常可以通过 Map 集合通过适配器模式得到。 PriorityQueue：Java 中 PriorityQueue 实现了 Queue 接口，不允许放入 null 元素；其通过堆实现，具体说是通过完全二叉树（complete binary tree）实现的小顶堆（任意一个非叶子节点的权值，都不大于其左右子节点的权值），也就意味着可以通过数组来作为 PriorityQueue 的底层实现。 优先队列的作用是能保证每次取出的元素都是队列中权值最小的（Java 的优先队列每次取最小元素，C++ 的优先队列每次取最大元素）。这里牵涉到了大小关系，元素大小的评判可以通过元素本身的自然顺序（natural ordering），也可以通过构造时传入的比较器（Comparator，类似于 C++ 的仿函数）。 NavigableSet：添加了搜索功能，可以对给定元素进行搜索：小于、小于等于、大于、大于等于，放回一个符合条件的最接近给定元素的 key。 EnumSet：线程不同步。内部使用 Enum 数组实现，速度比 HashSet 快。只能存储在构造函数传入的枚举类的枚举值。 注释：更多设计模式，请转向 Java 设计模式 Map TreeMap：线程不同步，基于 红黑树 （Red-Black tree）的 NavigableMap 实现，能够把它保存的记录根据键排序，默认是按键值的升序排序，也可以指定排序的比较器，当用 Iterator 遍历 TreeMap 时，得到的记录是排过序的。 TreeMap 底层通过红黑树（Red-Black tree）实现，也就意味着 containsKey(), get(), put(), remove() 都有着 log(n) 的时间复杂度。其具体算法实现参照了《算法导论》。 HashTable：线程安全，HashMap 的迭代器 (Iterator) 是 fail-fast 迭代器。HashTable 不能存储 NULL 的 key 和 value。 HashMap：线程不同步。根据 key 的 hashcode 进行存储，内部使用静态内部类 Node 的数组进行存储，默认初始大小为 16，每次扩大一倍。当发生 Hash 冲突时，采用拉链法（链表）。JDK 1.8中：当单个桶中元素个数大于等于8时，链表实现改为红黑树实现；当元素个数小于6时，变回链表实现。由此来防止hashCode攻击。 Java HashMap 采用的是冲突链表方式。 HashMap 是 Hashtable 的轻量级实现，可以接受为 null 的键值 (key) 和值 (value)，而 Hashtable 不允许。 LinkedHashMap：保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的。也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比 HashMap 慢，不过有种情况例外，当 HashMap 容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap 慢，因为 LinkedHashMap 的遍历速度只和实际数据有关，和容量无关，而 HashMap 的遍历速度和他的容量有关。 WeakHashMap：从名字可以看出它是某种 Map。它的特殊之处在于 WeakHashMap 里的 entry 可能会被 GC 自动删除，即使程序员没有调用 remove() 或者 clear() 方法。 WeakHashMap 的存储结构类似于HashMap 既然有 WeekHashMap，是否有 WeekHashSet 呢？答案是没有！不过 Java Collections 工具类给出了解决方案，Collections.newSetFromMap(Map&lt;E,Boolean&gt; map) 方法可以将任何 Map包装成一个Set。 工具类 Collections、Arrays：集合类的一个工具类帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。 Comparable、Comparator：一般是用于对象的比较来实现排序，两者略有区别。 类设计者没有考虑到比较问题而没有实现 Comparable 接口。这是我们就可以通过使用 Comparator，这种情况下，我们是不需要改变对象的。 一个集合中，我们可能需要有多重的排序标准，这时候如果使用 Comparable 就有些捉襟见肘了，可以自己继承 Comparator 提供多种标准的比较器进行排序。 说明：线程不同步的时候可以通过，Collections.synchronizedList() 方法来包装一个线程同步方法 通用实现ImplementationsHash TableResizable ArrayBalanced TreeLinked ListHash Table + Linked ListInterfacesSetHashSetTreeSetLinkedHashSetListArrayListLinkedListDequeArrayDequeLinkedListMapHashMapTreeMapLinkedHashMap 参考资料： CarpenterLee/JCFInternals:深入理解Java集合框架 Java基础-集合框架 - 掘金 二、深入源码分析源码分析基于 JDK 1.8 / JDK 1.7，在 IDEA 中 double shift 调出 Search EveryWhere，查找源码文件，找到之后就可以阅读源码。 ArrayList1. 概览实现了 RandomAccess 接口，因此支持随机访问，这是理所当然的，因为 ArrayList 是基于数组实现的。 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。 1private static final int DEFAULT_CAPACITY = 10; 2. 序列化基于数组实现，保存元素的数组使用 transient 修饰，该关键字声明数组默认不会被序列化。ArrayList 具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。ArrayList 重写了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 1transient Object[] elementData; // non-private to simplify nested class access 3. 扩容添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，也就是旧容量的 1.5 倍。 扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。 1234567891011121314151617181920212223242526272829303132333435363738394041// JDK 1.8 public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;}// 判断数组是否越界private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity);}private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);}// 扩容private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 1.5倍 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);}private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;} 4. 删除元素需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上。 12345678910public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;} 5. Fail-Fast开始之前我们想讲讲，什么是 fail-fast 机制? fail-fast 机制在遍历一个集合时，当集合结构被修改，会抛出 Concurrent Modification Exception。 fail-fast 会在以下两种情况下抛出 Concurrent Modification Exception （1）单线程环境 集合被创建后，在遍历它的过程中修改了结构。 注意 remove() 方法会让 expectModcount 和 modcount 相等，所以是不会抛出这个异常。 （2）多线程环境 当一个线程在遍历这个集合，而另一个线程对这个集合的结构进行了修改。 modCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 Concurrent Modification Exception。 1234567891011121314151617private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException{ // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) { s.writeObject(elementData[i]); } if (modCount != expectedModCount) { throw new ConcurrentModificationException(); }} Vector1. 同步它的实现与 ArrayList 类似，但是使用了 synchronized 进行同步。 123456789101112public synchronized boolean add(E e) { modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;}public synchronized E get(int index) { if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);} 2. ArrayList 与 Vector Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍空间，而 ArrayList 是 1.5 倍。 3. Vector 替代方案synchronizedList为了获得线程安全的 ArrayList，可以使用 Collections.synchronizedList(); 得到一个线程安全的 ArrayList。 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); CopyOnWriteArrayList 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWrite 容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行 Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对 CopyOnWrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以 CopyOnWrite 容器也是一种读写分离的思想，读和写不同的容器。 123456789101112131415161718192021public boolean add(T e) { final ReentrantLock lock = this.lock; lock.lock(); try { Object[] elements = getArray(); int len = elements.length; // 复制出新数组 Object[] newElements = Arrays.copyOf(elements, len + 1); // 把新元素添加到新数组里 newElements[len] = e; // 把原数组引用指向新数组 setArray(newElements); return true; } finally { lock.unlock(); }}final void setArray(Object[] a) { array = a;} 读的时候不需要加锁，如果读的时候有多个线程正在向 ArrayList 添加数据，读还是会读到旧的数据，因为写的时候不会锁住旧的 ArrayList。 123public E get(int index) { return get(getArray(), index);} CopyOnWrite的缺点 CopyOnWrite 容器有很多优点，但是同时也存在两个问题，即内存占用问题和数据一致性问题。所以在开发的时候需要注意一下。 内存占用问题。 因为 CopyOnWrite 的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意：在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说 200M 左右，那么再写入 100M 数据进去，内存就会占用 300M，那么这个时候很有可能造成频繁的 Yong GC 和 Full GC。之前我们系统中使用了一个服务由于每晚使用 CopyOnWrite 机制更新大对象，造成了每晚 15 秒的 Full GC，应用响应时间也随之变长。 针对内存占用问题，可以通过压缩容器中的元素的方法来减少大对象的内存消耗，比如，如果元素全是 10 进制的数字，可以考虑把它压缩成 36 进制或 64 进制。或者不使用 CopyOnWrite 容器，而使用其他的并发容器，如 ConcurrentHashMap 。 数据一致性问题。 CopyOnWrite 容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用 CopyOnWrite 容器。 关于 C++ 的 STL 中，曾经也有过 Copy-On-Write 的玩法，参见陈皓的《C++ STL String类中的Copy-On-Write》，后来，因为有很多线程安全上的事，就被去掉了。 参考资料： 聊聊并发-Java中的Copy-On-Write容器 | 并发编程网 – ifeve.com LinkedList 1. 概览LinkedList 底层是基于双向链表实现的，也是实现了 List 接口，所以也拥有 List 的一些特点 (JDK1.7/8 之后取消了循环，修改为双向链表) 。 LinkedList 同时实现了 List 接口和 Deque 接口，也就是说它既可以看作一个顺序容器，又可以看作一个队列（Queue），同时又可以看作一个栈（Stack）。这样看来， LinkedList 简直就是个全能冠军。当你需要使用栈或者队列时，可以考虑使用 LinkedList ，一方面是因为 Java 官方已经声明不建议使用 Stack 类，更遗憾的是，Java里根本没有一个叫做 Queue 的类（它是个接口名字）。 关于栈或队列，现在的首选是 ArrayDeque，它有着比 LinkedList （当作栈或队列使用时）有着更好的性能。 基于双向链表实现，内部使用 Node 来存储链表节点信息。 12345private static class Node&lt;E&gt; { E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;} 每个链表存储了 Head 和 Tail 指针： 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; LinkedList 的实现方式决定了所有跟下标相关的操作都是线性时间，而在首段或者末尾删除元素只需要常数时间。为追求效率LinkedList没有实现同步（synchronized），如果需要多个线程并发访问，可以先采用 Collections.synchronizedList() 方法对其进行包装。 2. add() add() 方法有两个版本，一个是 add(E e)，该方法在 LinkedList 的末尾插入元素，因为有 last 指向链表末尾，在末尾插入元素的花费是常数时间。只需要简单修改几个相关引用即可；另一个是 add(int index, E element)，该方法是在指定下表处插入元素，需要先通过线性查找找到具体位置，然后修改相关引用完成插入操作。 123456789101112131415161718192021// JDK 1.8public boolean add(E e) { linkLast(e); return true;}/*** Links e as last element.*/void linkLast(E e) { final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;} add(int index, E element) 的逻辑稍显复杂，可以分成两部分 先根据 index 找到要插入的位置； 修改引用，完成插入操作。 123456789101112public void add(int index, E element) { checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index));}private void checkPositionIndex(int index) { if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));} 上面代码中的 node(int index) 函数有一点小小的 trick，因为链表双向的，可以从开始往后找，也可以从结尾往前找，具体朝那个方向找取决于条件 index &lt; (size &gt;&gt; 1)，也即是 index 是靠近前端还是后端。 3. remove()remove() 方法也有两个版本，一个是删除跟指定元素相等的第一个元素 remove(Object o)，另一个是删除指定下标处的元素 remove(int index)。 两个删除操作都要： 先找到要删除元素的引用； 修改相关引用，完成删除操作。 在寻找被删元素引用的时候 remove(Object o) 调用的是元素的 equals 方法，而 remove(int index) 使用的是下标计数，两种方式都是线性时间复杂度。在步骤 2 中，两个 revome() 方法都是通过 unlink(Node&lt;E&gt; x) 方法完成的。这里需要考虑删除元素是第一个或者最后一个时的边界情况。 4. get()12345678910111213141516171819public E get(int index) { checkElementIndex(index); return node(index).item;} Node&lt;E&gt; node(int index) { // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) { Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; } else { Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; }} 由此可以看出是使用二分查找来看 index 离 size 中间距离来判断是从头结点正序查还是从尾节点倒序查。 node() 会以 O(n/2) 的性能去获取一个结点 如果索引值大于链表大小的一半，那么将从尾结点开始遍历 这样的效率是非常低的，特别是当 index 越接近 size 的中间值时。 5. 总结 LinkedList 插入，删除都是移动指针效率很高。 查找需要进行遍历查询，效率较低。 6. ArrayList 与 LinkedList ArrayList 基于动态数组实现，LinkedList 基于双向链表实现； ArrayList 支持随机访问，LinkedList 不支持； LinkedList 在任意位置添加删除元素更快。 HashMap我们这篇文章就来试着分析下 HashMap 的源码，由于 HashMap 底层涉及到太多方面，一篇文章总是不能面面俱到，所以我们可以带着面试官常问的几个问题去看源码： 了解底层如何存储数据的 HashMap 的几个主要方法 HashMap 是如何确定元素存储位置的以及如何处理哈希冲突的 HashMap 扩容机制是怎样的 JDK 1.8 在扩容和解决哈希冲突上对 HashMap 源码做了哪些改动？有什么好处? HashMap 的内部功能实现很多，本文主要从根据 key 获取哈希桶数组索引位置、put 方法的详细执行、扩容过程三个具有代表性的点深入展开讲解。 1. 存储结构JDK1.7 的存储结构在 1.7 之前 JDK 采用「拉链法」来存储数据，即数组和链表结合的方式： 「拉链法」用专业点的名词来说叫做链地址法。简单来说，就是数组加链表的结合。在每个数组元素上存储的都是一个链表。 我们之前说到不同的 key 可能经过 hash 运算可能会得到相同的地址，但是一个数组单位上只能存放一个元素，采用链地址法以后，如果遇到相同的 hash 值的 key 的时候，我们可以将它放到作为数组元素的链表上。待我们去取元素的时候通过 hash 运算的结果找到这个链表，再在链表中找到与 key 相同的节点，就能找到 key 相应的值了。 JDK1.7 中新添加进来的元素总是放在数组相应的角标位置，而原来处于该角标的位置的节点作为 next 节点放到新节点的后边。稍后通过源码分析我们也能看到这一点。 JDK1.8 的存储结构对于 JDK1.8 之后的 HashMap 底层在解决哈希冲突的时候，就不单单是使用数组加上单链表的组合了，因为当处理如果 hash 值冲突较多的情况下，链表的长度就会越来越长，此时通过单链表来寻找对应 Key 对应的 Value 的时候就会使得时间复杂度达到 O(n)，因此在 JDK1.8 之后，在链表新增节点导致链表长度超过 TREEIFY_THRESHOLD = 8 的时候，就会在添加元素的同时将原来的单链表转化为红黑树。 对数据结构很在行的读者应该，知道红黑树是一种易于增删改查的二叉树，他对与数据的查询的时间复杂度是 O(logn) 级别，所以利用红黑树的特点就可以更高效的对 HashMap 中的元素进行操作。 从结构实现来讲，HashMap 是数组+链表+红黑树（JDK1.8增加了红黑树部分）实现的，如下如所示。 这里需要讲明白两个问题：数据底层具体存储的是什么？这样的存储方式有什么优点呢？ （1）从源码可知，HashMap 类中有一个非常重要的字段，就是 Node[] table，即哈希桶数组，明显它是一个 Node 的数组。我们来看 Node（ JDK1.8 中） 是何物。 1234567891011121314static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; //用来定位数组索引位置 final K key; V value; Node&lt;K,V&gt; next; //链表的下一个node Node(int hash, K key, V value, Node&lt;K,V&gt; next) { ... } public final K getKey(){ ... } public final V getValue() { ... } public final String toString() { ... } public final int hashCode() { ... } public final V setValue(V newValue) { ... } public final boolean equals(Object o) { ... }} Node 是 HashMap 的一个内部类，实现了 Map.Entry 接口，本质是就是一个映射（键值对）。上图中的每个黑色圆点就是一个Node对象。 （2）HashMap 就是使用哈希表来存储的。哈希表为解决冲突，可以采用开放地址法和链地址法等来解决问题， Java 中 HashMap 采用了链地址法。链地址法，简单来说，就是数组加链表的结合。在每个数组元素上都一个链表结构，当数据被 Hash 后，得到数组下标，把数据放在对应下标元素的链表上。例如程序执行下面代码： 1map.put(&quot;美团&quot;,&quot;小美&quot;); 系统将调用 “美团” 这个 key 的 hashCode() 方法得到其 hashCode 值（该方法适用于每个 Java 对象），然后再通过 Hash 算法的后两步运算（高位运算和取模运算，下文有介绍）来定位该键值对的存储位置，有时两个 key 会定位到相同的位置，表示发生了 Hash 碰撞。当然 Hash 算法计算结果越分散均匀，Hash 碰撞的概率就越小，map 的存取效率就会越高。 如果哈希桶数组很大，即使较差的 Hash 算法也会比较分散，如果哈希桶数组数组很小，即使好的 Hash 算法也会出现较多碰撞，所以就需要在空间成本和时间成本之间权衡，其实就是在根据实际情况确定哈希桶数组的大小，并在此基础上设计好的 hash 算法减少 Hash 碰撞。 那么通过什么方式来控制 map 使得 Hash 碰撞的概率又小，哈希桶数组（Node[] table）占用空间又少呢？ 答案就是好的 Hash 算法和扩容机制。 在理解 Hash 和扩容流程之前，我们得先了解下 HashMap 的几个字段。从 HashMap 的默认构造函数源码可知，构造函数就是对下面几个字段进行初始化，源码如下： 1234int threshold; // 所能容纳的key-value对极限 final float loadFactor; // 负载因子int modCount; int size; 首先，Node[] table的初始化长度 length (默认值是16)**，Load factor 为负载因子(默认值是0.75)，threshold 是 HashMap 所能容纳的最大数据量的 Node (键值对)个数。threshold = length * Load factor**。也就是说，在数组定义好长度之后，负载因子越大，所能容纳的键值对个数越多。 结合负载因子的定义公式可知，threshold 就是在此 Load factor 和 length (数组长度)对应下允许的最大元素数目，超过这个数目就重新 resize(扩容)，扩容后的 HashMap 容量是之前容量的两倍。默认的负载因子 0.75 是对空间和时间效率的一个平衡选择，建议大家不要修改，除非在时间和空间比较特殊的情况下，如果内存空间很多而又对时间效率要求很高，可以降低负载因子 Load factor 的值；相反，如果内存空间紧张而对时间效率要求不高，可以增加负载因子 loadFactor 的值，这个值可以大于1。 size 这个字段其实很好理解，就是 HashMap 中实际存在的键值对数量。注意和 table 的长度 length、容纳最大键值对数量 threshold 的区别。而 modCount 字段主要用来记录 HashMap 内部结构发生变化的次数，主要用于迭代的快速失败。强调一点，内部结构发生变化指的是结构发生变化，例如 put 新键值对，但是某个 key 对应的 value 值被覆盖不属于结构变化。 在 HashMap 中，哈希桶数组 table 的长度 length 大小必须为 2n（一定是合数），这是一种非常规的设计，常规的设计是把桶的大小设计为素数。相对来说素数导致冲突的概率要小于合数，具体证明可以参考 为什么一般hashtable的桶数会取一个素数？ ，Hashtable 初始化桶大小为 11，就是桶大小设计为素数的应用（Hashtable 扩容后不能保证还是素数）。HashMap 采用这种非常规设计，主要是为了在取模和扩容时做优化，同时为了减少冲突，HashMap 定位哈希桶索引位置时，也加入了高位参与运算的过程。 这里存在一个问题，即使负载因子和 Hash 算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能。于是，在 JDK1.8 版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高 HashMap 的性能，其中会用到红黑树的插入、删除、查找等算法。本文不再对红黑树展开讨论，想了解更多红黑树数据结构的工作原理可以参考：教你初步了解红黑树。 2. 重要参数 参数 说明 buckets 在 HashMap 的注释里使用哈希桶来形象的表示数组中每个地址位置。注意这里并不是数组本身，数组是装哈希桶的，他可以被称为哈希表。 capacity table 的容量大小，默认为 16。需要注意的是 capacity 必须保证为 2 的 n 次方。 size table 的实际使用量。 threshold size 的临界值，size 必须小于 threshold，如果大于等于，就必须进行扩容操作。 loadFactor 装载因子，table 能够使用的比例，threshold = capacity * loadFactor。 TREEIFY_THRESHOLD 树化阀值，哈希桶中的节点个数大于该值（默认为8）的时候将会被转为红黑树行存储结构。 UNTREEIFY_THRESHOLD 非树化阀值，小于该值（默认为 6）的时候将再次改为单链表的格式存储 3. 确定哈希桶数组索引位置很多操作都需要先确定一个键值对所在的桶下标。 12int hash = hash(key);int i = indexFor(hash, table.length); （一）计算 hash 值 1234567891011121314151617final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);}public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value);} （二）取模 令 x = 1&lt;&lt;4，即 x 为 2 的 4 次方，它具有以下性质： 12x : 00010000x-1 : 00001111 令一个数 y 与 x-1 做与运算，可以去除 y 位级表示的第 4 位以上数： 123y : 10110010x-1 : 00001111y&amp;(x-1) : 00000010 这个性质和 y 对 x 取模效果是一样的： 123y : 10110010x : 00010000y%x : 00000010 我们知道，位运算的代价比求模运算小的多，因此在进行这种计算时用位运算的话能带来更高的性能。 确定桶下标的最后一步是将 key 的 hash 值对桶个数取模：hash%capacity，如果能保证 capacity 为 2 的 n 次方，那么就可以将这个操作转换为位运算。 123static int indexFor(int h, int length) { return h &amp; (length-1);} 4. 分析HashMap的put方法 HashMap 的 put 方法执行过程可以通过下图来理解，自己有兴趣可以去对比源码更清楚地研究学习。 ①.判断键值对数组 table[i] 是否为空或为 null，否则执行 resize() 进行扩容； ②.根据键值 key 计算 hash 值得到插入的数组索引i，如果 table[i]==null，直接新建节点添加，转向 ⑥，如果table[i] 不为空，转向 ③； ③.判断 table[i] 的首个元素是否和 key 一样，如果相同直接覆盖 value，否则转向 ④，这里的相同指的是 hashCode 以及 equals； ④.判断table[i] 是否为 treeNode，即 table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向 ⑤； ⑤.遍历 table[i]，判断链表长度是否大于 8，大于 8 的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现 key 已经存在直接覆盖 value 即可； ⑥.插入成功后，判断实际存在的键值对数量 size 是否超多了最大容量 threshold，如果超过，进行扩容。 JDK1.8 HashMap 的 put 方法源码如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) { // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);}final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 步骤①：tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 步骤②：计算index，并对null做处理 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else { Node&lt;K,V&gt; e; K k; // 步骤③：节点key存在，直接覆盖value if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 步骤④：判断该链为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 步骤⑤：该链为链表 else { for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key,value,null); //链表长度大于8转换为红黑树进行处理 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } // key已经存在直接覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; } } ++modCount; // 步骤⑥：超过最大容量 就扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;} 5. 扩容机制扩容 (resize) 就是重新计算容量，向 HashMap 对象里不停的添加元素，而 HashMap 对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然 Java 里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下 resize 的源码，鉴于 JDK1.8 融入了红黑树，较复杂，为了便于理解我们仍然使用 JDK1.7 的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213void resize(int newCapacity) { //传入新的容量 Entry[] oldTable = table; //引用扩容前的Entry数组 int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { //扩容前的数组大小如果已经达到最大(2^30)了 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 return; } Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组 transfer(newTable); //！！将数据转移到新的Entry数组里 table = newTable; //HashMap的table属性引用新的Entry数组 threshold = (int)(newCapacity * loadFactor);//修改阈值} 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer() 方法将原有 Entry 数组的元素拷贝到新的 Entry 数组里。 1234567891011121314151617void transfer(Entry[] newTable) { Entry[] src = table; //src引用了旧的Entry数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { //遍历旧的Entry数组 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 if (e != null) { src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） do { Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置 e.next = newTable[i]; //标记[1] newTable[i] = e; //将元素放在数组上 e = next; //访问下一个Entry链上的元素 } while (e != null); } }} newTable[i] 的引用赋给了 e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到 Entry 链的尾部(如果发生了 hash 冲突的话），这一点和 Jdk1.8 有区别，下文详解。在旧数组中同一条 Entry 链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的 hash 算法就是简单的用 key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组 table 的 size=2， 所以 key = 3、7、5，put 顺序依次为 5、7、3。在 mod 2 以后都冲突在 table[1] 这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小 size 大于 table 的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize 成 4，然后所有的 Node 重新 rehash 的过程。 下面我们讲解下 JDK1.8 做了哪些优化。经过观测可以发现，我们使用的是 2 次幂的扩展 (指长度扩为原来 2 倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动 2 次幂的位置。看下图可以明白这句话的意思，n 为 table 的长度，图（a）表示扩容前的 key1 和 key2 两种 key 确定索引位置的示例，图（b）表示扩容后 key1 和 key2 两种 key 确定索引位置的示例，其中 hash1 是 key1 对应的哈希与高位运算结果。 元素在重新计算 hash 之后，因为 n 变为 2 倍，那么 n-1 的 mask 范围在高位多 1bit (红色)，因此新的 index 就会发生这样的变化： 因此，我们在扩充 HashMap 的时候，不需要像 JDK1.7 的实现那样重新计算 hash，只需要看看原来的 hash 值新增的那个 bit 是 1 还是 0 就好了，是 0 的话索引没变，是 1 的话索引变成“原索引+oldCap”，可以看看下图为 16 扩充为 32 的 resize 示意图： 这个设计确实非常的巧妙，既省去了重新计算 hash 值的时间，而且同时，由于新增的 1bit 是 0 还是 1 可以认为是随机的，因此 resize 的过程，均匀的把之前的冲突的节点分散到新的 bucket 了。这一块就是 JDK1.8 新增的优化点。有一点注意区别，JDK1.7 中 rehash 的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8 不会倒置。有兴趣的同学可以研究下 JDK1.8 的 resize源 码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold } else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({&quot;rawtypes&quot;，&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else { // 链表优化重hash的代码块 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do { next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab;} 6. 线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的 HashMap，而使用线程安全的 ConcurrentHashMap。那么为什么说 HashMap 是线程不安全的，下面举例子说明在并发的多线程使用场景中使用 HashMap 可能造成死循环。代码例子如下(便于理解，仍然使用 JDK1.7 的环境)： 1234567891011121314151617181920public class HashMapInfiniteLoop { private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) { map.put(5， &quot;C&quot;); new Thread(&quot;Thread1&quot;) { public void run() { map.put(7, &quot;B&quot;); System.out.println(map); }; }.start(); new Thread(&quot;Thread2&quot;) { public void run() { map.put(3, &quot;A); System.out.println(map); }; }.start(); } } 其中，map初始化为一个长度为2的数组，loadFactor=0.75，threshold=2*0.75=1，也就是说当put第二个key的时候，map就需要进行resize。 通过设置断点让线程1和线程2同时debug到transfer方法(3.3小节代码块)的首行。注意此时两个线程已经成功添加数据。放开thread1的断点至transfer方法的“Entry next = e.next;” 这一行；然后放开线程2的的断点，让线程2进行resize。结果如下图。 注意，Thread1的 e 指向了key(3)，而next指向了key(7)，其在线程二rehash后，指向了线程二重组后的链表。 线程一被调度回来执行，先是执行 newTalbe[i] = e， 然后是e = next，导致了e指向了key(7)，而下一次循环的next = e.next导致了next指向了key(3)。 e.next = newTable[i] 导致 key(3).next 指向了 key(7)。注意：此时的key(7).next 已经指向了key(3)， 环形链表就这样出现了。 于是，当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。 7. JDK1.8与JDK1.7的性能对比HashMap中，如果key经过hash算法得出的数组索引位置全部不相同，即Hash算法非常好，那样的话，getKey方法的时间复杂度就是O(1)，如果Hash算法技术的结果碰撞非常多，假如Hash算极其差，所有的Hash算法结果得出的索引位置一样，那样所有的键值对都集中到一个桶中，或者在一个链表中，或者在一个红黑树中，时间复杂度分别为O(n)和O(lgn)。 鉴于JDK1.8做了多方面的优化，总体性能优于JDK1.7，下面我们从两个方面用例子证明这一点。 8. Hash较均匀的情况为了便于测试，我们先写一个类Key，如下： 123456789101112131415161718192021222324252627class Key implements Comparable&lt;Key&gt; { private final int value; Key(int value) { this.value = value; } @Override public int compareTo(Key o) { return Integer.compare(this.value, o.value); } @Override public boolean equals(Object o) { if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Key key = (Key) o; return value == key.value; } @Override public int hashCode() { return value; }} 这个类复写了equals方法，并且提供了相当好的hashCode函数，任何一个值的hashCode都不会相同，因为直接使用value当做hashcode。为了避免频繁的GC，我将不变的Key实例缓存了起来，而不是一遍一遍的创建它们。代码如下： 123456789101112131415public class Keys { public static final int MAX_KEY = 10_000_000; private static final Key[] KEYS_CACHE = new Key[MAX_KEY]; static { for (int i = 0; i &lt; MAX_KEY; ++i) { KEYS_CACHE[i] = new Key(i); } } public static Key of(int value) { return KEYS_CACHE[value]; }} 现在开始我们的试验，测试需要做的仅仅是，创建不同size的HashMap（1、10、100、……10000000），屏蔽了扩容的情况，代码如下： 1234567891011121314151617181920static void test(int mapSize) { HashMap&lt;Key, Integer&gt; map = new HashMap&lt;Key,Integer&gt;(mapSize); for (int i = 0; i &lt; mapSize; ++i) { map.put(Keys.of(i), i); } long beginTime = System.nanoTime(); //获取纳秒 for (int i = 0; i &lt; mapSize; i++) { map.get(Keys.of(i)); } long endTime = System.nanoTime(); System.out.println(endTime - beginTime);}public static void main(String[] args) { for(int i=10;i&lt;= 1000 0000;i*= 10){ test(i); }} 在测试中会查找不同的值，然后度量花费的时间，为了计算getKey的平均时间，我们遍历所有的get方法，计算总的时间，除以key的数量，计算一个平均值，主要用来比较，绝对值可能会受很多环境因素的影响。结果如下： 通过观测测试结果可知，JDK1.8的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。由于Hash算法较均匀，JDK1.8引入的红黑树效果不明显，下面我们看看Hash不均匀的的情况。 9. Hash极不均匀的情况假设我们又一个非常差的Key，它们所有的实例都返回相同的hashCode值。这是使用HashMap最坏的情况。代码修改如下： 123456789class Key implements Comparable&lt;Key&gt; { //... @Override public int hashCode() { return 1; }} 仍然执行main方法，得出的结果如下表所示： 从表中结果中可知，随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表太长的时候，HashMap会动态的将它替换成一个红黑树，这话的话会将时间复杂度从O(n)降为O(logn)。hash算法均匀和不均匀所花费的时间明显也不相同，这两种情况的相对比较，可以说明一个好的hash算法的重要性。 测试环境：处理器为2.2 GHz Intel Core i7，内存为16 GB 1600 MHz DDR3，SSD硬盘，使用默认的JVM参数，运行在64位的OS X 10.10.1上。 10. HashMap与HashTable HashTable 使用 synchronized 来进行同步。 HashMap 可以插入键为 null 的 Entry。 HashMap 的迭代器是 fail-fast 迭代器。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 11. 小结 扩容是一个特别耗性能的操作，所以当程序员在使用 HashMap 的时候，估算 map 的大小，初始化的时候给一个大致的数值，避免 map 进行频繁的扩容。 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 HashMap 是线程不安全的，不要在并发的环境中同时操作 HashMap，建议使用 ConcurrentHashMap。 JDK1.8 引入红黑树大程度优化了 HashMap 的性能。 参考资料： Java 8系列之重新认识HashMap——美团技术 搞懂 Java HashMap 源码 - 掘金 搞懂 Java equals 和 hashCode 方法 - 掘金 ConcurrentHashMap1. 概述 众所周知，哈希表是中非常高效，复杂度为 O(1) 的数据结构，在 Java 开发中，我们最常见到最频繁使用的就是 HashMap 和 HashTable，但是在线程竞争激烈的并发场景中使用都不够合理。 HashMap ：先说 HashMap，HashMap 是线程不安全的，在并发环境下，可能会形成环状链表（扩容时可能造成），导致 get 操作时，cpu 空转，所以，在并发环境中使 用HashMap 是非常危险的。 HashTable ： HashTable 和 HashMap的实现原理几乎一样，差别无非是：（1）HashTable不允许key和value为null；（2）HashTable是线程安全的。 但是 HashTable 线程安全的策略实现代价却太大了，简单粗暴，get/put 所有相关操作都是 synchronized 的，这相当于给整个哈希表加了一把大锁，多线程访问时候，只要有一个线程访问或操作该对象，那其他线程只能阻塞，相当于将所有的操作串行化，在竞争激烈的并发场景中性能就会非常差。 HashTable 性能差主要是由于所有操作需要竞争同一把锁，而如果容器中有多把锁，每一把锁锁一段数据，这样在多线程访问时不同段的数据时，就不会存在锁竞争了，这样便可以有效地提高并发效率。这就是ConcurrentHashMap 所采用的 “分段锁“ 思想。 2. 存储结构ConcurrentHashMap 采用了非常精妙的”分段锁”策略，ConcurrentHashMap 的主干是个 Segment 数组。 1final Segment&lt;K,V&gt;[] segments; Segment 继承了 ReentrantLock，所以它就是一种可重入锁（ReentrantLock)。在 ConcurrentHashMap，一个 Segment 就是一个子哈希表，Segment 里维护了一个 HashEntry 数组，并发环境下，对于不同 Segment 的数据进行操作是不用考虑锁竞争的。（就按默认的 ConcurrentLeve 为16来讲，理论上就允许 16 个线程并发执行，有木有很酷） 所以，对于同一个 Segment 的操作才需考虑线程同步，不同的 Segment 则无需考虑。 Segment 类似于 HashMap，一个 Segment 维护着一个 HashEntry 数组 1transient volatile HashEntry&lt;K,V&gt;[] table; HashEntry 是目前我们提到的最小的逻辑处理单元了。一个 ConcurrentHashMap 维护一个 Segment 数组，一个 Segment 维护一个 HashEntry 数组。 123456static final class HashEntry&lt;K,V&gt; { final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;} ConcurrentHashMap 和 HashMap 实现上类似，最主要的差别是 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），多个线程可以同时访问不同分段锁上的桶，从而使其并发度更高（并发度就是 Segment 的个数）。 Segment 继承自 ReentrantLock。 1234567891011121314151617static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable { private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;} 1final Segment&lt;K,V&gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment。 1static final int DEFAULT_CONCURRENCY_LEVEL = 16; 2. size 操作每个 Segment 维护了一个 count 变量来统计该 Segment 中的键值对个数。 12345/** * The number of elements. Accessed only either within locks * or among other volatile reads that maintain visibility. */transient int count; 在执行 size 操作时，需要遍历所有 Segment 然后把 count 累计起来。 ConcurrentHashMap 在执行 size 操作时先尝试不加锁，如果连续两次不加锁操作得到的结果一致，那么可以认为这个结果是正确的。 尝试次数使用 RETRIES_BEFORE_LOCK 定义，该值为 2，retries 初始值为 -1，因此尝试次数为 3。 如果尝试的次数超过 3 次，就需要对每个 Segment 加锁。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Number of unsynchronized retries in size and containsValue * methods before resorting to locking. This is used to avoid * unbounded retries if tables undergo continuous modification * which would make it impossible to obtain an accurate result. */static final int RETRIES_BEFORE_LOCK = 2;public int size() { // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try { for (;;) { // 超过尝试次数，则对每个 Segment 加锁 if (retries++ == RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation } sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) { Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) { sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; } } // 连续两次得到的结果一致，则认为这个结果是正确的 if (sum == last) break; last = sum; } } finally { if (retries &gt; RETRIES_BEFORE_LOCK) { for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); } } return overflow ? Integer.MAX_VALUE : size;} 3. 同步方式Segment 继承自 ReentrantLock，所以我们可以很方便的对每一个 Segment 上锁。 对于读操作，获取 Key 所在的 Segment 时，需要保证可见性。具体实现上可以使用 volatile 关键字，也可使用锁。但使用锁开销太大，而使用 volatile 时每次写操作都会让所有 CPU 内缓存无效，也有一定开销。ConcurrentHashMap 使用如下方法保证可见性，取得最新的 Segment。 1Segment&lt;K,V&gt; s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u) 获取 Segment 中的 HashEntry 时也使用了类似方法 12HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE) 对于写操作，并不要求同时获取所有 Segment 的锁，因为那样相当于锁住了整个 Map。它会先获取该 Key-Value 对所在的 Segment 的锁，获取成功后就可以像操作一个普通的 HashMap 一样操作该 Segment，并保证该Segment 的安全性。同时由于其它 Segment 的锁并未被获取，因此理论上可支持 concurrencyLevel（等于 Segment 的个数）个线程安全的并发读写。 获取锁时，并不直接使用 lock 来获取，因为该方法获取锁失败时会挂起。事实上，它使用了自旋锁，如果 tryLock 获取锁失败，说明锁被其它线程占用，此时通过循环再次以 tryLock 的方式申请锁。如果在循环过程中该 Key 所对应的链表头被修改，则重置 retry 次数。如果 retry 次数超过一定值，则使用 lock 方法申请锁。 这里使用自旋锁是因为自旋锁的效率比较高，但是它消耗 CPU 资源比较多，因此在自旋次数超过阈值时切换为互斥锁。 4. JDK 1.8 的改动 JDK 1.7 使用分段锁机制来实现并发更新操作，核心类为 Segment，它继承自重入锁 ReentrantLock，并发程度与 Segment 数量相等。 JDK 1.8 使用了 CAS 操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。 并且 JDK 1.8 的实现也在链表过长时会转换为红黑树。 参考资料： ConcurrentHashMap演进从Java7到Java8 ConcurrentHashMap实现原理及源码分析 - dreamcatcher-cx - 博客园 HashSet 前面已经说过 HashSet 是对 HashMap 的简单包装，对 HashSet 的函数调用都会转换成合适的 HashMap 方法，因此 HashSet 的实现非常简单，只有不到 300 行代码（适配器模式）。这里不再赘述。 12345678910111213141516//HashSet是对HashMap的简单包装public class HashSet&lt;E&gt;{ ...... private transient HashMap&lt;E,Object&gt; map;//HashSet里面有一个HashMap // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); public HashSet() { map = new HashMap&lt;&gt;(); } ...... public boolean add(E e) {//简单的方法转换 return map.put(e, PRESENT)==null; } ......} 1. 成员变量首先了解下 HashSet 的成员变量: 1234private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); 发现主要就两个变量: map ：用于存放最终数据的。 PRESENT ：是所有写入 map 的 value 值。 2. 构造函数1234567public HashSet() { map = new HashMap&lt;&gt;();}public HashSet(int initialCapacity, float loadFactor) { map = new HashMap&lt;&gt;(initialCapacity, loadFactor);} 构造函数很简单，利用了 HashMap 初始化了 map 。 3. add()123public boolean add(E e) { return map.put(e, PRESENT)==null;} 比较关键的就是这个 add() 方法。 可以看出它是将存放的对象当做了 HashMap 的健，value 都是相同的 PRESENT 。由于 HashMap 的 key 是不能重复的，所以每当有重复的值写入到 HashSet 时，value 会被覆盖，但 key 不会收到影响，这样就保证了 HashSet 中只能存放不重复的元素。 4. 总结HashSet 的原理比较简单，几乎全部借助于 HashMap 来实现的。 所以 HashMap 会出现的问题 HashSet 依然不能避免。 LinkedHashSet and LinkedHashMap1. 概览 如果你已看过前面关于 HashSet 和 HashMap，的讲解，一定能够想到本文将要讲解的 LinkedHashSet 和 LinkedHashMap 其实也是一回事。 LinkedHashSet 和 LinkedHashMap 在 Java 里也有着相同的实现，前者仅仅是对后者做了一层包装，也就是说 LinkedHashSet 里面有一个 LinkedHashMap（适配器模式）。因此本文将重点分析 LinkedHashMap。 LinkedHashMap 实现了 Map 接口，即允许放入 key 为 null 的元素，也允许插入 value 为 null 的元素。从名字上可以看出该容器是 LinkedList 和 HashMap 的混合体，也就是说它同时满足 HashMap 和 LinkedList 的某些特性。可将 LinkedHashMap 看作采用 LinkedList 增强的 HashMap。 事实上 LinkedHashMap 是 HashMap 的直接子类，二者唯一的区别是 LinkedHashMap 在 HashMap 的基础上，采用双向链表（doubly-linked list）的形式将所有 entry 连接起来，这样是为保证元素的迭代顺序跟插入顺序相同。上图给出了 LinkedHashMap 的结构图，主体部分跟 HashMap 完全一样，多了 header 指向双向链表的头部（是一个哑元），该双向链表的迭代顺序就是 entry 的插入顺序。 除了可以保迭代历顺序，这种结构还有一个好处：迭代 LinkedHashMap 时不需要像 HashMap 那样遍历整个table，而只需要直接遍历 header 指向的双向链表即可，也就是说 LinkedHashMap 的迭代时间就只跟entry的个数相关，而跟table的大小无关。 有两个参数可以影响 LinkedHashMap 的性能：初始容量（inital capacity）和负载系数（load factor）。初始容量指定了初始table的大小，负载系数用来指定自动扩容的临界值。当entry的数量超过capacity*load_factor时，容器将自动扩容并重新哈希。对于插入元素较多的场景，将初始容量设大可以减少重新哈希的次数。 将对象放入到 LinkedHashMap 或 LinkedHashSet 中时，有两个方法需要特别关心：hashCode() 和 equals()。hashCode() 方法决定了对象会被放到哪个 bucket 里，当多个对象的哈希值冲突时，equals() 方法决定了这些对象是否是“同一个对象”。所以，如果要将自定义的对象放入到 LinkedHashMap 或 LinkedHashSet 中，需要 @OverridehashCode() 和 equals() 方法。 通过如下方式可以得到一个跟源 Map 迭代顺序 一样的 LinkedHashMap： 1234void foo(Map m) { Map copy = new LinkedHashMap(m); ...} 出于性能原因，LinkedHashMap 是非同步的（not synchronized），如果需要在多线程环境使用，需要程序员手动同步；或者通过如下方式将 LinkedHashMap 包装成（wrapped）同步的： Map m = Collections.synchronizedMap(new LinkedHashMap(...)); 2. get()get(Object key) 方法根据指定的 key 值返回对应的 value。该方法跟HashMap.get()方法的流程几乎完全一样，读者可自行参考前文，这里不再赘述。 3. put()put(K key, V value) 方法是将指定的 key, value 对添加到 map 里。该方法首先会对 map 做一次查找，看是否包含该元组，如果已经包含则直接返回，查找过程类似于get()方法；如果没有找到，则会通过 addEntry(int hash, K key, V value, int bucketIndex) 方法插入新的 entry。 注意，这里的插入有两重含义： 从 table 的角度看，新的 entry 需要插入到对应的 bucket 里，当有哈希冲突时，采用头插法将新的 entry 插入到冲突链表的头部。 从 header 的角度看，新的 entry 需要插入到双向链表的尾部。 addEntry()代码如下： 123456789101112131415// LinkedHashMap.addEntry()void addEntry(int hash, K key, V value, int bucketIndex) { if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { resize(2 * table.length);// 自动扩容，并重新哈希 hash = (null != key) ? hash(key) : 0; bucketIndex = hash &amp; (table.length-1);// hash%table.length } // 1.在冲突链表头部插入新的entry HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;&gt;(hash, key, value, old); table[bucketIndex] = e; // 2.在双向链表的尾部插入新的entry e.addBefore(header); size++;} 上述代码中用到了 addBefore()方 法将新 entry e 插入到双向链表头引用 header 的前面，这样 e 就成为双向链表中的最后一个元素。addBefore() 的代码如下： 1234567// LinkedHashMap.Entry.addBefor()，将this插入到existingEntry的前面private void addBefore(Entry&lt;K,V&gt; existingEntry) { after = existingEntry; before = existingEntry.before; before.after = this; after.before = this;} 上述代码只是简单修改相关 entry 的引用而已。 4. remove()remove(Object key)的作用是删除key值对应的entry，该方法的具体逻辑是在removeEntryForKey(Object key)里实现的。removeEntryForKey()方法会首先找到key值对应的entry，然后删除该entry（修改链表的相应引用）。查找过程跟get()方法类似。 注意，这里的删除也有两重含义： 从table的角度看，需要将该entry从对应的bucket里删除，如果对应的冲突链表不空，需要修改冲突链表的相应引用。 从header的角度来看，需要将该entry从双向链表中删除，同时修改链表中前面以及后面元素的相应引用。 removeEntryForKey() 对应的代码如下： 12345678910111213141516171819202122232425// LinkedHashMap.removeEntryForKey()，删除key值对应的entryfinal Entry&lt;K,V&gt; removeEntryForKey(Object key) { ...... int hash = (key == null) ? 0 : hash(key); int i = indexFor(hash, table.length);// hash&amp;(table.length-1) Entry&lt;K,V&gt; prev = table[i];// 得到冲突链表 Entry&lt;K,V&gt; e = prev; while (e != null) {// 遍历冲突链表 Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) {// 找到要删除的entry modCount++; size--; // 1. 将e从对应bucket的冲突链表中删除 if (prev == e) table[i] = next; else prev.next = next; // 2. 将e从双向链表中删除 e.before.after = e.after; e.after.before = e.before; return e; } prev = e; e = next; } return e;} 5. LinkedHashSet前面已经说过LinkedHashSet是对LinkedHashMap的简单包装，对LinkedHashSet的函数调用都会转换成合适的LinkedHashMap方法，因此LinkedHashSet的实现非常简单，这里不再赘述。 1234567891011121314public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable { ...... // LinkedHashSet里面有一个LinkedHashMap public LinkedHashSet(int initialCapacity, float loadFactor) { map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor); } ...... public boolean add(E e) {//简单的方法转换 return map.put(e, PRESENT)==null; } ......} 6. LinkedHashMap经典用法LinkedHashMap 除了可以保证迭代顺序外，还有一个非常有用的用法：可以轻松实现一个采用了FIFO替换策略的缓存。具体说来，LinkedHashMap 有一个子类方法 protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest)，该方法的作用是告诉 Map 是否要删除“最老”的 Entry，所谓最老就是当前 Map 中最早插入的 Entry，如果该方法返回 true，最老的那个元素就会被删除。在每次插入新元素的之后 LinkedHashMap 会自动询问 removeEldestEntry() 是否要删除最老的元素。这样只需要在子类中重载该方法，当元素个数超过一定数量时让 removeEldestEntry() 返回 true，就能够实现一个固定大小的 FIFO 策略的缓存。示例代码如下： 12345678910111213/** 一个固定大小的FIFO替换策略的缓存 */class FIFOCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt;{ private final int cacheSize; public FIFOCache(int cacheSize){ this.cacheSize = cacheSize; } // 当Entry个数超过cacheSize时，删除最老的Entry @Override protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) { return size() &gt; cacheSize; }} 三、容器中的设计模式迭代器模式 Collection 实现了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。 从 JDK 1.5 之后可以使用 foreach 方法来遍历实现了 Iterable 接口的聚合对象。 123456List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(&quot;a&quot;);list.add(&quot;b&quot;);for (String item : list) { System.out.println(item);} 适配器模式java.util.Arrays.asList() 可以把数组类型转换为 List 类型。 12@SafeVarargspublic static &lt;T&gt; List&lt;T&gt; asList(T... a) 如果要将数组类型转换为 List 类型，应该注意的是 asList() 的参数为泛型的变长参数，因此不能使用基本类型数组作为参数，只能使用相应的包装类型数组。 12Integer[] arr = {1, 2, 3};List list = Arrays.asList(arr); 也可以使用以下方式生成 List。 1List list = Arrays.asList(1,2,3); 四、面试指南1. ArrayList和LinkedList区别 ArrayList 和 LinkedList 可想从名字分析，它们一个是 Array (动态数组) 的数据结构，一个是 Link (链表) 的数据结构，此外，它们两个都是对 List 接口的实现。前者是数组队列，相当于动态数组；后者为双向链表结构，也可当作堆栈、队列、双端队列； 当随机访问 List 时（get和set操作），ArrayList 比 LinkedList的效率更高，因为 LinkedList 是线性的数据存储方式，所以需要移动指针从前往后依次查找； 当对数据进行增加和删除的操作时（add 和 remove 操作），LinkedList 比 ArrayList 的效率更高，因为 ArrayList 是数组，所以在其中进行增删操作时，会对操作点之后所有数据的下标索引造成影响，需要进行数据的移动； 从利用效率来看，ArrayList 自由性较低，因为它需要手动的设置固定大小的容量，但是它的使用比较方便，只需要创建，然后添加数据，通过调用下标进行使用；而 LinkedList 自由性较高，能够动态的随数据量的变化而变化，但是它不便于使用； ArrayList 主要空间开销在于需要在 List 列表预留一定空间；而 LinkList 主要控件开销在于需要存储结点信息以及结点指针信息。 ArrayList、LinkedList 和 Vector如何选择？ 当对数据的主要操作为索引或只在集合的末端增加、删除元素时，使用 ArrayList 或 Vector 效率比较高； 当对数据的操作主要为制定位置的插入或删除操作时，使用 LinkedList 效率比较高； 当在多线程中使用容器时（即多个线程会同时访问该容器），选用 Vector 较为安全； 2. HashMap和HashTable区别，HashMap的key类型 Hash Map和HashTable的区别 Hashtable 的方法是同步的，HashMap 非同步，所以在多线程场合要手动同步 Hashtable 不允许 null 值 (key 和 value 都不可以)，HashMap 允许 null 值( key 和 value 都可以)。 两者的遍历方式大同小异，Hashtable 仅仅比 HashMap 多一个 elements 方法。 Hashtable 和 HashMap 都能通过 values() 方法返回一个 Collection ，然后进行遍历处理。 两者也都可以通过 entrySet() 方法返回一个 Set ， 然后进行遍历处理。 HashTable 使用 Enumeration，HashMap 使用 Iterator。 哈希值的使用不同，Hashtable 直接使用对象的 hashCode。而 HashMap 重新计算hash值，而且用于代替求模。 Hashtable 中 hash 数组默认大小是11，增加的方式是 old*2+1。HashMap 中 hash 数组的默认大小是16，而且一定是 2 的指数。 HashTable 基于 Dictionary 类，而 HashMap 基于 AbstractMap 类 HashMap中的key可以是任何对象或数据类型吗 可以为null，但不能是可变对象，如果是可变对象的话，对象中的属性改变，则对象 HashCode 也进行相应的改变，导致下次无法查找到已存在Map中的数据。 如果可变对象在 HashMap 中被用作键，那就要小心在改变对象状态的时候，不要改变它的哈希值了。我们只需要保证成员变量的改变能保证该对象的哈希值不变即可。 HashTable是线程安全的么 HashTable 是线程安全的，其实现是在对应的方法上添加了 synchronized 关键字进行修饰，由于在执行此方法的时候需要获得对象锁，则执行起来比较慢。所以现在如果为了保证线程安全的话，使用 CurrentHashMap。 3. HashMap和ConcurrentHashMap HashMap和Concurrent HashMap区别？ HashMa p是非线程安全的，CurrentHashMap 是线程安全的。 ConcurrentHashMap 将整个 Hash 桶进行了分段 segment，也就是将这个大的数组分成了几个小的片段segment，而且每个小的片段 segment 上面都有锁存在，那么在插入元素的时候就需要先找到应该插入到哪一个片段 segment，然后再在这个片段上面进行插入，而且这里还需要获取 segment 锁。 ConcurrentHashMap 让锁的粒度更精细一些，并发性能更好。 ConcurrentHashMap 线程安全吗， ConcurrentHashMap如何保证 线程安全？ HashTable 容器在竞争激烈的并发环境下表现出效率低下的原因是所有访问 HashTable 的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是 ConcurrentHashMap 所使用的分段锁，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 get 操作的高效之处在于整个 get 过程不需要加锁，除非读到的值是空的才会加锁重读。get 方法里将要使用的共享变量都定义成 volatile，如用于统计当前 Segement 大小的 count 字段和用于存储值的 HashEntry 的 value。定义成 volatile 的变量，能够在线程之间保持可见性，能够被多线程同时读，并且保证不会读到过期的值，但是只能被单线程写（有一种情况可以被多线程写，就是写入的值不依赖于原值），在 get 操作里只需要读不需要写共享变量 count 和 value，所以可以不用加锁。 put 方法首先定位到 Segment，然后在 Segment 里进行插入操作。 插入操作需要经历两个步骤：（1）判断是否需要对 Segment 里的 HashEntry 数组进行扩容；（2）定位添加元素的位置然后放在HashEntry数组里。 4. Hashtable的原理Hashtable 使用链地址法进行元素存储，通过一个实际的例子来演示一下插入元素的过程： 假设我们现在 Hashtable 的容量为 5，已经存在了 (5,5)，(13,13)，(16,16)，(17,17)，(21,21) 这 5 个键值对，目前他们在 Hashtable 中的位置如下： 现在，我们插入一个新的键值对，put(16,22)，假设 key=16 的索引为 1.但现在索引 1 的位置有两个 Entry 了，所以程序会对链表进行迭代。迭代的过程中，发现其中有一个 Entry 的 key 和我们要插入的键值对的 key 相同，所以现在会做的工作就是将 newValue=22 替换 oldValue=16，然后返回 oldValue = 16. 然后我们现在再插入一个，put(33,33)，key=33 的索引为 3，并且在链表中也不存在 key=33 的 Entry，所以将该节点插入链表的第一个位置。 Hashtable 与 HashMap 的简单比较 HashTable 基于 Dictionary 类，而 HashMap 是基于 AbstractMap。Dictionary 是任何可将键映射到相应值的类的抽象父类，而 AbstractMap 是基于 Map 接口的实现，它以最大限度地减少实现此接口所需的工作。 HashMap 的 key 和 value 都允许为 null，而 Hashtable 的 key 和 value 都不允许为 null。HashMap 遇到 key 为 null 的时候，调用 putForNullKey 方法进行处理，而对 value 没有处理；Hashtable遇到 null，直接返回 NullPointerException。 Hashtable 方法是同步，而HashMap则不是。我们可以看一下源码，Hashtable 中的几乎所有的 public 的方法都是 synchronized 的，而有些方法也是在内部通过 synchronized 代码块来实现。所以有人一般都建议如果是涉及到多线程同步时采用 HashTable，没有涉及就采用 HashMap，但是在 Collections 类中存在一个静态方法：**synchronizedMap()**，该方法创建了一个线程安全的 Map 对象，并把它作为一个封装的对象来返回。 参考资料： Hashtable 的实现原理 - Java 集合学习指南 - 极客学院Wiki 5. Hash冲突的解决办法 链地址法 开放地址法（向后一位） 线性探测 平方探测 二次哈希 再哈希法 6. 什么是迭代器 Java 集合框架的集合类，我们有时候称之为容器。容器的种类有很多种，比如 ArrayList、LinkedList、HashSet…，每种容器都有自己的特点，ArrayList 底层维护的是一个数组；LinkedList 是链表结构的；HashSet 依赖的是哈希表，每种容器都有自己特有的数据结构。 因为容器的内部结构不同，很多时候可能不知道该怎样去遍历一个容器中的元素。所以为了使对容器内元素的操作更为简单，Java 引入了迭代器模式！ 把访问逻辑从不同类型的集合类中抽取出来，从而避免向外部暴露集合的内部结构。 迭代器模式：就是提供一种方法对一个容器对象中的各个元素进行访问，而又不暴露该对象容器的内部细。 1234567891011121314151617public static void main(String[] args) { // 使用迭代器遍历ArrayList集合 Iterator&lt;String&gt; listIt = list.iterator(); while(listIt.hasNext()){ System.out.println(listIt.hasNext()); } // 使用迭代器遍历Set集合 Iterator&lt;String&gt; setIt = set.iterator(); while(setIt.hasNext()){ System.out.println(listIt.hasNext()); } // 使用迭代器遍历LinkedList集合 Iterator&lt;String&gt; linkIt = linkList.iterator(); while(linkIt.hasNext()){ System.out.println(listIt.hasNext()); }} 参考资料： 深入理解Java中的迭代器 - Mr·Dragon - 博客园 7. 构造相同hash的字符串进行攻击，这种情况应该怎么处理？JDK7如何处理攻击原理： 当客户端发送一个请求到服务器，如果该请求中带有参数，服务器端会将 参数名-参数值 作为 key-value 保存在 HashMap 中。如果有人恶意构造请求，在请求中加入大量相同 hash 值的 String 参数名（key），那么在服务器端用于存储这些 key-value 对的 HashMap 会被强行退化成链表，如图： 如果数据量足够大，那么在查找，插入时会占用大量 CPU，达到拒绝服务攻击的目的。 怎么处理 限制 POST 和 GET 请求的参数个数 限制 POST 请求的请求体大小 Web Application FireWall（WAF） JDK7如何处理 HashMap 会动态的使用一个专门 TreeMap 实现来替换掉它。 8. Hashmap为什么大小是2的幂次首先来看一下 hashmap 的 put 方法的源码 1234567891011121314151617181920212223public V put(K key, V value) { if (key == null) return putForNullKey(value); //将空key的Entry加入到table[0]中 int hash = hash(key.hashCode()); //计算key.hashcode()的hash值，hash函数由hashmap自己实现 int i = indexFor(hash, table.length); //获取将要存放的数组下标 /* * for中的代码用于：当hash值相同且key相同的情况下，使用新值覆盖旧值（其实就是修改功能） */ //注意：for循环在第一次执行时就会先判断条件 for (Entry&lt;K, V&gt; e = table[i]; e != null; e = e.next) { Object k; //hash值相同且key相同的情况下，使用新值覆盖旧值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; //e.recordAccess(this); return oldValue;//返回旧值 } } modCount++; addEntry(hash, key, value, i);//增加一个新的Entry到table[i] return null;//如果没有与传入的key相等的Entry，就返回null} 1234567/** * &quot;按位与&quot;来获取数组下标 */static int indexFor(int h, int length) { return h &amp; (length - 1);} hashmap 始终将自己的桶保持在2n，这是为什么？indexFor这个方法解释了这个问题 大家都知道计算机里面位运算是基本运算，位运算的效率是远远高于取余 % 运算的 举个例子：2n 转换成二进制就是 1+n 个 0，减 1 之后就是 0+n个1，如16 -&gt; 10000，15 -&gt; 01111 那么根据 &amp; 位运算的规则，都为 1 (真)时，才为 1，那 0≤运算后的结果≤15，假设 h &lt;= 15，那么运算后的结果就是 h 本身，h &gt;15，运算后的结果就是最后四位二进制做 &amp; 运算后的值，最终，就是 % 运算后的余数。 当容量一定是 2n 时，h &amp; (length - 1) == h % length 更新日志 2018/8/3 v2.5 基础版 2018/9/1 v3.0 初稿版","link":"/2019/05/16/Java%E9%9B%86%E5%90%88%E6%A1%86%E6%9E%B6/"},{"title":"MMySQL(九)：MVCC能否解决幻读问题","text":"幻读【前后多次读取，数据总量不一致】 同一个事务里面连续执行两次同样的sql语句，可能导致不同结果的问题，第二次sql语句可能会返回之前不存在的行。 事务A执行多次读取操作过程中，由于在事务提交之前，事务B（insert/delete/update）写入了一些符合事务A的查询条件的记录，导致事务A在之后的查询结果与之前的结果不一致，这种情况称之为幻读。 MVCC能否解决幻读问题 首先可以明确的是，MVCC在快照读的情况下可以解决幻读问题，但是在当前读的情况下是不能解决幻读的。 1、快照读和当前读mysql里面实际上有两种读取的方式：快照读和当前读，在之前的文章中《MySQL(八)：MVCC多版本并发控制》也有介绍，这里重新简单回顾一下。 快照读【Consistent Read】 也叫普通读，读取的是记录数据的可见版本，不加锁，不加锁的普通select语句都是快照读，即不加锁的非阻塞读。 快照读的执行方式是生成 ReadView，直接利用 MVCC 机制来进行读取，并不会对记录进行加锁。 如下语句： 1select * from table; 当前读 也称锁定读【Locking Read】，读取的是记录数据的最新版本，并且需要先获取对应记录的锁。如下语句： 12345SELECT * FROM student LOCK IN SHARE MODE; # 共享锁SELECT * FROM student FOR UPDATE; # 排他锁INSERT INTO student values ... # 排他锁DELETE FROM student WHERE ... # 排他锁UPDATE student SET ... # 排他锁 2、MVCC能解决幻读问题的场景当我们在读取数据的时候是【快照读】的情况下是可以解决【幻读】的问题，其原理就是MVCC。 下面使用案例说明： 假设表中有三条数据，以及有两个事物A/B，A读取数据，B插入数据 1234#事物A：select name from user where id &gt; 3;#事物B：insert into user valus('6','edwin'); 执行过程 时间 事务A 事物C 1 开始事务 2 第一次查询：select name from user where id &gt; 3; 6 开始事务 7 执行插入：insert into user valus(‘6’,’edwin’); 8 提交事务 9 第二次查询：select name from user where id &gt; 3; 10 提交事务 由于采用的是【快照读】的方式，在A事物开启时会产生一个版本快照，产生版本快照如下： 然后通过MVCC的【Read View】对版本快照中各个版本链中的数据进行可见性判断，读取相应的数据版本。两次查询结果都是【id=4，5】两条数据。 Read View具体可见性规则判断在之前的文章中《MySQL(八)：MVCC多版本并发控制》有详细的图文详解，这里就不再赘述。 因此，即使事务B新插入了数据，由于已经生成了版本快照，也不会影响Read View的可见性规则判读，所以在【快照读】的情况下，使用MVCC不会产生幻读问题。 3、MVCC不能解决幻读问题的场景3.1、MVCC什么场景下不能解决幻读问题当我们在读取数据的时候是【当前读】的情况下，无法使用MVCC解决幻读问题。 案例说明：还是先准备几条数据， 有两个事物A/B，A先读取数据，在修改数据，最后有读取数据，B插入数据，看看结果会什么 123456#事物A：select name from user where id &gt; 3;#事物B：insert into user valus('6','edwin');#事物A：update user set name = '彬' where id = 6; 执行过程 时间 事务A 事物C 1 开始事务 2 第一次查询：select name from user where id &gt; 3; 6 开始事务 7 执行插入：insert into user valus (‘6’,’edwin’); 8 提交事务 9 第二次查询：select name from user where id &gt; 3; 10 修改数据:update user set name = ‘彬’ where id = 6; 11 第三次查询：select name from user where id &gt; 3; 12 提交事务 在时间点为9的时候 事务A的【第一次】与【第二次】查询结果与上面的快照读是一样的，基于MVCC两次查询结果都是【id=4，5】两条数据。 在时间点为10的时候 事务A修改了事务B插入的数据，**由于update是当前读，所以此时会读取最新的数据(包括其他已经提交的事务)**。 在时间点为11的时候 事务A执行【第三次】查询，是基于当前最新版本查询的，所以会查询到事务B插入的【id=6】的数据，一共会查询到三条数据【id=4，5，6】，与前两次查询结果不同，从而产生了幻读。 3.2、如何解决当前读的幻读问题在可重复读(RR)的隔离级别下，执行当前读， 案例说明：还是使用上述的数据 事务A，执行当前读，查询id&gt;3的所有记录。 事务B，插入id=5的一条数据。 1234#事物A：select name from user where id &gt; 3 lock in share mode;#事物B：insert into user valus('6','edwin'); 执行过程 时间 事务A 事物B 1 开始事务 2 第一次查询：select name from user where id &gt; 3 lock in share mode; 6 开始事务 7 执行插入时发现，id&gt;3的范围有间隙锁，插入阻塞，处于等待状态 8 第二次查询：select name from user where id &gt; 3; 9 提交事务 10 事物A提交，间隙锁释放，执行插入：insert into user valus (‘6’,’edwin’); 11 提交事务 事务A在执行当前读【select … lock in share mode】的时候，在【id=4，5】的记录上加了共享锁，并且在【id &gt; 6】这个范围上也加了间隙锁，所以上图中的事务B执行插入操作时被阻塞了。所以事务A两次读取的数据是一样的。因此，在这种情况下是不会存在幻读问题。 总结： RR隔离级别下，当前读执行如下语句时会带上锁之外，还会使用间隙锁+临键锁，锁住索引记录之间的范围，避免范围间插入记录，以避免产生幻影行记录， 12345SELECT * FROM student LOCK IN SHARE MODE; # 共享锁SELECT * FROM student FOR UPDATE; # 排他锁INSERT INTO student values ... # 排他锁DELETE FROM student WHERE ... # 排他锁UPDATE student SET ... # 排他锁 注意 这种方式不能解决3.1中的幻读问题，因为在3.1中事务A执行修改数据，获取锁之前，已经读取到了事务B插入的数据，并且已经记录到Undo日志中。","link":"/2021/08/06/MySQL%E4%B9%9D%EF%BC%9AMVCC%E8%83%BD%E5%90%A6%E8%A7%A3%E5%86%B3%E5%B9%BB%E8%AF%BB%E9%97%AE%E9%A2%98/"},{"title":"MySQL二：SQL运行机制","text":"MySQL用了很久，但是一直也是工作的使用，对于MySQL的知识点都比较零散碎片，一直也没有整体梳理过，趁着最近不忙，梳理一下相关的知识点。 一、 MySQL的起源MySQL是一个开源的关系数据库管理系统。原开发者为瑞典的 MySQL AB公司，2008 年AB公司被Sun公司收购，并发布收购之后的首个版本 MySQL5.1。2010 年 Oracle 收购 Sun 公司，至此MySQL归入Oracle门下，之后发布了收购以后的首个版本 5.5 。 MySQL5.1 ，该版本引入分区、基于行复制以及plugin API 。 MySQL 5.5 ，改善集中在性能、扩展性、复制、分区以及对 windows 的支持。 「MySQL通过其【插件式的存储引擎架构】，将查询处理和其它的系统任务以及数据的存储提取分离来。并且根据不同的使用场景选择合适的存储引擎，使其在不同应用都能发挥良好作用。」 二、MySQL执行过程在逻辑上MySQL 在执行脚本时自上而下可以分为四层，逻辑图如下： 「sql执行流程解析」 首先客户端（jdbc，PHP）通过连接处理层连接mysql服务器，然后解析器通过解析树对sql语句进行解析，优化器对sql语句进行优化，最终调用第四层的存储引擎的接口，执行语句。 三、MySQL Server基本架构组成「MySQL Server架构自顶向下大致可以分网络连接层、服务层、存储引擎层和系统文件层」。架构图如下 3.1 第一层：网络连接层「客户端连接器，MySQL向外提供交互接口连接各种不同的客户端。」 几乎支持所有主流的服务端编程技术，如常见的 Java、C、Python、.NET等，都通过各自API与MySQL建立连接。 3.2 第二层：服务层MySQL Server的核心，主要包含系统「管理和控制工具、连接池、SQL接口、解析器、查询优化器和缓存」六个部分。 「连接池（Connection Pool）」 「负责存储和管理客户端与数据库的连接，一个线程负责管理一个连接。」 在服务器内部，每个client连接都有自己的线程。这个连接的查询都在一个单独的线程中执行。这些线程轮流运行在某一个CPU内核(多核CPU)或者CPU中。服务器缓存了线程，因此不需要为每个client连接单独创建和销毁线程 。 「系统管理和控制工具（Management Services &amp; Utilities）」 备份恢复、安全管理、集群管理等 「SQL接口（SQL Interface）」 用于接受客户端发送的SQL命令，并且返回查询的结果。 比如DML、DDL、存储过程、视图、触发器等。 「解析器（Parser）」 负责将请求的SQL解析生成一个【解析树】。根据MySQL规则进一步检查解析树是否合法。 「查询优化器（Optimizer）」 当【解析树】通过解析器语法检查后，将交「由优化器将其转化成执行计划与存储引擎交互」。一般执行sql脚本会遵循【「选取–&gt;投影–&gt;联接」】的策略 1selectid,namefromuserwhere gender=1; 执行以上sql脚本的过程： select先根据where语句进行选取，并不是查询出全部数据再过滤 select查询根据uid和name进行属性投影，并不是取出所有字段 将前面选取和投影联接起来最终生成查询结果 「缓存（Cache&amp;Buffer）」 缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，权限缓存，引擎缓存等。 「如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据。」 3.3 第三层：存储引擎层「存储引擎负责MySQL中数据的存储与提取，与底层系统文件进行交互。」 MySQL存储引擎是插件式的，服务器中的查询执行引擎通过【「接口」】与存储引擎进行通信，接口屏蔽了不同存储引擎之间的差异 。通过上图可以看出MySQL有好几种不同的存储引擎，最常见的是MyISAM和InnoDB。 3.4 第四层：系统文件层「主要是将数据和日志存储在运行设备的文件系统之上，并完成于存储引擎的交互，是文件的物理存储层。」 主要包含日志文件，数据文件，配置文件，pid 文件，socket 文件等。 「日志文件」 【事务日志】 包含重做日志（Redo Log）和撤销日志（Undo Logs），在后续文章中详细介绍 【错误日志（Error log）】 1show variables like '%log_error%' --默认开启 【二进制日志（bin log）】「记录对MySQL数据库执行的更改操作，包括语句的发生时间、执行时长，主要用于数据库恢复和主从复制」。但不记录select、show等不修改数据库的SQL。 123showvariableslike'%log_bin%'; --是否开启showvariableslike'%binlog%'; --参数查看showbinarylogs;--查看日志文件 【通用查询日志（General query log）】 1showvariableslike'%general%';--记录一般查询语句 【慢查询日志（Slow query log） 】 记录所有执行时间超时的查询SQL，默认是10秒。 12showvariableslike'%slow_query%'; //是否开启showvariableslike'%long_query_time%'; //超时时间 「数据文件」 【db.opt 文件】 记录当前库默认使用的字符集和校验规则。 【frm 文件】 存储与表相关的元数据（meta）信息，包括表结构的定义信息等，每一张表都会有一个frm 文件。 【MYD 文件】 「MyISAM 存储引擎专用」，存放 MyISAM 表的数据（data)，每一张表都会有一个.MYD 文件。 【MYI 文件】 「MyISAM 存储引擎专用」，存放 MyISAM 表的索引相关信息，每一张 MyISAM 表对应一个 .MYI 文件。 【ibd文件和 IBDATA 文件】 「存放 InnoDB 的数据文件（包括索引）」。InnoDB 存储引擎有两种表空间方式：独享表空间和共享表空间。 「独享表空间」使用 .ibd 文件来存放数据，且每张InnoDB 表对应一个 .ibd 文件。 「共享表空间」使用 .ibdata 文件，所有表共同使用一个（或多个，自行配置）.ibdata 文件。 【ibdata1 文件】 系统表空间数据文件，存储表元数据、Undo日志等 。 【ib_logfile0、ib_logfile1 文件】 Redo log 日志文件。 「配置文件」 用于存放MySQL所有的配置信息文件，比如my.cnf、my.ini等。 「pid 文件」 pid 文件是 mysqld 应用程序在 Unix/Linux 环境下的一个进程文件，和许多其他 Unix/Linux 服务端程序一样，存放着自己的进程 id。 「socket文件」 socket 文件也是在 Unix/Linux 环境下才有的，用户在 Unix/Linux 环境下客户端连接可以不通过TCP/IP 网络而直接使用 Unix Socket 来连接 MySQL。","link":"/2021/08/06/MySQL%E4%BA%8C%EF%BC%9ASQL%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6/"},{"title":"MySQL五：InnoDB线程模型","text":"一、InnoDB线程模型的组成在Innodb存储引擎中，后台线程的主要作用是「负责刷新内存池中的数据，保证缓冲池中的内存缓存的是最近的数据」。此外它会将已经修改的数据文件刷新到磁盘文件中，保证在发生异常的情况下，Innodb能够恢复到正常的运行状态。 「InnoDB存储引擎是多线程的模型，所以有多个不同的后台线程，负责处理不同的任务」。主要有： Master Thread、IO Thread、Purge Thread、Page Cleaner Thread四种。 二 Master Thread「Master thread是InnoDB的主线程，负责调度其他各线程，优先级最高」。 「主要作用」 将缓冲池中的数据一步刷新到磁盘，保证数据的一致性。 「主要工作」 脏页的刷新（page cleaner thread）、undo页回收（purge thread）、redo日志刷新（log thread）、合并写缓冲等。 Master thread内部有两个主要处理时机，分别是每隔1秒和10秒处理。 每隔1秒 innodb_max_dirty_pages_pct innodb_io_capacity 合并插入缓冲时,每秒合并插入缓冲的数量为 innodb_io_capacity值的5%，默认就是 200*5%=10 在从缓冲区刷新脏页时,每秒刷新脏页的数量就等于innodb_io_capacity的值，默认200 刷新日志缓冲区，刷到磁盘 合并写缓冲区数据，根据IO读写压力来决定是否操作 刷新脏页数据到磁盘，根据脏页比例达到75%才操作，此处涉及两个参数 脏页比例通过innodb_max_dirty_pages_pct配置，innodb_max_dirty_pages_pct参数值保存在变量srv_max_buf_pool_modified_pct 里面，这是一个全局变量，初始值为 75.0 每隔10秒 刷新脏页数据到磁盘 合并写缓冲区数据 刷新日志缓冲区 删除无用的undo页 三、 IO Thread「为了提高数据库的性能，在InnoDB中使用了大量的AIO（Async IO）来做读写处理」。 一共有4种总共10个IO Thread： 「read thread」（4个） 负责读取操作，将数据从磁盘加载到Buffer Pool的Page页。 「write thread」（4个） 负责写操作，将Buffer Pool的dirty page刷新到磁盘。 1show variables like &quot;%innodb%io_threads%&quot;; 「log thread」（1个） 负责将Log Buffer内容刷新到磁盘。 「insert buffer thread」（1个） 负责将Change Buffer内容刷新到磁盘。 12#查看当前IO线程的工作状态show engine innodb status; 四 Purge Thread事务提交之后，其使用的undo日志将不再需要，因此需要Purge Thread回收已经分配的undo 页。 早前的版本只支持一个Purge Thread，目前mysql 5.7版本支持多个Purge Thread，目的是为了进一步加快undo数据页的回收速度。 1show variables like '%innodb_purge_threads%'; 五 Page Cleaner Thread「作用是将脏数据放入到单独的线程中刷新到磁盘，脏数据刷盘后相应的redo log也就可以覆盖，即可以同步数据，又能达到redo log循环使用的目的」。 减轻原来的Master Thread的工作，同时可以缓解用户查询线程的阻塞，进一步提高Innodb 存储引擎的性能。 1show variables like '%innodb_page_cleaners%';","link":"/2021/08/06/MySQL%E4%BA%94%EF%BC%9AInnoDB%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/"},{"title":"MySQL(八)：读懂MVCC多版本并发控制","text":"mysql在并发的情况下，会引起脏读，幻读，不可重复读等一系列的问题，为解决这些问题，引入了mvcc的机制。本文就详细看看mvcc是怎么解决脏读，幻读等问题的。 1、 数据库事务1.1 事务事务是操作数据库的最小单元，将【多个任务作为单个逻辑工作单元】执行的一系列数据库操作，他们作为一个整体一起向数据库提交，要么都执行、要么都不执行。 大白话解释： 事务就是当要完成一件事件，这件事又包含多个任务的时候，只有当所有的任务都执行成功，则认为这个事情是成功；只要有其中一个任务没有执行成功，则认为这件事执行失败，其他的执行成功的任务也要回滚到未执行的状态。 开启事务【开始记录一个事情中的多个任务】 执行事务【正常情况下，一条语句就是一个任务】 提交事务【成功】| 回滚事务【失败】 事务的作用：保证数据的最终一致性。 1.2 事务四大特性事务四大特性即ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。 原子性（Atomicity） 事务是操作数据库的最小单元，作为一个整体被执行，包含一个事务中的所有操作要么全部都执行，要么全部失败回滚。 一致性（Consistency） 事务必须使数据库从一个一致性状态转换到另一个一致性状态，即在事务开始之前和事务结束以后，数据不会被破坏，保持一致性。 假如A账户给B账户转100块钱，不管事务是否成功，A账户给B账户的总金额是不变的。 隔离性（Isolation） 当多个事务并发访问数据库时，一个事务不应该被其他事务干扰，多个并发事务之间是相互隔离的。 持久性（Durability） 事务一旦完成后被提交，该事务对数据库所作的操作更改，将持久地保存在数据库之中。 1.3 并发下的事务问题虽然事务能保持数据最终一致性，但是在并发下执行事务，发会引起脏读、不可重复读、幻读等问题。 脏读【读取未提交数据】 如果一个事务读取到了另一个未提交事务修改过的数据，称发生了脏读。 一般事务的脏读都是拿转账的案例说明，这里也转账和取款为案例： 时间 事务A：转账 事务B:取款 1 开始事务 2 开始事务 3 查询账户余额为10000元 4 执行取款操作，取款3000元，余额更改为7000元 5 查询账户余额为7000元（产生脏读） 6 取款失败，回滚事务，余额还原为10000元 7 转入5000元，余额被更改为12000元（脏读的7000+5000） 8 提交事务 从上述执行过程的结果，最后账户余额为12000元，但是实际上B取款失败，余额为10000，加上A转入的5000元，账户最终的余额应该为15000元，平白无故少了3000元，这就是脏读。银行肯定是不允许这种事情发生的，不然就没人敢在银行存钱了…… 不可重复读【前后多次读取，数据内容不一致】 同一个事务内，前后多次读取，读取到的数据内容不一致，称之为不可重复读。 还是以转账的案例： 时间 事务A：查询 事务B:取款 1 开始事务 2 第一次查询，账户的余额为10000元 3 开始事务 4 执行取款操作，取款3000元，余额更改为7000元 5 提交事务 6 第二次查询，账户的余额为7000元 7 提交事务 从上述案例描述中可以看出，事务A执行的过程中，事务B修改了账户余额，导致事务A中的两次查询结果不一致，这就是不可重复读，对于事务A而言莫名其妙的余额变少了，那肯定不干…… 幻读【前后多次读取，数据总量不一致】 事务A执行多次读取操作过程中，由于在事务提交之前，事务B（insert/delete/update）写入了一些符合事务A的查询条件的记录，导致事务A在之后的查询结果与之前的结果不一致，这种情况称之为幻读。 以student表中的数据为例： 依次执行下面这两个语句 1234#查询语句select * from student where id &gt; 2;#写入语句insert into student(id,c_id,name,sex,score) value(6,2,'吕布','男',89); 时间 事务A：读取 事务B:写入 1 开始事务 2 第一次执行查询语句，结果为3条数据结果 3 开始事务 4 执行写入语句，插入一条ID为6的数据 5 提交事务 6 第二次执行查询语句，结果为4条数据结果 7 提交事务 从上述案例描述中可以看出，事务A在前后两次执行的过程中，由于事务B插入了满足查询语句的数据，导致事务A两次查询结果的总数不一样，这就是幻读。 总结 一般我们再理解幻读与不可重复读的时候，容易混淆，其实只需要分清一点就可以， 一般而言：幻读是指查询数据的【条数总量】不一致，不可重复读是指查询数据的数据内容不一致。 1.4 事务的四大隔离级别数据库设计了四种隔离级别：串行化(Serializable)、可重复读(Repeatable read)、读已提交(Read committed)、读未提交(Read uncommitted)**，用来解决并发事务存在的脏读、不可重复读、幻读**等问题。 读未提交(Read uncommitted) 在读未提交的隔离级别下，所有事务能够读取【其他事务未提交】的数据。 读取其他事务未提交的数据，会造成脏读。因此在该种隔离级别下，不能解决脏读、不可重复读和幻读。 读已提交(Read committed) 在读已提交的隔离级别下，所有事务只能读取【其他事务已经提交】的数据。Oracle和SQL Server的默认的隔离级别。 读已提交能够解决脏读的现象，但是还是会有不可重复读、幻读的问题 读已提交会有一个事务的前后多次的查询中却返回了不同内容的数据的现象。 可重复读(Repeatable read) 在可重复读的隔离级别下，限制了读取数据的时候，不可以进行修改，所有事务前后多次的读取到的数据内容是不变的。mysql的默认事务隔离级别 这种隔离级别解决了重复读的问题，但是读取范围数据的时候，是可以add数据的，所以还是会造成某个事务前后多次读取到的数据总量不一致的现象，从而产生幻读。 针对以上问题，一般我们也可以使用间隙锁和临键锁来解决幻读问题，这个以后再讲 串行化(Serializable) 事务最高的隔离级别，在串行化的隔离级别下，所有的事务顺序执行，不存在任何冲突，可以避免脏读、不可重复读与幻读所有并发问题。 但是串行化的隔离级别，会导致大量的操作超时和锁竞争，从而大大降低数据库的性能，一般不使用这样事务隔离级别。 四种隔离级别存在的并发问题如下： 【 ×】表示未解决，【√】表示已解决 隔离级别 脏读 不可重复读 幻读 读未提交(Read uncommitted) × × × 读已提交(Read committed) √ × × 可重复读(Repeatable read) √ √ × 串行化(Serializable) √ √ √ 2、MVCC基础概念 数据库通过加锁，可以实现事务的隔离性，串行化隔离级别就是加锁实现的，但是加锁会降低数据库性能。 因此，数据库引入了MVCC多版本并发控制，在读取数据不用加锁的情况下，实现读取数据的同时可以修改数据，修改数据时同时可以读取数据。 2.1 什么是MVCCMVCC(Mutil-Version Concurrency Control)，多版本并发控制。是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问。用于支持读已提交(RC）和可重复读(RR）隔离级别的实现。 MVCC在MySQL InnoDB引擎中的实现主要是为了在处理读-写冲突时提高数据库并发性能，记录读已提交和可重复读这两种隔离级别下事务操作版本连的过程。 数据库并发场景一般有三种： 读-读：不存在任何问题，不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，可能会有脏读，幻读，不可重复读 写-写：有线程安全问题，可能会存在更新丢失问题。 MVCC主要是用来解决【读-写】冲突的无锁并发控制，可以解决以下问题： 在并发读写数据时，可以做到在读操作时不用阻塞写操作，写操作不用阻塞读操作，提高数据库并发读写的性能。 可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决【写-写】引起的更新丢失问题。 MVCC与锁的组合： 一般数据库中都会采用以上MVCC与锁的两种组合来解决并发场景的问题，以此最大限度的提高数据库性能。 MVCC + 悲观锁MVCC解决读-写冲突，悲观锁解决写-写冲突。 MVCC + 乐观锁MVCC解决读-写冲突，乐观锁解决写-写冲突。 通过上述描述，MVCC的作用可以概括为就是为了解决【读写冲突】，提高数据库性能的，而MVCC的实现又依赖于六个概念：【隐式字段】【undo日志】【版本链】【快照读和当前读】【读视图】。 2.2 隐式字段在InnoDB存储引擎，针对每行记录都有固定的两个隐藏列【DB_TRX_ID】【DB_ROLL_PTR】以及一个可能存在的隐藏列【DB_ROW_ID】。 隐式字段 描述 是否必须存在 DB_TRX_ID 事物Id，也叫事物版本号，占用6byte的标识，事务开启之前，从数据库获得一个自增长的事务ID，用其判断事务的执行顺序 是 DB_ROLL_PTR 占用7byte，回滚指针，指向这条记录的上一个版本的undo log记录，存储于回滚段（rollback segment）中 是 DB_ROW_ID 隐含的自增ID（隐藏主键），如果表中没有主键和非NULL唯一键时，则会生成一个单调递增的行ID作为聚簇索引 否 表中的数据会因此分为两种形式： 有主键或唯一非空字段 没有主键且没有唯一非空字段 2.3 undo日志一种用于撤销回退的日志，在事务开始之前，会先记录存放到 Undo 日志文件里，备份起来，当事务回滚时或者数据库崩溃时用于回滚事务。undo日志的详细介绍在之前的《MySQL(七)：一文详解六大日志》中有详细介绍。 undo日志的主要作用是事务回滚和实现MVCC快照读。 undo log日志分为两种： insert undo log**代表事务在insert新记录时产生的undo log, 仅用于事务回滚，并且在事务提交后可以被立即丢弃**。 update undo log**事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在实现MVCC快照读时也需要**；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被清理线程统一清除。 MVCC实际上是使用的update undo log 实现的快照读。 InnoDB 并不会真正地去开辟空间存储多个版本的行记录，只是借助 undo log 记录每次写操作的反向操作。所以B+ 索引树上对应的记录只会有一个最新版本，InnoDB 可以根据 undo log 得到数据的历史版本，从而实现多版本控制。 2.4 版本链 一致性非锁定读是通过 MVCC 来实现的。但是MVCC 没有一个统一的实现标准，所以各个存储引擎的实现机制不尽相同。InnoDB 存储引擎中 MVCC 的实现是通过 undo log 来完成的 当事务对某一行数据进行改动时，会产生一条Undo日志，多个事务同时操作一条记录时，就会产生多个版本的Undo日志，这些日志通过回滚指针（DB_ROLL_PTR）连成一个链表，称为版本链。 只要有事务写入数据时，就会产生一条对应的 undo log，一条 undo log 对应这行数据的一个版本，当这行数据有多个版本时，就会有多条 undo log 日志，undo log 之间通过回滚指针（DB_ROLL_PTR）连接，这样就形成了一个 undo log 版本链。 2.5 快照读和当前读 快照读【Consistent Read】 也叫普通读，读取的是记录数据的可见版本，不加锁，不加锁的普通select语句都是快照读，即不加锁的非阻塞读。 快照读的执行方式是生成 ReadView，直接利用 MVCC 机制来进行读取，并不会对记录进行加锁。 如下语句： 1select * from table; 当前读 也称锁定读【Locking Read】，读取的是记录数据的最新版本，并且需要先获取对应记录的锁。如下语句： 12345SELECT * FROM student LOCK IN SHARE MODE; # 共享锁SELECT * FROM student FOR UPDATE; # 排他锁INSERT INTO student values ... # 排他锁DELETE FROM student WHERE ... # 排他锁UPDATE student SET ... # 排他锁 2.6 读视图【Read View】Read View提供了某一时刻事务系统的快照，主要是用来做可见性判断, 里面保存了【对本事务不可见的其他活跃事务】。 当事务在开始执行的时候，会产生一个读视图（Read View），用来判断当前事务可见哪个版本的数据，即可见性判断。 实际上在innodb中，每个SQL语句执行前都会生成一个Read View。 2.6.1 读视图的四个属性MySQL5.7源码中对Read View定义了四个属性，如下： 123456789101112131415161718192021222324252627282930313233class ReadView { private: /** The read should not see any transaction with trx id &gt;= this value. In other words, this is the &quot;high water mark&quot;. */ trx_id_t m_low_limit_id; /** The read should see all trx ids which are strictly smaller (&lt;) than this value. In other words, this is the low water mark&quot;. */ trx_id_t m_up_limit_id; /** trx id of creating transaction, set to TRX_ID_MAX for free views. */ trx_id_t m_creator_trx_id; /** Set of RW transactions that was active when this snapshot was taken */ ids_t m_ids; /** The view does not need to see the undo logs for transactions whose transaction number is strictly smaller (&lt;) than this value: they can be removed in purge if not needed by other views */ trx_id_t m_low_limit_no; /** AC-NL-RO transaction view that has been &quot;closed&quot;. */ bool m_closed; typedef UT_LIST_NODE_T(ReadView) node_t; /** List of read views in trx_sys */ byte pad1[64 - sizeof(node_t)]; node_t m_view_list;}; creator_trx_id 创建当前read view的事务ID m_ids 当前系统中所有的活跃事务的 id，活跃事务指的是当前系统中开启了事务，但还没有提交的事务; m_low_limit_id 表示在生成ReadView时，当前系统中活跃的读写事务中最小的事务id，即m_ids中的最小值。 m_up_limit_id 当前系统中事务的 id 值最大的那个事务 id 值再加 1，也就是系统中下一个要生成的事务 id。 ReadView 会根据这 4 个属性，结合 undo log 版本链，来实现 MVCC 机制，决定一个事务能读取到数据那个版本。 假设现在有事务 A 和事务 B 并发执行，事务 A 的事务 id 为 10，事务 B 的事务 id 为 20。 事务A的ReadView ：m_ids=[10,20]，m_low_limit_id=10，m_up_limit_id=21，creator_trx_id=10。 事务B的ReadView ：m_ids=[10,20]，m_low_limit_id=10，m_up_limit_id=21，creator_trx_id=20。 2.6.2 读视图可见性判断规则将Read View中的活跃事务Id按照大小放在坐标轴上表示的话，如下图： 当一个事务读取某条数据时，会通过DB_TRX_ID【Uodo日志的事务Id】在坐标轴上的位置来进行可见性规则判断，如下： DB_TRX_ID &lt; m_low_limit_id 表示DB_TRX_ID对应这条数据【Undo日志】是在当前事务开启之前，其他的事务就已经将该条数据修改了并提交了事务(事务的 id 值是递增的)，所以当前事务【开启Read View的事务】能读取到。 DB_TRX_ID &gt;= m_up_limit_id 表示在当前事务【creator_trx_id】开启以后，有新的事务开启，并且新的事务修改了这行数据的值并提交了事务，因为这是【creator_trx_id】后面的事务修改提交的数据，所以当前事务【creator_trx_id】是不能读取到的。 m_low_limit_id =&lt; DB_TRX_ID &lt; m_up_limit_id DB_TRX_ID 在 m_ids 数组中 表示DB_TRX_ID【写Undo日志的事务】 和当前事务【creator_trx_id】是在同一时刻开启的事务 DB_TRX_ID 不等于creator_trx_id **DB_TRX_ID事务修改了数据的值，并提交了事务，所以当前事务【creator_trx_id】不能读取到。** - **DB_TRX_ID 等于creator_trx_id** ​ 表明数据【Undo日志】 是自己生成的，因此是可见的 DB_TRX_ID 不在 m_ids 数组中 表示的是在当前事务【creator_trx_id】开启之前，其他事务【DB_TRX_ID】将数据修改后就已经提交了事务，所以当前事务能读取到。 2.6.3 读视图可见性判断规则案例说明了解了读视图可见性判断规则，下面通过一个场景案例图解的方式来详细逐条验证上述规则。一般来说，我们的行数据结构都为一下模式： 假设有一个事物【DB_TRX_ID = 10】在表中插入了一条数据，则它的数据结构为为： 【第一步】：假设现在有事务 A【DB_TRX_ID = 20】 和事务 B 【DB_TRX_ID = 30】并发执行 1234#事物A：select name from user where id = 1;#事物B：update user set name = 'edwin' where id = 1; 事物开始后分别生成ReadView 事务A的ReadView ：m_ids=[20,30]，m_low_limit_id=20，m_up_limit_id=31，creator_trx_id=20。 事务B的ReadView ：m_ids=[20,30]，m_low_limit_id=20，m_up_limit_id=31，creator_trx_id=30。 【第二步】：事物A开启事物之后通过版本链第一次读取数据，版本链中的DB_TRX_ID = 10，小于事物A的【DB_TRX_ID = 20】，说明DB_TRX_ID = 10这条数据是事物A开启之前就已经写入，并提交了事物，所以事物A可以读取到。 【第三步】：事务 B 【DB_TRX_ID = 30】修改数据，将name修改为Edwin，修改后写入Undo Log日志，此时还没有提交事务B。示意图如下： 【第四步】：事务A【DB_TRX_ID = 20】第二次去读取数据 在 undo log版本链中，数据最新版本的事务id为30，这个值处于事务A的 ReadView 里 m_low_limit_id 和 m_up_limit_id 并且存在于m_ids 数组中，表示这个版本的数据是和自己同一时刻启动的事务修改的，因此这个版本的数据，数据 A 读取不到。 此时需要沿着 undo log 的版本链向前找，接着会找到该行数据的上一个版本db_trx_id=10，由于db_trx_id=10小于 m_low_limit_id的值，因此事务 A 能读取到该版本的值，即事务 A 读取到的值是星之码。 【第五步】：现在事务 B 提交，此时系统中活跃的事务只有事物A，事物A第三次读取，读取到内容就有两种可能性： 这里留一个问题一：造成这两种情况的原因是什么？ 我们留到本文第三节【不同隔离级别MVCC实现原理】中说明，继续案例 读已提交（RC）隔离级别：读取到是事物B提交的Edwin。 可重复读（RR）隔离级别：读取到是原始数据提交的星河之码。 【第六步】：新的事物C【DB_TRX_ID = 40】修改数据，将name修改为彬 12#事物C：update user set name = '彬' where id = 1; 执行脚本前生成的ReadView如下，执行脚本后，提交事物C。 事务C的ReadView ：m_ids=[20,40]，m_low_limit_id=20，m_up_limit_id=41，creator_trx_id=40。 【第七步】：事务 A【DB_TRX_ID = 20】第四次读取数据， 此时由于事物A，由于事物A的m_up_limit_id=31，而日志中的DB_TRX_ID=40，根据可见性判断规则可以知到，事物A不能读取到DB_TRX_ID=40的记录，按照版本链的DB_POLL_PTR继续往上找，找到DB_TRX_ID=30的记录，虽然30在事物A的的m_ids=[20,30]，但是DB_TRX_ID=30不等于事物A的creator_trx_id=20，所以还是不能读取，继续往上找，最终读取到了DB_TRX_ID=10的记录，name=星河之码 实际上，这里事务A在不同场景下也是可以读取到DB_TRX_ID=40得数据的。 这里也留一个问题二：在什么场景下能够读取到DB_TRX_ID=40得数据name=彬呢？ 我们留到本文第三节【不同隔离级别MVCC实现原理】中说明，继续案例 【第八步】：事务 A【DB_TRX_ID = 20】开始修改数据，将name 修改为 ‘法外狂徒张三’ 12#事物A：update user set name = '法外狂徒张三' where id = 1; 此时事务A还没有提交，但是已经写入了Undo 日志，新的版本链如下 【第九步】：事务 A第五次读取数据 由于Undo日志中的最新数据DB_TRX_ID=20等于事物A的creator_trx_id=20，说明是自己修改的数据，可以查到，name=法外狂徒张三 通过以上九个步骤图解的方式，对读视图可见性判断规则做了分析，通过ReadView 和 undo log分析了MVCC 的实现原理，接下来结合事务的隔离级别，看看MVCC是怎么读取数据的。 3、不同隔离级别MVCC实现原理3.1 MVCC实现原理通过上述对【Read View】的分析可以总结出：InnoDB 实现MVCC是通过 Read View与Undo Log 实现的，Undo Log 保存了历史快照，形成版版本链，Read View可见性规则判断当前版本的数据是否可见。 InnnoDB执行查询语句的具体步骤为： 执行语句之前获取查询事务自己的事务Id，即事务版本号。 通过事务id获取Read View 查询存储的数据，将其事务Id与Read View中的事务版本号进行比较 不符合Read View的可见性规则，则读取Undo log中历史快照数据 找到当前事务能够读取的数据返回 而在实际的使用过程中，Read View在不同的隔离级别下是得工作方式是不一样。 3.2 读已提交（RC）MVCC实现原理在读已提交(Read committed)的隔离级别下实现MVCC，同一个事务里面，【每一次查询都会产生一个新的Read View副本】，这样可能造成同一个事务里前后读取数据可能不一致的问题（不可重复读并发问题）。 还是按照上述案例来说明一下： 【第一步】：准备一条原始数据 【第二步】：假设现在有事务 A【DB_TRX_ID = 20】 和事务 B 【DB_TRX_ID = 30】并发执行 1234#事物A：select name from user where id = 1;#事物B：update user set name = 'edwin' where id = 1; 执行过程为 时间 事务A 事务B 1 开始事务 2 第一次查询：select name from user where id = 1; 3 开始事务 4 执行修改：update user set name = ‘edwin’ where id = 1; 5 提交事务 6 第二次查询：select name from user where id = 1; 7 提交事务 版本链为： 案例结果分析： 上述案例在在读已提交(Read committed)的隔离级别下实现，同一个事务里面，【每一次查询都会产生一个新的Read View副本】。所以第二步实际上产生了三个Read View m_ids m_low_limit_id m_up_limit_id creator_trx_id 事务A：第一次查询Read View [20,30] 20 31 20 事务B：Read View [20,30] 20 31 30 事务A：第二次查询Read View [20] 20 31 20 通过可见性判断： 事务A第一次查询时 日志事务Id【DB_TRX_ID = 10】 &lt; 最小活跃事务ID【m_low_limit_id=20】，因此可以读取到DB_TRX_ID = 10这条版本链中的数据。即name = 星河之码。 事务A第二次查询时 此时事务B已经提交，版本链中最新版本为DB_TRX_ID = 30，而可见性规则中虽然满足 【m_low_limit_id=20】=&lt;【DB_TRX_ID=30】&lt;【m_up_limit_id=20】但是【DB_TRX_ID=30】不在m_ids集合[20]中，因此事务A的第二次查询可以读取【DB_TRX_ID=30】的数据，即name = edwin。 案例总结： 通过上述案例说明，同一个事务A的两个相同查询，第一次结果为星河之码，第二次结果为edwin，因此在读已提交（RC）隔离级别下，存在不可重复读并发问题。 此处也就解答了2.6.3中【第五步】的问题一中的第一种情况：读已提交（RC）隔离级别：读取到是事物B提交的Edwin。同样也解答了【第七步】的问题二，为什么能读取DB_TRX_ID=40得数据name=彬。 3.3 可重复读（RR）MVCC实现原理在可重复读(Repeatable read)的隔离级别下实现MVCC，【同一个事务里面，多次查询，都只会产生一个共用Read View】，以此不可重复读并发问题。 案例与3.2一样，这里就不重复赘述，可以再看一遍3.2的【第一步】【第二步】，直接进行案例分析 案例结果分析： 由于同一个事物只会产生一个共用Read View，所以可重复读的隔离级别下第二步只产生了两个Read View 上述案例在可重复读(Repeatable read)，【每一次查询都会产生一个新的Read View副本】。所以第二步实际上产生了三个Read View m_ids m_low_limit_id m_up_limit_id creator_trx_id 事务A：Read View [20,30] 20 31 20 事务B：Read View [20,30] 20 31 30 通过可见性判断： 事务A第一次查询时 日志事务Id【DB_TRX_ID = 10】 &lt; 最小活跃事务ID【m_low_limit_id=20】，因此可以读取到DB_TRX_ID = 10这条版本链中的数据。即name = 星河之码。 事务A第二次查询时 此时事务B已经提交，版本链中最新版本为DB_TRX_ID = 30，而可见性规则中虽然满足 【m_low_limit_id=20】=&lt;【DB_TRX_ID=30】&lt;【m_up_limit_id=20】并且【DB_TRX_ID=30】也在m_ids集合[20，30]中，但是【DB_TRX_ID=30】不等于事物A的【creator_trx_id=20】，说明DB_TRX_ID=30是同一时刻其他事物提交的，事物A不能读取到，因此事物A只能按照版本链继续往上找，最终读取到【DB_TRX_ID=10】的数据，即name = 星河之码。 案例总结： 通过上述案例说明，同一个事务A的两个相同查询，结果都为星河之码，因此在可重复读（RR）隔离级别下，解决了不可重复读并发问题。 其实读已经提交与可重复读的可见性判断的区别就在于事务A第二次查询时使用的Read View不通。 此处也就解答了2.6.3中【第五步】的问题一中的第二种情况：可重复读（RR）隔离级别：读取到是原始数据提交的星河之码。同样也解释了【第七步】，为什么能读取到的是DB_TRX_ID=10得数据name=星河之码。","link":"/2021/08/06/MySQL%E5%85%AB%EF%BC%9A%E8%AF%BB%E6%87%82MVCC%E5%A4%9A%E7%89%88%E6%9C%AC%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/"},{"title":"MySQL六：InnoDB数据文件","text":"一、数据文件的组成innodb数据逻辑存储形式为表空间,而每一个独立表空间都会有一个.ibd数据文件,ibd文件从大到小组成： 一个ibd数据文件–&gt;Segment（段）–&gt;Extent（区）–&gt;Page（页）–&gt;Row（行） 表空间(Tablesapce) 表空间，用于存储多个ibd数据文件，用于存储表的记录和索引，一个文件包含多个段。 段(Segment) 段由数据段、索引段、回滚段组成，innodb存储引擎索引与数据共同存储，数据段即是B+树叶节点，索引段则存储非叶节点。 区(Extent) 区则是由连续页组成，每个区的大小为1M，一个区中一共有64个连续的页。 页(Page) 页是innodb存储引擎磁盘管理的最小单位，页的大小为16KB，即每次数据的读取与写入都是以页为单位。 “ 包含很多种页类型，比如数据页，undo页，系统页，事务数据页，大的BLOB对象页 行(Row) 行包含记录的字段值，事务ID（Trx id）、滚动指针（Roll pointer）、字段指针（Field pointers）等信息。 二 InnoDB数据页结构InnoDB将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般为 16KB。 页的组成 如图所示，InnoDB数据页由以下七个部分组成， 2.1 File Header（文件头）File Header用来记录页的一些头信息，由如下8个部分组成，共占用38个字节，如表4-3所示： FIL_PAGE_SPACE_OR_CHKSUM 在MySQL4.0.14版本之前 该值代表该页属于哪个表空间，当innodb_file_per_table没有开启事，共享表空间中可能存放了许多页，并且这些页属于不同的表空间。 MySQL4.0.14之后版本 该值代表页的checksum值（一种新的checksum值）。 FIL_PAGE_OFFSET 表空间中页的偏移值。 FIL_PAGE_PREV，FIL_PAGE_NEXT 当前页的上一个页以及下一个页。B+Tree特性决定了叶子节点必须是双向列表。 FIL_PAGE_LSN 该值代表该页最后被修改的日志序列位置LSN（Log Sequence Number）。 FIL_PAGE_TYPE 页的类型。十六进制表示，0x45BF代表B+tree叶结点（存放数据的数据页）。 FIL_PAGE_FILE_FLUSH_LSN 该值仅在数据文件中的一个页中定义，代表文件至少被更新到了该LSN值。 FIL_PAGE_ARCH_LOG_NO_OR_SPACE_ID 从MySQL 4.1开始，该值代表页属于哪个表空间。 2.2 Page Header（页头）用来记录数据页的状态信息，由以下14个部分组成，共占用56个字节。 “ 比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等。 PAGE_N_DIR_SLOTS 在Page Directory（页目录）中的Slot（槽）数。 PAGE_HEAP_TOP 堆中第一个记录的指针。 PAGE_N_HEAP 堆中的记录数。 PAGE_FREE 指向空闲列表的首指针。 PAGE_GARBAGE 已删除记录的字节数，即行记录结构中，delete flag为1的记录大小的总数。 PAGE_LAST_INSERT 最后插入记录的位置。 PAGE_DIRECTION 最后插入的方向。取值为： PAGE_LEFT（0x01） PAGE_RIGHT（0x02） PAGE_SAME_REC（0x03） PAGE_SAME_PAGE（0x04） PAGE_NO_DIRECTION（0x05） PAGE_N_DIRECTION 一个方向连续插入记录的数量。 PAGE_N_RECS 该页中记录的数量。 PAGE_MAX_TRX_ID 修改当前页的最大事务ID，该值仅在Secondary Index定义。 PAGE_LEVEL 当前页在索引树中的位置，0x00代表叶节点。 PAGE_INDEX_ID 当前页属于哪个索引ID。 PAGE_BTR_SEG_LEAF B+树的叶节点中，文件段的首指针位置。注意该值仅在B+树的Root页中定义。 PAGE_BTR_SEG_TOP B+树的非叶节点中，文件段的首指针位置。注意该值仅在B+树的Root页中定义。 2.3 Infimun+Supremum Records在InnoDB存储引擎中，每个数据页中有两个虚拟的行记录，用来限定记录的边界。 Infimum记录是比该页中任何主键值都要小的值。 Supremum指比任何可能大的值还要大的值。 这两个值在页创建时被建立，并且在任何情况下不会被删除，如下图 ： 2.4 User Records（用户记录，即行记录）User Records即实际存储行记录的内容，InnoDB存储引擎表总是B+树索引组织的。 “ 每当我们插入一条记录，都会从Free Space部分申请一个记录大小的空间划分到User Records部分，当Free Space部分的空间全部被User Records部分替代掉之后，当前页就被用完了，此时如果还有新的记录插入就需要申请新的页了。 2.5 Free Space（空闲空间）Free Space指的就是空闲空间，同样也是个链表数据结构。当一条记录被删除后，该空间会被加入空闲链表中。 2.6 Page Directory（页目录）Page Directory（页目录）中存放了记录的相对位置 所有正常的记录（包括最大和最小记录，不包括为已删除的记录）会被划分为几个组。 每个组的最后一条记录的头信息中的n_owned属性表示该组内共有几条记录。 将每个组的最后一条记录的地址偏移量按顺序存储起来，每个地址偏移量也被称为一个槽。这些地址偏移量都会被存储到靠近页的尾部的地方。 InnoDB并不是每个记录拥有一个槽，InnoDB存储引擎的槽是一个稀疏目录（sparse directory），即一个槽中可能属于（belong to）多个记录，最少属于4条记录，最多属于8条记录。 “ Slots中记录按照键顺序存放，这样可以利用二叉查找迅速找到记录的指针。假设有（’i’，’d’，’c’，’b’，’e’，’g’，’l’，’h’，’f’，’j’，’k’，’a’），同时假设一个槽中包含4条记录，则Slots中的记录可能是（’a’，’e’，’i’），然后通过recorder header中的next_record来继续查找相关记录。 【B+树索引本身并不能找到具体的一条记录，B+树索引能找到只是该记录所在的页】。数据库把页载入内存，然后通过Page Directory再进行二叉查找。由于二叉查找的时间复杂度很低，同时内存中的查找很快，因此通常我们忽略了这部分查找所用的时间。 所以在一个数据页中查找指定主键值的记录的过程分为两步： 通过二分法确定该记录所在的槽。 通过记录的next_record属性组成的链表遍历查找该槽中的各个记录。 2.7 File Trailer（文件结尾信息）File Trailer只有一个FIL_PAGE_END_LSN部分，占用8个字节。 主要作用 保证页能够完整地写入磁盘，校验数据完整性。 “ 写入过程中磁盘损坏、机器宕机时 具体方法 前4个字节代表该页的checksum值，最后4个字节和File Header中的FIL_PAGE_LSN相同。 通过这两个值来和File Header中的FIL_PAGE_SPACE_OR_CHKSUM和FIL_PAGE_LSN值进行比较，看是否一致，以此来保证页的完整性（not corrupted）","link":"/2021/08/06/MySQL%E5%85%AD%EF%BC%9AInnoDB%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6/"},{"title":"MySQL(十一)：索引基本原理","text":"在上一篇《索引基础知识回顾》中提到索引按照存储结构划分有B-Tree索引、Hash索引、B+Tree索引类型，接下来就学习一下这几种索引结构以及在实际存储引擎中的使用情况 一、Hash索引 「Hash底层是由Hash表来实现的，存储引擎都会【对所有的索引列计算一个哈希码】（hash code），哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针，根据键值 &lt;key,value&gt; 存储数据的结构存储&lt;哈希码,指针&gt;，非常适合根据key查找value值，也就是等值查询（单个key查询）」。其结构如下所示： 「Hash索引优缺点」 哈希表按值查询的性能很好，时间复杂度是O(1)，在等值查询的时候hash索引要比B+ 树索引更高效， 「Hash索引缺点」 这也是为什么用树，而不用哈希表的原因 存在 hash 冲突问题 仅能支持【等值查询】，当查询条件为【范围查找（如：in）】就会全表扫描。 Hash索引在MySQL 中Hash结构主要应用在Memory原生的Hash索引 、InnoDB 自适应哈希索引。在《InnoDB的存储结构》已经介绍过自适应哈希索引，这里不再赘述。 二、B-Tree索引「平衡多路查找树（B-Tree）：是为磁盘等外存储设备设计的一种平衡查找树」。 「我们知道在InnoDB存储引擎中页是其磁盘管理的最小单位，默认是16KB，而系统一个磁盘块的存储空间没有这么大，因此InnoDB每次申请磁盘空间时都会申请若干地址连续磁盘块来达到页的大小16KB。在查询时如果数据都在一个页中，会减少磁盘I/O次数，提高查询效率。」 「B-Tree结构的数据可以让系统高效的找到数据所在的磁盘块」。它每个节点根据实际情况可以包含大量的关键字信息和分支，下图为一个3阶的B-Tree： 由上图中可以看出B-Tree有如下「特征」： 每个节点占用一个磁盘块的磁盘空间 一个节点上有两个升序排序的关键字和三个指向子树根节点的指针 指针存储的是子节点所在磁盘块的地址 两个关键字划分成的三个范围域对应三个指针指向的子树的数据的范围域 模拟查找关键字29的过程： 根据根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】 关键字为17和35，P1指针指向的子树的数据范围为小于17，P2指针指向的子树的数据范围为17~35，P3指针指向的子树的数据范围为大于35。 比较关键字29在区间（17,35），找到磁盘块1的指针P2。 根据P2指针找到磁盘块3，读入内存。【磁盘I/O操作第2次】 比较关键字29在区间（26,30），找到磁盘块3的指针P2。 根据P2指针找到磁盘块8，读入内存。【磁盘I/O操作第3次】 在磁盘块8中的关键字列表中找到关键字29。 通过以上过程，我们查找29，只需要三次IO，「而3阶的B-Tree可以容纳百万级数据，这对查询性能的提升是巨大的」。如果没有索引，每个数据项都要发生一次IO，总共需要百万次的IO，显然成本非常非常高。 三、B+Tree索引「B+Tree是在B-Tree基础上的一种优化，使其更适合实现外存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构」。 在上述中提到数据页的存储空间默认是16KB，而B-Tree结构图中可以看到每个节点中不仅包含数据的key值，还有data值。当数据较大时，一个节点（即一个页）能存储的key的数量就会很小，导致B-Tree的深度变大，增大查询时的磁盘I/O次数，进而影响查询效率。 对此，「B+Tree进行了优化，将所有数据都是按照键值大小顺序存放在同一层的叶子节点上，非叶子节点上只存储key值，以此增大每个节点存储的key值数量，降低B+Tree的高度」。下图为一个3阶的B+Tree： 通过示意图可以看出「B+Tree不仅仅将数据存放在叶子结点，而且所有叶子节点（数据节点）之间都有一个链指针，从而方便叶子节点的范围遍历」。特征如下： 所有关键字都「有序的」出现在叶子结点的链表中（稠密索引）。 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层。 每一个叶子节点都包含指向下一个叶子节点的指针，从而方便叶子节点的范围遍历。 四、MySQL索引实现「MySQL中索引属于存储引擎级别的概念，MyISAM和InnoDB都是使用B+Tree作为索引结构，但是不同存储引擎对索引的实现方式不同」。 4.1 MyISAM索引实现在《存储引擎》一文中介绍到MyISAM针对每个表有两个文件：「一个.frm表结构文件，一个MYD表数据文件，一个.MYI索引文件」。数据和索引是分开存储的。 「MyISAM使用B+Tree作为索引结构时，.MYI索引文件的叶节点的data域存放的是数据记录的地址」。 按照示意图：MyISAM在索引检索时首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 「在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复」。 4.2 InnoDB索引实现在《存储引擎》一文中介绍到InnoDB针对每个表有两个文件：「一个.frm表结构文件，一个.ibd数据文件」，数据和索引是一起存储的，定位到了数据也就找到了数据。 4.2.1聚簇索引（聚集索引）「InnoDB的聚簇索引就是按照主键顺序构建 B+Tree结构。叶子节点data域存储了完整的数据记录，行记录和主键值紧凑地存储在一起。即 InnoDB 的主键索引就是数据表本身，它按主键顺序存放了整张表的数据，占用的空间就是整个表数据量的大小。」 InnoDB的表要求必须要有聚簇索引： 如果表定义了主键，则主键索引就是聚簇索引 如果表没有定义主键，则第一个非空unique列作为聚簇索引 都没有InnoDB会从建一个隐藏的row-id作为聚簇索引 聚簇索引示意图： 4.2.2辅助索引（二级索引）「在InnoDB中，主索引和辅助索引（Secondary key）的Data域存储是不同的」，这与MyISAM是不同的。 「但在 B+Tree 的叶子节点中只存了【索引列和主键】的信息。二级索引占用的空间会比聚簇索引小很多， 通常创建辅助索引就是为了提升查询效率。一个表InnoDB只能创建一个聚簇索引，但可以创建多个辅助索引」。 如示意图所示，辅助索引索引中data域中存储的是主键，所以辅助索引一般需要两次查找才能查到数据： 「第一次通过辅助索引找到主键列的值」 「第二次通过主键列的值在聚簇索引中查找数据」","link":"/2021/08/06/MySQL%E5%8D%81%E4%B8%80%EF%BC%9A%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/"},{"title":"MySQL十七：Change Buffer","text":"在之前的文章《InnoDB的存储结构》介绍的InnoDB的存储结构的组成中，我们知道Change Buffer也是用InnoDB内存结构的组成部分。 Change Buffer主要是为了在写入是减少磁盘IO而存在的， 一、什么是什么是Change Buffer「在《Buffer Pool》中介绍了buffer pool会缓存热的数据页和索引页，减少磁盘读操作，而对于磁盘的写操作，innoDB同样也有类似的策略，即通过change buffer缓解磁盘写操作产生的磁盘IO」。 「Change Buffer是在【非唯一普通索引页】不在buffer pool中时，当对页进行了写操作时，在不影响数据一致性的前提下。InnoDB会将数据先写入Change Buffer中，等未来数据被读取时，再将 change buffer 中的操作merge到原数据页中」。 在MySQL5.5之前，只针对insert做了优化，叫插入缓冲(insert buffer)，后面进行了优化，对delete和update也有效，叫做写缓冲(change buffer)。 二、Change buffer 执行过程「我们知道当执行写操作时，数据页存在于Buffer Pool中时，会直接修改数据页。那如果数据页不存在于Buffer Pool中时，过程会有一些不一样，这种情况会将写操作缓存到Change Buffer中，等未来在特定条件下其合并到Buffer Pool中」。因此当需要执行一个写入操作时，一般分为走Change buffer和不走Change buffer两种情况。 2.1 写入的数据页在内存中 这种情况在在《Buffer Pool》的第4.3.2节 Flush链表写入过程中已经提过了，感兴趣的可以去看看，不看也没关系，这里在写一遍，凑一下字数~~ 当我们在写入数据的时候，写入的数据页在内存中，MySQL不会直接更新直接更新磁盘，而是经过以下两个步骤： 第一步：更新Buffer Pool中的数据页，一次内存操作； 第二步：将更新操作顺序写Redo log，一次磁盘顺序写操作； 这样的效率是最高的。顺序写Redo log，每秒几万次，问题不大。 「这种情况是被更新的数据已经别加载到Buffer Pool的前提下」。 「是否会产生数据一致性问题」 「因此写入的数据页在内存中这中情况不会产生数据一致性问题」 读取数据，会命中缓冲池的页（已经被修改）。 缓冲池LRU数据淘汰，则会将【脏页】刷回磁盘。 数据库奔溃，redo log可以恢复数据。 2.2 写入的数据页不在内存中当我们修改的数据所在的数据页之前没有别读取过，或者干脆就是一条插入语句，则会经过以下两个步骤： 第一步：在Change buffer中记录这个写入操作，一次内存操作。 第二步：将写入操作顺序写Redo log，一次磁盘顺序写操作； 可以看到，这种方式跟上面的方式「仅仅只是第一步写入的位置不一样而已，而且都是内存操作」。 如果没有Change buffer，那更新可能会变成 第一步：先从磁盘读取所在数据页加载到缓冲池，一次磁盘随机读操作； 第二步：更新Buffer Pool中的数据页，一次内存操作； 第三步：将更新操作顺序写Redo log，一次磁盘顺序写操作； 也就是会多一次磁盘IO，磁盘IO相比较内存操作时很慢的，并发下性能就会急剧下降。 这种方式的效率跟第一次差不多，写缓冲是降低磁盘IO，提升数据库写性能的一种机制。 「是否会产生数据一致性问题」 读取数据，会将Change Buffer中的数据合并到Buffer Pool中。 如果没有读取，Change也会被被定期刷盘到写缓冲系统表空间。 数据库奔溃，redo log可以恢复数据。 「因此写入的数据页不在内存中这中情况也不会产生数据一致性问题」。 三、Change Buffer大小配置「从下图中可以看出，Change Buffer被包含在了Buffer Pool中的，change buffer用的是buffer pool里的内存，由于Buffer Pool的内存大小是有限制的，所以change buffer大小也是有限制的，可通过参数innodb_change_buffer_max_size设置」。 1show variables like '%innodb_change_buffer_max_size%'; innodb_change_buffer_max_size表示允许change_buffer占Buffer Pool总大小的百分比，默认值为25%，最大可设置为50%。 当在系统中有大量插入，更新和删除操作时，可以增大innodb_change_buffer_max_size，以提高系统的写入性能。 当在系统中有大量查询操作时，可以减小innodb_change_buffer_max_size，以减少Buffer Pool中数据页的淘汰的概率，提高系统的读取性能。 innodb_change_buffer_max_size 设置是动态的，它允许修改设置而无需重新启动服务器。 四、配置Change Buffer的类型「前面说到Change Buffer在MySQL5.5之后可以支持新增、删除、修改的写入，对于受I/O限制的操作（大量DML、如批量插入）有很大的性能提升价值。但是对于一些特定的场景，可以通过修改innodb_change_buffering来变更Change Buffer支持的类型，分别为插入，删除，清除启用或禁用缓冲，更新操作是插入和删除的组合」。 1show variables like '%innodb_change_buffering%'; all ：默认值，缓冲区插入，删除和清除。 none：不缓存任何操作 inserts：缓冲区插入操作。 deletes：缓冲区删除标记操作。 changes：缓冲区插入和删除标记操作。 purges：缓冲区在后台发生的物理删除操作。 五、Change buffer被merge的时机既然Change buffer是单独内存中，写入之后会被合并到Buffer Pool中,那么是时候时候会被merge呢？ 「Change buffer会被merge触发时机」 读取Change buffer中记录的数据页时，会将Change buffer合并到buffer Pool 中，然后被刷新到磁盘。 当系统空闲或者slow shutdown时，后台master线程发起merge。 change buffer的内存空间用完了，后台master线程会发起merge。 redo log写满了，但是一般不会发生。 六、Change buffer为什么只对非唯一普通索引页有效「不知道大家有没有印象，在本文第一节就重点说了一个词【非唯一普通索引页】，Change buffer只有在非唯一普通索引页时才生效，这是为什么呢？」 相信大家在日常工作中会经常遇到一个问题：「主键冲突」。 「主键索引，唯一索引」 实际上对于【唯一索引】的更新，插入操作都会「先判断当前操作是否违反唯一性约束」，而这个操作就必须要将索引页读取到内存中，此时既然已经读取到内存了，那直接更新即可，没有需要在用Change buffer了。 「非唯一普通索引」 「不需要判断当前操作是否违反唯一性约束」，也就不需要将数据页读取到内存，因此可以直接使用 change buffer 更新。 「基于此，Change buffer只有对普通索引可以使用，对唯一索引的更新无法生效」。","link":"/2021/08/06/MySQL%E5%8D%81%E4%B8%83%EF%BC%9AChange%20Buffer/"},{"title":"MySQL(十三)：小一万字+14张图读懂锁机制","text":"MySQL中的锁有很多种，各种锁应用在不同的地方。「MySQL依靠锁机制可以让多个事务更新一行数据的时候串行化」。 MySQL中锁总的来说有两种概念：Lock和Latch Latch 称为闩锁（轻量级的锁），因为Latch要求锁定的时间非常短。其目的是用来保证并发线程操作临界资源的正确性，并且通常没有死锁检测的机制。在InnoDB引擎中，Latch又分为mutex（互斥量）和rwlock（读写锁）。 Lock 「Lock的对象是事务，用来锁定的是数据库中的对象，如表、页、行」。并且一般lock的对象仅在事务commit或rollback后进行释放（不同事务隔离级别释放的时间可能不同）。 一、锁的分类实际上MySQL的锁在不同的维度上划分是多种多样的，在特地的场景下，发挥不一样的作用，下面来看看锁的分类。 「锁定的粒度（加锁的范围）划分」 「全局锁」 「对整个数据库加锁」。 应用场景是「做全库的逻辑备份」。 「表锁」 「每次操作锁住整张表」。锁定粒度大，发生锁冲突的概率最高，并发度最低。 应用在MyISAM、InnoDB、BDB 等存储引擎中。 「页锁」 「每次锁定相邻的一组记录」，锁定粒度、开销和加锁时间都界于表锁和行锁之间，并发度一般。 应用在BDB 存储引擎中 「行锁」 每次操作锁住一行数据。锁定粒度最小，发生锁冲突的概率最低，并发度最高。应用在InnoDB 存储引擎中。 「操作类型划分」 「读锁（S锁）」 共享锁，针对同一份数据，多个读操作可以同时进行而不会互相影响。 「写锁（X锁）」 排他锁，当前写操作没有完成前，它会阻断其他写锁和读锁。 「意向锁」 InnoDB支持多粒度的锁，允许事务在行级上的锁和表级上的锁同时存在。为了支持在不同粒度上进行加锁操作，InnoDB存储引擎支持 一种特有锁「意向锁」。分为意向读锁（IS锁）、意向写锁（IX锁）。 「锁的机制上划分」 「乐观锁」 操作数据时不会对操作的数据进行加锁，只是对记录的版本进行比对，在数据更新提交的时候才会进行冲突检测，如果发现冲突了，则提示错误信息。 「悲观锁」 在对一条数据修改的时候，为了避免同时被其他事物修改，在修改数据之前先锁定，再修改数据的方式。共享锁和排他锁是悲观锁的不同实现。 二、全局锁2.1 什么是全局锁「全局锁，即对整个数据库实例加锁」。一般当我们需要让整个库处于只读状态的时候，可以给数据库加上全局锁。「加上全局锁之后其他线程的：数据更新语句（增删改）、数据定义语句（包括建表、修改表结构等）都会被阻塞」。 「加锁方式」 MySQL提供了一个加全局读锁的方法，命令 1Flush tables with read lock (FTWRL) 「全局锁应用场景」 「做全库逻辑备份」。即把整库每个表都select出来保存成文本。 「通过FTWRL确保不会有其他线程对数据库做更新，然后对整个库做备份。在备份过程中整个库完全处于【只读状态】」。但是整个库都只能读不能写，会有很大的弊端： 如果在主库上备份。那么在备份期间都不能执行写入操作。 如果在从库上备份。那么在备份期间，从库不能执行主库同步过来的 binlog，从而造成主从延时。 由此可见，做全库逻辑备份的时候加全局锁，对系统的影响非常的大，既然如此，为什么要加全局锁呢？ 2.2 为什么要加全局锁先来看一个案例： 假设现在数据库中现在有两张表：账户余额表，订单表，当我们下一个订单时，会扣减余额，同时在订单表中写入一个订单记录。 下面通过图解来说明对这两张表进行备份的过程，由于备份数据又先后顺序，所以分两种情况来看 「先备份账户余额表，再备份订单表」 由图可以看出：先备份账户余额表，再备份订单表导致备份数据中账户余额没扣钱，但是订单有了，商家血亏，消费者乐的不行，这肯定是不允许发生的。 「先备份订单表，再备份账户余额表」 由图可以看出：先备份订单表，再备份账户余额表导致备份数据中账户余额扣了钱，但是订单没有了，商家白嫖，消费者肯定不干，这肯定也是不允许发生的。 案例结论： 「通过上述案例说明，不加锁的话，备份系统备份的得到的数据不一致的，其实就是数据一个逻辑时间点的，这个读视图【Read View】是逻辑不一致的」。 2.3 不加全局锁行不行「通过上述的描述，我们知道在做数据备份的时候，需要加全局锁（FTWRL）来保证数据的一致性，但是由于FTWRL需要关闭所有表对象，数据库禁止写入，执行命令时容易导致数据库hang住」。 Q：既然加全局锁会影响业务，危害大，那做备份的时候有没有不用FTWRL，又能保证数据一致性的方法呢？A：有，方法是由有的，但是有局限性 「不加全局锁怎么保证数据一致性」 在之前文章《MVCC多版本并发控制》的3.3可重复读（RR）MVCC实现原理一节中有提到，「可重复读隔离级别下【同一个事务里面，多次查询，都只会产生一个共用Read View】」，「因此我们将事物的隔离级别调整为可重复读（RR）时是可以得到一致性视图的，而一致性视图通过MVCC能够确保数据的逻辑一致性」。 「MySQL官方提供了一个逻辑备份工具是【mysqldump】。当mysqldump使用参数 -single-transaction 时，备份数据之前会启动一个事务，在这个事务内生成一致性视图。基于MVCC，备份过程中数据是可以正常更新」。 –single-transaction：设置事务的隔离级别为可重复读(REPEATABLE READ) 「一致性视图局限性」 「一致性视图实现的前提是事物的隔离级别是可重复读（RR），但是不是所有的引擎都支持事物的，如果使用的MyISAM引擎，一致性视图就无法使用，只能FTWRL命令」。 MyISAM引擎不支持事务，总是能够拿到最新的数据 2.4数据库只读能否替换FTWRL「FTWRL的本质就是给数据库加一个锁，禁止其他线程写入，也就是将数据库设置成了只读状态」。数据库设置为只读状态其实还有一种更简单的方式，直接使用以下命令即可： 1set global readonly=true 但是一般不推荐使用readonly，两者比较，还是推荐使用FTWRL 「readonly的值有时候会被用来做其他逻辑判断，修改global变量影响更大」 判断一个库是主库还是备库 「当备份过程中出现异常时：」 FTWRL命令会释放全局锁，数据库可以被其他线程正常读写 设置为readonly时，由于时全局变量，数据库就会一直保持readonly状态，直到改变readonly的值 三、表锁MySQL里面表级别的锁有两种：一种是表锁，一种是元数据锁（metadata lock，MDL)。 3.1表锁「表级别的锁定是MySQL各【存储引擎中】最大颗粒度的锁定机制」。由于直接锁定一张表，所以获取锁和释放锁的速度很快，避免了死锁问题，但是出现锁定资源争用的概率也最高，并发量降低。 「表锁的加锁语法」 12345#隐式上锁（默认，自动加锁自动释放insert、update、delete //上写锁#显式上锁（手动）lock table tableName read;//读锁lock table tableName write;//写锁 「表锁的释放锁语法」 1UNLOCK TABLES 客户端断开的时候也会自动释放锁。 「查看表上加过的锁」 1show open tables; 「MyISAM引擎默认的锁是表锁」。表锁一般是在数据库引擎不支持行锁的时候才会被用到的。 「表级读锁」 当前表加read锁，当前连接和其他的连接都可以读操作；但是当前连接写操作会报错，其他连接写操作会被阻塞。 「表级写锁」 当前表加write锁，当前连接可以对表做读写操作，其他连接对该表所有操作（读写操作）都被阻塞。 「表级读锁会阻塞写操作，但是不会阻塞读操作。而写锁则会把读和写操作都阻塞」。 3.2元数据锁（metadata lock，MDL) 当我们查询查询一个表中的数据时，另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构就不一致了，这肯定是允许。这里就用到了元数据锁 在MySQL 5.5版本中引入了MDL，「元数据锁(MDL) 不需要显式使用，在访问一个表的时候会被自动加上」。 「当对一个表做增删改查的时候会加上【MDL读锁】」 读锁之间不互斥，因此可以有多个线程同时对一张表增删改查操作。 「当对一个表做结构变更的时候会加上【MDL写锁】」 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。 四、页锁 页级锁定是「MySQL中比较独特」的一种锁定级别，在其他数据库管理软件中并不常见。 页级锁定和行级锁定一样，会发生死锁 使用页级锁定的主要是BerkeleyDB存储引擎。 开销和加锁时间界于表锁和行锁之间； 锁定粒度界于表锁和行锁之间，并发度一般。 页锁应用于 BDB 引擎，一般很少见，了解一下即可，重点理解下行锁 五、行锁5.1行锁是什么「行锁顾名思义就是对数据行进行加锁。行锁的锁定颗粒度在 MySQL中是最细的，应用于 InnoDB 存储引擎，通过对索引数据页上的记录加锁实现的【即行锁是针对索引加锁】」。 「行锁的优缺点」 并发情况下，产生锁等待的概率较低，支持较大的并发数，但开销大，加锁慢，而且会出现死锁。 「行锁的前提条件」 「检索数据时需要通过索引，【因为 InnoDB 是通过给索引的索引项加锁来实现行锁的】」。 在不通过索引条件查询的时候，InnoDB 会使用表锁，表锁会产生锁冲突 「行锁是针对索引加锁」，所以即使访问的不同记录，只要使用的是同一索引项，也可能会出现锁冲突。 MySQL会比较不同执行计划，当全表扫描比索引效率更高时，InnoDB就使用表锁。因此不一定使用了索引就一定会使用行锁，也有可能使用表锁。 「行锁会产生死锁」 在我之前文章《索引分析》中的回表查询有提到，当我们走辅助索引的时候，会扫两遍索引树，如下： 「实际上InnoDB 的行锁也是分为两步获得的：锁住主键索引，锁住非主键索引」。 当两个事务同时执行时， 一个锁住了主键索引，在等待其他索引； 另一个锁住了非主键索引，在等待主键索引， 这样就可能会发生死锁。 「InnoDB可以检测到这种死锁，检测到后会让其中一个事务释放锁回退，另一个获取锁完成事务」。 5.2行锁的实现算法前面讲到「InnoDB行锁是通过对 索引数据页上的记录加锁实现的」，接下来看看它具体是怎么实现， InnoDB存储引擎有3种实现行锁的算法： 「【Record Lock】：记录锁，单个行记录上的锁」 RC、RR隔离级别都支持，如果表中没有主键和任何一个索引，那InnoDB会使用隐式的主键来进行锁定。 「【Gap Lock】：间隙锁，锁定一个范围，但不包含记录本身」 范围锁，锁定索引记录范围，确保索引记录的间隙不变，RR隔离级别支持 「【Next-Key Lock】：Gap Lock与Record Lock的组合」 锁定数据前后范围，并且锁定记录本身，RR隔离级别支持 「在RR隔离级别，InnoDB对于行的查询都是采用【Next-Key Lock】的组合锁定算法」，但是「在查询的列是唯一索引（包含主键索引）的情况下，Next-key Lock会降级为Record Lock，仅锁住索引本身而非范围」。 下面具体看下针对不同的sql语句采用的是那种加锁方式： 查询语句类型一 1select ... from ... 「对于普通的select语句，InnoDB引擎采用MVCC机制实现非阻塞读，【InnoDB引擎不加锁】」。 查询语句类型二 1select ... from ... lock in share mode 「添加共享锁，InnoDB会使用Next-Key Lock锁进行处理，扫描如果有唯一索引，则降级为RecordLock锁」。 查询语句类型三 1select ... from ... for update 「添加排他锁，InnoDB会使用Next-Key Lock锁进行处理，扫描如果有唯一索引，则降级为RecordLock锁」。 修改语句 1update ... from ... where ... 「InnoDB会使用Next-Key Lock锁进行处理，扫描如果有唯一索引，则降级为RecordLock锁」。 删除语句 1delete ... from ... where 「InnoDB会使用Next-Key Lock锁进行处理，扫描如果有唯一索引，则降级为RecordLock锁」。 插入语句 1insert ... from ... 「InnoDB会在将要插入的那一行设置一个排他的RecordLock锁」。 六、读锁/写锁/意向锁在前文中提到的锁类型按照「操作类型划分」有「读锁（S锁），写锁（X锁）」，其实它们与共享锁，排他锁是一个意思，只是不同叫法而已。 6.1共享锁（行级锁-读锁/S锁）「共享锁（Shared Lock）又称为读锁，简称S锁，是一种行级锁」。 顾名思义：「共享锁就是多个事务对于同一数据共享一把锁，都能访问到数据，但是只能读不能修改」。 「加锁方式」 1select ... from ... lock in share mode 「释放方式」： 12commit;rollback; 「共享锁工作原理」 「一个事务获取了一条记录的共享锁后，其他事务也能获得该记录对应的共享锁，但不能获得排他锁」。即一个事务使用了共享锁（读锁），其他事务只能读取，不能写入，写操作被阻塞。 6.2排他锁（行级锁-写锁/X锁）「排他锁（EXclusive Lock）又称为写锁，简称X锁，是一种行锁也可以是表锁」。 顾名思义：「排他锁就是不能与其他锁并存，即当前写操作没有完成前，会阻断其他写锁和读锁」。 「加锁方式」 innodb引擎默认会在update，delete语句加上 for update 1234SELECT * FROM student FOR UPDATE; # 排他锁INSERT INTO student values ... # 排他锁DELETE FROM student WHERE ... # 排他锁UPDATE student SET ... # 排他锁 「释放方式」： 12commit;rollback; 「共享锁工作原理」 「如一个事务获取了一条记录的排他锁，其他事务就不能对该行记录做其他操作，也不能获取该行的锁（共享锁、排他锁），但是获取到排他锁的事务可以对数据进行读写操作」。 这里要注意一下，其他事务不加锁的读是不会被阻塞的，阻塞的是加锁的读 「排他锁为什么是一种行锁也是表锁」 innodb引擎默认会在update，delete语句加上 for update 「读锁，写锁都属于行级锁，行级锁的实现是依靠其对应的索引，如果没用到索引的查询，就会走表锁」。 有索引：以索引列为条件更新数据，会存在间隙锁，行锁，页锁，而锁住一部分行。 没有索引：更新数据时会锁住整张表。 「可重复读隔离级别下」 「串行化隔离级别下」 读写数据都会锁住整张表 6.3意向锁（表锁）6.1 意向锁是什么「意向锁（Intention Lock）简称I锁，是一种表级锁」。 「InnoDB 实现了标准的行级锁，包括：共享锁（S锁）、排它锁（X锁）」，那么为什么需要引入意向锁呢？意向锁解决了什么问题？ 假设，事务A获取了某一行记录的排它锁，事物A尚未提交,事务B想要获取表锁时，则事物B必须要确认表的每一行都不存在排他锁，需要进行全表扫描，效率很低，此时就引入意向锁 如果事务A获取了某一行记录的排它锁，实际此时表存在两种锁，行记录的排他锁和表上的意向排他锁。 如果事务B试图在该表加表级锁时，则会被意向锁阻塞，因此事物B不必检查各个页锁或行锁，而只需检查表上的意向即可。 如上，数据库中存储数据，范围由大到小：表–&gt;页–&gt;行，加锁也是分别加在表–&gt;页–&gt;行中，当我们把锁加在更大一级范围时，也就不需要全表扫描下一级的某些锁，可以很大程度提升性能。 「锁定允许事务在行级上的锁和表级上的锁同时存在。为了支持在不同粒度上进行加锁操作，InnoDB存储引擎支持一种额外的锁方式，即意向锁」 通过上述描述我们知道「意向锁是加在表上，用于防止全表扫描的一种锁，即意向锁是表锁」。意向锁分为两种类型： 「意向共享锁（intention shared lock）」简称IS锁，事务想要给某一个数据行加行级共享锁（S锁）之前必须先获取该表的IS锁（表级锁） 「意向排他锁（intention exclusive lock）」简称IX锁，事务想要给某一个数据行加行级排他锁（X锁）之前必须先获取该表的IX锁（表级锁） 「【意向锁都是InnoDB存储引擎自己维护的,用户是无法操作意向锁的】」。 【「在为数据行加共享锁/排他锁之前，InooDB会先获取该数据行所在在数据表的对应意向锁(表级锁)」】，如果没有获取到，否则等待innodb_lock_wait_timeout超时后根据innodb_rollback_on_timeout决定是否回滚事务。 从锁粒度角度：InnoDB 允许行级锁与表级锁共存,而意向锁是表锁； 从锁模式角度：意向锁是一种独立类型，辅助解决记录锁效率不及的问题； 从兼容性角度：意向锁包含了共享/排他两种。 6.2 意向锁的兼容互斥性 意向锁之间的兼容互斥性：意向锁之间是互相兼容的 意向共享锁（IS） 意向排他锁（IX） 意向共享锁（IS） 兼容 兼容 意向排他锁（IX） 兼容 兼容 意向锁与其他锁兼容互斥性：意向锁与普通的排他锁/共享锁互斥 意向共享锁（IS） 意向排他锁（IX） 表级共享锁（S） 兼容 互斥 表级排他锁（X） 互斥 互斥 「上述的排他锁（X锁）共享锁（S锁）指的都是表锁，意向锁不会与行级的共享锁/排他锁互斥」 七、乐观锁/悲观锁「乐观锁/悲观锁其实都是概念上的，只是在并发下防止数据被修改的一种加锁形式」。 7.1 悲观锁（Pessimistic Locking）「对数据的修改抱有悲观态度的一种并发控制方式，悲观的认为自己(当前线程)拿到的数据是被修改过的，所以在操作数据之前先加锁」。 「悲观锁的形式（类型）」 「数据库的行锁、表锁、读锁、写锁、共享锁、排他锁等，以及syncronized 实现的锁都是悲观锁的范畴」。 「优点」 「可以保证数据的独占性和正确性」。 「缺点」 「每次请求都需要加锁、释放锁，这个过程会降低系统性能」。 7.2 乐观锁「乐观锁是对于数据冲突保持一种乐观态度，每次读取数据的时都认为其他线程不会修改数据，所以不上锁，只是在数据修改后提交时才通过【版本号机制或者CAS算法】来验证数据是否被其他线程更新」。 因为乐观锁中并没有【加锁和解锁】操作，因此乐观锁策略也被称为「无锁编程」。 「乐观锁实现的关键点」：检测冲突 「乐观锁实现方式」 版本号机制（常用） CAS算法实现 「优点」 「没有加锁和解锁操作，可以提高吞吐量」 「缺点」 乐观锁需要自己实现，且外部系统不受控制 「乐观锁的应用」 在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS。 「适用场景」：读多写少 「注意」 乐观锁不是数据库提供的功能，需要开发者自己去实现。 除了开发者自己手动实现乐观锁之外，很多数据库访问框架也封装了乐观锁的实现 比如 hibernate框架，MyBatis框架的OptimisticLocker插件。 7.2.1版本号机制实现乐观锁版本号机制有两种方式：使用版本字段（version）和使用时间戳（Timestamp），两者实现原理是一样的。 前文中提到「乐观锁需要开发者自己去实现，所以版本号实现时通过在表中加字段的形式实现的」。 「使用版本字段（version）」 「在数据表增加一个版本(version) 字段，每操作一次，将那条记录的版本号加 1」。version 是用来查看被读的记录有无变化，防止记录在业务处理期间被其他事务修改。 「使用时间戳（Timestamp）」 与使用version版本字段基本一致，「同样需要给在数据表增加一个字段，字段类型使用timestamp时间戳，通过时间戳比较数据版本」。 「乐观锁实现案例」 修改用户表中Id为1的用户姓名 第一步：查询记录信息 1234#使用版本字段（version)select name,version from user where id=1;#使用时间戳（Timestamp）select name,timestamp from user where id=1; 第二步：逻辑处理之后，修改姓名为张三 12345#使用版本字段（version)update user set name = '张三',version=version+1 where id=1 and version = #{version};#version 为第一步查询的值#使用时间戳（timestamp）update user set name = '张三',timestamp=now() where id=1 and timestamp = #{timestamp}; 7.2.2CAS算法实现乐观锁「CAS算法即compare and swap（比较与交换），是一种有名的无锁算法。即不使用锁的情况下实现多线程之间的变量同步，也就是无锁编程」。 「特点」 不加锁，即使没有线程被阻塞的情况下实现变量的同步，也叫非阻塞同步 CAS算法涉及到三个操作数 「当且仅当V的值等于A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作」（比较和替换是一个原子操作），一般情况下是一个自旋操作，即不断的重试。 变量当前内存值 V 旧的预期值 A 要写入的新值 B 「CAS缺点」 「ABA问题」 当线程1读到某变量的值为A，在其逻辑处理的过程中，另外一个线程2将该变量的值从A先修改为B、然后又将其从B修改回A。此时，当线程1通过CAS操作进行新值写入虽然可以成功，而实际上线程1执行CAS操作时预期值的A 和读取该变量当前值的A已经不是同一个了，后者是线程2修改的 「CPU开销大」 虽然CAS算法是非阻塞的，但如果CAS操作一直不成功不断循环，会浪费CPU资源 「只能保证一个共享变量的原子性」 当对多个变量进行操作时，CAS算法无法保证原子性。 可以将多个变量封装为一个对象再使用CAS算法（Java中的AtomicReference） 八、死锁和死锁检测8.1 死锁是什么「死锁是指两个或两个以上的事务在执行过程等中，因争夺资源而造成的一种相互等待的现象」。 「死锁产生本质原因」 系统资源有限 进程推进顺序不合理 「死锁产生的4个必要条件」 「互斥条件（Mutual exclusion，简称Mutex）」资源要么被一个线程占用,要么是可用状态 「不可抢夺（No preemption）」资源被占用后,除非占有线程主动释放,其他线程不能把它从该线程占用中抢夺 「占有和等待（Hold and wait）」一个进程必须占有至少一个资源，并等待另一资源，而该资源被其他进程占用 「循环等待（Circular wait）」一组等待进程{P0, P1…Pn-1, Pn}，P0等待资源被P1占有，P1等待资源被P2占有，Pn-1等待资源被Pn占有，Pn等待资源被P0占有，循环等待，则形成环形结构。 「死锁发生的以上四个条件缺一都无法导致死锁，而由于互斥条件是非共享资源所必须的，不仅不能改变，还应加以保证，所以恢复死锁主要是破坏产生死锁的其他三个条件」。 8.2 常见死锁现象和解决方案8.2.1表级锁死锁 「案例」 有线程A、B分别需要访问用户表与订单表，访问表的时候都会加表级锁。线程A访用户表，并对用户表加锁（线程A锁住了用户表），然后又访问订单表；此时线程B先访问订单表，并对订单表加锁（线程B锁住了订单表），然后线程想访问用户表。 「产生原因」 上述案例由于线程B已经锁住订单表，线程A必须等待线程B释放订单表能继续，同样线程B要等线程A释放用户表才能继续，「线程A、B相互等待对方释放锁，就产生了死锁」。 「解决方案」 「这种死锁是由于程序的BUG产生的，比较常见，只能通过调整程序的逻辑来解决」。 对于数据库的多表操作时，尽量按照相同的顺序进行处理，避免同时锁定两个资源， 如操作A和B两张表时，总是按先A后B的顺序处理， 必须同时锁定两个资源时，要保证在任何时刻都应该按照相同的顺序来锁定资源。 8.2.2行级锁死锁行级锁产生死锁有两种情况，一直是资源争夺，一种是行级锁升级为表级锁 资源争夺 「产生原因」 当事务中某个查询没有走索引时，就会走全表扫描，把行级锁上升为全表记录锁定（等价于表级锁），并发下多个线程同时执行，就可能会产生死锁和阻塞 「解决方案」 SQL语句中尽量不要有太复杂的多表关联查询，并通过执行对SQL语句进行分析，建立索引优化，避免全表扫描和全表锁定。 行级锁升级为表级锁 「产生原因」 两个事务分别想拿到对方持有的锁，互相等待，于是产生死锁。 「解决方案」 在同一个事务中，尽量一次锁定需要的所有资源 将每个资源编号，通过资源编号的线性顺序来预防死锁，当一个进程占有编号为i的资源时，那么它下一次只能申请编号大于i的资源。 8.2.3共享锁转换为排他锁 「案例」 事务A有两个操作，首先查询一条纪录M，然后更新纪录M；此时事务B在事物A查询之后更新之前去更新纪录M，此时事物A获取了记录M的共享锁，事物B获取了记录M的排他锁， 事务B的排他锁由于事务A有共享锁，必须等A释放共享锁后才可以获取，事物B只能排队等待。 「产生原因」 案例中事物B已经进入等待，事物A更新M需要排他锁，而此时事务B已经有一个排他锁请求，并且正在等待事务A释放其共享锁，因此无法给事物A授予排他锁锁请求，事物A也进入排队等待 注意：这里事物B还没有拿到M的排它锁，只是进入排队等到状态 「解决方案」 通过「手动实现乐观锁」进行控制，乐观锁的无锁机制可以避免长事务中的数据库加锁开销，增大并发量，提升系统性能。 8.3死锁排查MySQL提供了几个与锁有关的参数和命令，可以辅助我们优化锁操作，减少死锁发生。 「查看近期死锁日志信息」 1show engine innodb status; 通过以上命令查看近期死锁日志信息，然后使用执行计划进行SQL优化 「查看锁状态变量」 通过以下命令可以检查锁状态变量，从而分析系统中的行锁的争夺情况 1show status like'innodb_row_lock%'; Innodb_row_lock_current_waits：当前正在等待锁的数量 Innodb_row_lock_time：从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg：每次等待锁的平均时间 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次锁的时间 Innodb_row_lock_waits：系统启动后到现在总共等待的次数 「如果等待次数高，而且每次等待时间长，则需要对其进行分析优化」。","link":"/2021/08/06/MySQL%E5%8D%81%E4%B8%89%EF%BC%9A%E5%B0%8F%E4%B8%80%E4%B8%87%E5%AD%97+14%E5%BC%A0%E5%9B%BE%E8%AF%BB%E6%87%82%E9%94%81%E6%9C%BA%E5%88%B6/"},{"title":"MySQL十九：分库分表实践","text":"在很多小型应用中都没真正使用分库分表，但是说起来并不陌生，因为我们在面试中经常会被问到，今天我们从从以下几个方面来聊聊分库分表：「是什么？解决什么？怎么做？为什么要这么做？即：」 分库分表是什么？ 分库分表解决什么问题？ 分库分表怎么做？ 分库分表什么时候做？ 分库分表引发的问题是什么？ 分库分表中间件有哪些？ 一、什么是分库分表 分库分表其实很好理解，「顾名思义，即把存于一个库的数据分散到多个库中，把存于一个表的数据分散到多个表中」。但是需要明确一点，分库分表不是一件事，而是三件事，也就是「分库分表的三种方案」： 「只分库不分表」 「只分表不分库」 「既分库又分表」 1.1 只分库不分表「从单个数据库拆分成多个数据库的过程，将数据散落在多个数据库中，多个数据库同时提供服务」。 1.2 只分表不分库「从单张表拆分成多张表的过程，将数据散落在多张表内」。 1.3 既分库又分表「把存于一个数据库的单表数据分散到不同库的多个表中」。 二、分库分表解决什么问题大型应用系统需要处理大量用户的请求，比如微信，美团，淘宝等每天都会产生海量的数据，我们知道当数据量或者请求数达到一定数量之后，数据库就会产生性能瓶颈，而为了缓解数据量的压力，比较普遍的方案一方面是使用NoSQL，而另一方面就是分库分表。 2.1 分库解决什么问题分库就是在我们系统的业务量增长到一定程度之后，「解决数据库本身的性能瓶颈问题」，所以「一般都是业务量增长，遇到以下两种情况时就会考虑分库」： 「数据库QPS过高，连接数不足」 数据库的连接是有上限的，在高并发下，大量请求访问数据库，可能会让数据库宕机，致使整个服务不可用，因此我们可以把「单个数据库拆分成多个数据库，分摊请求，缓解单个数据的读写压力，提高并发量」。 「磁盘瓶颈」 数据量是随着业务量的增多而增多的，而单个数据库的磁盘存储量也是有限的，把「单个数据库拆分成多个数据库，缓解磁盘压力，降低磁盘使用率」。 2.2 分表解决什么问题在之前的《单表最大2000W行数据》中，分析了单表的数据存储量，当单表存储的数据过多时，查询数据会非常慢，在高并发场景下，一个慢查询可能会导致整个数据库的宕机。所以「一般当单个表的数据量太大，可以考虑通过分表来提升查询效率」。 一般推荐单表数据量在 上千万级别时就要考虑分表，因为千万级别可能会导致B+Tree高度变高。 不清楚单表数据量存储量大小的，可以参考一下单表最大2000W行数据这篇文章。 「因此分表可以解决单个表数据被分散，查询是B+Tree（MySQL）的高度比较低，减少磁盘IO，提升效率」。 三、分库分表怎么做「当我们使用分库分表时，都在物理空间的拆分，主要有两种拆分模式，都可以应用到分库或分表中」： 「垂直拆分」 垂直拆分又称为纵向拆分，应用时有「垂直分库和垂直分表」两种方式，「主要解决表过多或者是表字段过多问题」，一般谈到的垂直拆分主要指的是垂直分库。 「垂直分库：是将不同的表分离到不同的库中」。 「垂直分表：修改表结构按照访问的差异将不同的列拆分到不同的表中。」 「水平拆分」 水平拆分又称为横向拆分，应用时有「水平分库和水平分表」两种方式，「解决表中记录过多，缓解单机单库的性能瓶颈和压力问题」。一般谈到的水平拆分主要指的是水平分库。 水平拆分不再像垂直拆分那样将数据根据业务逻辑分类，而是「通过一定的策略将数据分散至多个库或表中，每个库或表仅包含数据的一部分行」。 「水平分库：将数据切分到不同的数据库上，每个数据库都具有相同的表，只是数据行不一样」。 「水平分表：将一张表水平切分，不同的记录可以分开保存，拆分成几张结构相同的表。」 3.1 垂直分库随着业务快速发展，数据库中的数据量猛增，所有的数据限制在一台服务器，「数据库物理机本身的CPU、内存、网络IO、磁盘等都会成为性能瓶颈」，此时我们可以按照业务的划分，将不同的表放在不同的服务器中，分散流量，减轻单个数据库的压力，提高系统的性能。 「垂直分库本质是专库专用，指按照业务将表进行分类，分布到不同的数据库中，每个库可以放在不同的服务器上」。 「垂直分库优点」 专库专用，业务层面解耦 能够针对不同业务的数据进行分级管理、维护、监控、扩展 在一定程度上提升了IO、数据库连接数、降低单机硬件资源的瓶颈 「垂直分库缺点」 事务一致性的问题 多表连接查询困难 3.1 垂直分表在之前的文章《InnoDB的存储结构》中解释了了数据在MySQL的的存储方式，我们知道数据是以数据页的方式存储的，而数据页中的数据是数据行，因此「当我们的一行数据过大时，数据页存储的数据行就会减少，也就是说跨数据页查询的概率就会增加」，因此垂直分表就是将一个表拆分到多个表，避免出现数据库跨页存储的问题，从而提升查效率。 「垂直分表本质是将一个表按照字段分成多表，每个表存储其中一部分字段」。 「垂直分表拆分原则」 将热点字段和不常用的字段区分，放在不同的表中 将text，blob等大字段拆分出来放在附表中 将组合查询的列放在一张表中 「垂直分表优点」 减少锁竞争，查询不同字段数据互不影响 可实现冷热分离的数据表设计 可以使得行数据变小，一个数据页能存放更多的数据，最大限度利用数据页缓存，减少查询的 I/O 次 数 「垂直分表缺点」 事务一致性的问题 多表连接查询困难 无法解决单表数据量过大 3.1 水平分库垂直分库是将不同业务表分别放在了不同数据库中以此减轻单个数据库的性能瓶颈，但是「如果某个核心业务的并发非常高，比如订单库，双十一下单的并发非常高，单个的订单库仍然存在单个订单数据库性能瓶颈问题，因此我们可以对数据进行分片，将单个订单库进行拆分成多个库，以此提高数据库总体性能」。 「水平分库的本质也是分表，是把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上」。 「水平分库优点」 「解决单个库高并发的性能瓶颈」 切分的表的结构相同，应用层改造较少，只需要增加路由规则即可 提高了系统的稳定性和负载能力。 「水平分库缺点」 分片事务的一致性难以解决 数据扩容的难度和维护量极大 3.1 水平分表当我们的业务量猛增，「单表数据达数千万甚至上亿的时候，查询效率会降低，此时我们考虑将数据表按照一定的规则将表中的记录进行分片，存储在不同的表中，以此提高查询效率」。 具体单表的存储数据量在《单表最大2000W行数据》文中有详细介绍，有兴趣的可以看看， 「水平分表的本质是数据分片，将不同的数据按照一定的规则（ hash取模/range范围）将数据存储在不同的表中，以此减少单表的数据量，提高查询效率」。 「水平分表优点」 「解决单表数据量大，查询性能下降的问题」 可实现多表连接查询 「水平分表缺点」 「引发排序、分页、函数计算等问题」 数据扩容的难度和维护量极大。 四、解决了什么问题/引发什么问题通过以上分别对【 垂直分表、 垂直分库，水平分表、水平分库】的分析，可以看出来无论是分库还是分表，垂直划分和水平划分的时候，他们的优缺点很类似，所以接下来总结一下分库分表解决了什么问题/引发什么问题。 4.1分库分表解决了什么问题 「水平划分」 「解决了单库的高并发的性能瓶颈，提高了系统的稳定性和负载能力」 「解决了单表的大数据量的查询效率低问题」 可以实现多表的join查询 「垂直划分」 专库专用，能够针对不同业务的数据进行分级管理、维护、监控、扩展 在一定程度上提升了IO、数据库连接数、降低单机硬件资源的瓶颈 减少锁竞争，查询不同字段数据互不影响 可实现冷热分离的数据表设计 4.2分库分表引发什么问题分库分表之后，虽然能够解决数据库的性能问题，但是也带来了一系列的其他问题： 事务问题 跨库关联问题 排序、分页、函数计算问题 分布式ID问题 多数据源问题 4.2.1 事务问题分库分表之后一个无法避免的问题就是问题问题，这也是一个非常频繁的面试问题：分布式事务，针对此类问题常用解决方案有：本地事务表，基于可靠消息（MQ）的解决方案、两阶段事务提交等。具体的解决放方案可参考我之前的文章《分布式集群：分布式事物解决⽅案》，这里不再展开。 4.2.2 跨库关联问题在单库单表中，我们经常使用JOIN来进行多表查询，但是经过分库分表后多个表可能存在于多个数据库中，无法直接使用join进行联表查询，但是联表查询是非常常见的，所以针对这种情况有以下几种解决方式 「字段冗余」 利用空间换时间，为了性能而避免join查询，将查询字段冗余。 例如：订单表保存userId时候，也将用户名称性别等需要查询的字段冗余保存一份，这样查询订单详情时就不需要再去查询用户表了。 「全局表」 「在系统模块中，对于一些依赖的数据表，在每个数据库都保存一份，避免跨库join查询。」 「ER 表（绑定表）」 「先确定各个业务表的关联关系，将那些存在关联关系的表记录存放在同一个分片上，避免跨分片join问题」 「系统层组装」 「在系统层面，分多次查询，将获得到的数据通过代码进行字段拼装」。 4.2.3 排序、分页、函数计算问题分库分表之后，数据分散，在跨节点进行count,order by,group by,limit 以及聚合函数的时候需要特殊处理，可以「采用分片的方式：先在每个分片上执行相应的函数，得到结果后在应用程序端进行合并，得到最终结果」。 4.2.4 分布式ID问题分库分表之后，我们不能再依赖数据库自增主键了，分表以后每个表都可以自增，会导致ID 重复或者混乱的问题，因此我们需要单独设计全局主键，以避免跨库主键重复问题。有一些常见的主键生成策略： UUID 基于数据库自增单独维护一张 ID表 号段模式 Redis 雪花算法（Snowflake） 美团Leaf 滴滴Tinyid 针对分布式ID的问题，有兴趣的可以看看我之前的文章《分布式集群：分布式ID解决⽅案》，文中详细介绍了几种分布式ID的实现方式，这里就不再展开赘述。 4.2.5 多数据源问题多数据源主要针对分库，既然数据库变成了多个，那什么时候查询那个库必然是一个必须要解决的问题，一般的解决方式有：「应用程序适配和代理层适配」。一般我们都会使用比较成熟的中间件来处理。 五、分库分表中间件目前市面上有很多比较成熟的分库分表中间件，可以帮助我们解决分库分表后多数据源问题 「shardingsphere（前身 sharding-jdbc）」 「cobar」 「Mycat」 「Atlas」 「TDDL（淘宝）」 「vitess」 这里简单介绍一下shardingsphere和Mycat，也是用的比较多的 5.1 Mycat「MyCat属于服务器数据库中间件，是一个基于第三方应用中间件数据库代理框架，客户端所有的jdbc请求都必须要先交给MyCat，再有MyCat转发具体的真实数据库服务器中」。 「Mycat优点」 数据添加不会影响到程序 应用层不需管理数据库层方面，由代理层去管理 添加数据源不需要重启程序 「Mycat缺点」 程序依赖的中间件，提高系统复杂性和维护工作 中间件本身需要解决高可用问题 增加了proxy，程序性能下降 5.2 ShardingSphere（sharding-jdbc）「ShardingJdbc是一个本地数据库中间件框架，以一个Jar形式在本地应用层重写jdbc原生的方法，实现数据库分片形式」。 「ShardingSphere优点」 程序自动完成，数据源方便管理 支持sql标准下的任何数据库 「ShardingSphere缺点」 以JAR的形式引入，存在代码入侵性，加大开发成本 不能做到动态添加数据源，添加数据源还需要重启程序","link":"/2021/08/06/MySQL%E5%8D%81%E4%B9%9D%EF%BC%9A%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%AE%9E%E8%B7%B5/"},{"title":"MySQL(十二)：索引分析","text":"数据库优化是一个很常见的面试题，下面就针对这一问题详细聊聊如何进行索引与sql的分析与优化。 一、执行计划（EXPLAIN）MySQL 提供了一个 EXPLAIN 命令，它「可以对 sql语句进行分析，并输出sql执行的详细信息」，可以让我们有针对性的优化。例如： 1explain select * from student where id &gt; 2; 这里需要注意一下版本差异 「MySQL 5.6.3」 MySQL 5.6.3以前只能 EXPLAIN SELECT ；MYSQL 5.6.3以后可以 EXPLAIN SELECT，UPDATE，DELETE 「MySQL 5.7」 MySQL 5.7以前想要显示 partitions 需要使用 explain partitions 命令；想要显示filtered 需要使用 explain extended 命令。在5.7版本后，默认explain直接显示partitions和filtered中的信息。 1.1执行计划详解「在使用索引的时候首先应该学会分析SQL的执行，使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，可以知道MySQL是如何处理SQL语句」。 使用格式： 12#explain sql语句 如下：explain select * from student where id &gt; 2; 从执行计划输出的结果可以看出，它有很多的字段，每个字段都有自己的含义 「id」 「选择标识符」：在一个查询语句中每个【SELECT】关键字都对应一个唯一的 id。两种例外的情况： 「id相同」优化器对子查询做了「半连接（semi-jion）优化」时，两个查询的 id 是一样的 1explain select * from student where id in(select id from student where id &gt; 1); 「id为null」 1explain select * from student union select * from student where id &gt; 1; 因为「union会对结果去重，内部创建了一个 &lt;union1,2&gt; 名字的临时表，把查询 1 和查询 2 的结果集都合并到这个临时表中，利用唯一键进行去重，这种情况下查询 id 就为 NULL」。 「select_type」 「查询的类型」，常用的值如下： 查询的类型 类型含义 SIMPLE 简单的select查询，不包含子查询或union查询，是最常见的。 PRIMARY 若查询中包含有子查询，最外层查询会别标记为PRIMARY UNION 若第二个SELECT出现在UNION之后，则被标记为UNION；若UNION包含在FROM子句的子查询中,外层SELECT将被标记为：DERIVED SUBQUERY 在SELECT或WHERE列表中包含了子查询 DERIVED 在FROM列表中包含的子查询被标记为DERIVED(衍生);MySQL会递归执行这些子查询, 把结果放在临时表里。 UNION RESULT 从UNION表获取结果的SELECT DEPENDENT SUBQUERY 在SELECT或WHERE列表中包含了子查询,子查询基于外层 UNCACHEABLE SUBQUREY 无法被缓存的子查询 「table」 输出结果集的表，即查询的表名 「partitions」 匹配的分区 「type」 表示存储引擎查询数据时采用的方式。它「可以判断出查询是全表扫描还是基于索引的部分扫描」。 常用属性值如下，从上至下效率依次增强。 ALL：表示全表扫描，性能最差。 index：表示基于索引的全表扫描，先扫描索引再扫描全表数据。 range：表示使用索引范围查询。使用&gt;、&gt;=、&lt;、&lt;=、in等等。 ref：表示使用非唯一索引进行单值查询。 eq_ref：一般情况下出现在多表join查询，表示前面表的每一个记录，都只能匹配后面表的一 行结果。 const：表示使用主键或唯一索引做等值查询，常量查询。 NULL：表示不用访问表，速度最快。 「possible_keys」 表示在某个查询语句中，对某个表执行单表查询时「可能用到的索引列表」 「key」 表示在某个查询语句中，列表示「实际用到的索引」有哪些。 「key_len」 表示查询使用索引的字节数量。可以判断是否全部使用了组合索引。 如果键是 NULL，则长度为 NULL。「使用的索引的长度」。在不损失精确性的情况下，长度越短越好 。 「ref」 当使用索引列等值匹配的条件去执行查询时，ref 列展示「与索引列作等值匹配的对象」。 「rows」 「扫描出的行数(估算的行数)」， 如果查询优化器决定使用全表扫描的方式对某个表执行查询时，rows 列就代表预计需要扫描的行数； 如果使用索引来执行查询时，rows 列就代表预计扫描的索引记录行数。 「filtered」 按表条件过滤的行百分比 如果是全表扫描，filtered 值代表满足 where 条件的行数占表总行数的百分比 如果是使用索引来执行查询，filtered 值代表从索引上取得数据后，满足其他过滤条件的数据行数的占比。 「Extra」 Extra 是 EXPLAIN 输出中另外一个很重要的列，各种操作都会在Extra提示相关信息，常见几种如下： Using where：表示查询需要通过索引回表查询数据。 Using index：表示查询需要通过索引，索引就可以满足所需数据。 Using filesort：表示查询出来的结果需要额外排序， 数据量小在内存排序，数据量大在磁盘排序，因此有Using filesort 建议优化。 Using temprorary：查询使用到了临时表，一般出现于去重、分组等操作。 二、回表查询在之前《索引基本原理》 中提到InnoDB索引有聚簇索引和辅助索引。 聚簇索引的叶子节点存储行记录，InnoDB必须要有，且只有一个。 辅助索引的叶子节点存储的是主键值和索引字段值 由上图可知：「通过辅助索引无法直接定位行记录，通常情况下，需要扫两遍索引树。先通过辅助索引定位主键值，然后再通过聚簇索引定位行记录，即回表查询」。性能比扫一遍索引树低。 三、覆盖索引索引覆盖：「只需要在一棵索引树上就能获取SQL所需的所 有列数据，无需回表，速度更快」 覆盖索引形式：，搜索的索引键中的字段恰好是查询的字段 实现索引覆盖最常见的方法就是：将被查询的字段，建立到组合索引。 四、最左前缀原则在之前《索引基本原理》 中提到组合索引的概念，在组合索引的使用中最关键的就是最左前缀原则。 「组合索引使用时遵循最左前缀原则，最左前缀顾名思义，就是最左优先，即查询中使用到最左边的列， 那么查询就会使用到索引，如果从索引的第二列开始查找，索引将失效」。 五、索引与排序5.1排序方式MySQL查询支持filesort和index两种方式的排序， filesort是先把结果查出，然后在缓存或磁盘进行排序 操作，效率较低。 index是指利用索引自动实现排序，不需另做排序操作，效率会比较高。 5.2 排序方式的选择 「使用index方式的排序的场景」 ORDER BY 子句索引列组合满足索引最左前列 1explain select id from user order by id; //对应(id)、(id,name)索引有效 WHERE子句+ORDER BY子句索引列组合满足索引最左前缀 12 #对应(age,name)组合索引explain select id from user where age=18 order by name; 「使用filesort方式的排序的场景」 对索引列同时使用了ASC和DESC 12 #对应(age,name)组合索引explain select id from user order by age asc,name desc; WHERE子句和ORDER BY子句满足最左前缀，但where子句使用了范围查询（例如&gt;、&lt;、in 等） 12 #对应(age,name)组合索引explain select id from user where age&gt;10 order by name; ORDER BY或者WHERE+ORDER BY索引列没有满足索引最左前缀 12 #对应(age,name)组合索引explain select id from user order by name; 使用了不同的索引，MySQL每次只采用一个索引，ORDER BY涉及了两个索引 12#对应(name)、(age)两个索引explain select id from user order by name,age; WHERE子句与ORDER BY子句，使用了不同的索引 12#对应(name)、(age)索引explain select id from user where name='tom' order by age; WHERE子句或者ORDER BY子句中索引列使用了表达式，包括函数表达式 12#对应(age)索引explain select id from user order by abs(age); 5.3排序算法filesort有两种排序算法：双路排序和单路排序。 双路排序：需要两次磁盘扫描读取，得到最终数据。第一次将排序字段读取出来，然后排序；第二 次去读取其他字段数据。 单路排序：从磁盘查询所需的所有列数据，然后在内存排序将结果返回。 如果查询数据超出缓存 sort_buffer，会导致多次磁盘读取操作，并创建临时表，最后产生了多次IO，反而会增加负担。 解决方案：少使用select *；增加sort_buffer_size容量和max_length_for_sort_data容量。 如果Explain分析SQL时Extra属性显示Using filesort，表示使用了filesort排序方式，需要优化。如果Extra属性显示Using index时，表示覆盖索引，所有操作在索引上完成。","link":"/2021/08/06/MySQL%E5%8D%81%E4%BA%8C%EF%BC%9A%E7%B4%A2%E5%BC%95%E5%88%86%E6%9E%90/"},{"title":"MySQL十八：写语句的执行过程","text":"当我们需要修改一个记录时，数据库会先根据条件找到要修改的数据，然后执行修改写入操作，因此我们再分析写操作的执行过程时，其实是包含读语句的执行过程的。 一、读语句的执行过程 在之前《MySQL运行机制》文中，详细说明了一个查询语句的执行的过程，查询sql的执行过程基本上分为六步: 「建立连接（Connectors&amp;Connection Pool）」 「查询缓存（Cache&amp;Buffer）」 「解析器（Parser）」 「预处理器（preprocessor）」 「查询优化器（Optimizer）」 「操作引擎执行 SQL 语句」 通过以上六步即可在数据库中查询到相应的数据，针对每个步骤的过程，这之前的文中有详细说明，这里不再赘述。 1.1 查询缓存弊大于利「查询缓存分为Cache和Buffer，两者都是缓存，但是作用不一样：」 Cache：缓存读取的数据 「cache是从磁盘读取数据然后存起来方便以后使用。实现数据的重复使用，减少读取的磁盘IO」。 将硬盘中的数据读取出来放在内存的缓存区中，这样以后再次访问同一个资源，速度会快很多。 Buffer：缓冲修改过的数据 「buffer是为了提高内存和硬盘的IO设计的，写入到磁盘的数据会先写入buffer中，然后一起刷盘」。 从内存中将数据往硬盘中写入，并不是直接写入，而是缓冲到一定大小之后刷入硬盘中。 「两者共性都属于内存，数据都是临时的，一旦关机数据都会丢失」。 查询缓存的弊端在《MySQL运行机制》文中未曾说明，这里做一下补充说明： 「大多数情况下一般不要使用查询缓存，因为查询缓存往往弊大于利」。 「当对一个表进行更新操作时，这个表上所有的查询缓存都会被清空」。所以当对更新比较频繁的表使用查询缓存的时候，命中率会非常低，得不偿失。一般我们可以对静态表使用查询缓存（比如字典表，配置表）。也是基于此，MySQL才提供了上述开启查询缓存的方式，直接在SQL语句中指定，按需使用。 「MySQL 8.0版本已经没有查询缓存这个功能了，直接将查询缓存整个模块删掉了」。 二、写语句的执行过程2.1写语句是怎么执行的前面说到，更新操作时，也会走先查询，所以它的执行流程也是大同小异的。 还是通过这张图，按照【读语句的执行过程】的六个步骤去执行，不同的是： 「分析器」 通过【「词法分析」】和【「语法解析」】确定当前的SQL语句是一条更新语句。 「优化器」 确定索引，执行计划 「执行器」 内存中修改数据，调用存储引擎的修改接口，最终修改数据 「以上就是更新语句的执行过程，看起来似乎跟查询语句没啥区别，只不过一个查询语句，一个是更新语句，两者调用的存储引擎的接口不一样而已。事实上也的确如此，它们很类似，但是更新语句会比查询语句多两个步骤」。 2.2写语句比读语句多了什么上面说到更新语句会比查询语句多两个步骤，具体是多了什么呢？我们假设一下 按照上述的方式进行更新，似乎是没有问题的，数据也确实能写到数据库中，最终通过存储引擎写入磁盘中。但是有一个问题，我们知道「磁盘是很慢的，而我们的程序操作内存是需要IO操作的，当更新比较频繁的时候，磁盘IO必然会很慢，会降低数据库的性能，高并发下，很容易就会导致数据库宕机」。 既然有这种隐患，那么MYSQL不可能没有解决的，这里就涉及到了MySQL中两个非常重要的日志模块： Undo log (撤销日志) Redo log（重做日志） Binlog（归档日志） MySQL正式利用这两个日志来解决上述频繁IO问题的。也就是经常说的「预写式日志记录（Write-Ahead Logging），即WAL技术，核心就是先写日志，再写磁盘」。 它很类似MQ【异步、削峰】的特性，在更新的时候，先写日志，不更新磁盘（即异步刷盘），在数据库不繁忙的时候（特定的时间点或者时机）再将日志记录更新磁盘（即削峰） 关于Undo log / Redo log与Binlog在之前的《一文详解六大日志》中也有详细的介绍，这里就不再具体描述，只重点说一下在更新操作的中使用过程。 如果对Undo log / Redo log与Binlog不是很熟悉的话，可以看一下之前的文章，以作参考 2.3 Undo log（撤销日志）「撤消日志是在事务开始之前保存的被修改数据的备份，由InnoDB存储引擎实现」。 主要作用： 「用于回滚事务」，直接从undo日志中取到原始值 「MVCC机制的实现」：对不同隔离级别下事物能读取到的数据 MVCC机制的实现原理在之前的《读懂MVCC多版本并发控制》 中已经详细描述，感兴趣的可以参考看一下。 2.3 Redo log（重做日志）「当执行一条更新语句的时候，InnoDB引擎会先把记录写到redo log里，并更新内存，到此更新操作就完成了，此时数据并没有写入磁盘，InnoDB会在特定的时机将记录写入磁盘中」。 我们知道「InnoDB的redo log是固定大小的，所以为了避免在刷盘之前redo log被写满，所以redo log采用的是循环写的方式」，如下： write pos ：表示 redo log 当前记录的位置，一边写一边后移 check point ：表示 「数据页更改记录」 刷盘后对应擦除的位置。 write pos 到 check point 之间的部分是 redo log 空着的部分，用于记录新的记录； check point 到 write pos 之间是 redo log 待落盘的数据页更改记录。 当 write pos追上check point 时，会先推动 check point 向前移动（先刷盘，后擦除），空出位置再记录新的日志。 2.4 Binlog（归档日志）在MySQL系列的第一篇《架构体系》中的就已经阐述了基本架构组成包含Server层与存储引擎层，上述的「Redo log是InnoDB引擎所特有的一种日志，而MySQL支持的引擎是多种的，因此Server层也有自己的日志，即binlog（归档日志）」。 顾名思义：binlog日志只能用于归档， Redo log能够保证MySQL在任何时间段突然奔溃，重启后以前提交的记录都不会丢失，也就是「crash-safe」能力。 「简单说一下Redo log与Binlog的不同」 Redo log是InnoDB引擎特有的,属于物理日志；binlog是MySQL的Server层实现的，所有引擎都可以使用，属于逻辑日志。 Redo log：记录的是结果，某个数据页某条记录做了什么修改，记录修改结果 Binlog：记录的是原始逻辑，也就是修改的过程 redo log是循环写的，空间固定会用完，用完就刷盘再清空；binlog是追加写入的，文件写到一定大小后会切换到下一个，不会覆盖之前的日志。 2.5 写入语句的执行过程通过对以下的几篇文章的介绍，可以使我们对MySQL的写入有了一个大概的认识，内部的执行原理也有了比较清晰的认知，接下来看一下一条sql在执行的整个流程中，从它经历组件，各个组件做的操作等角度来分析一下写操作的执行过程，下面来看一下具体的写操作的执行过程。 1update user set name='星河之码' where id=1; 来看看执行上述这个修改语句的整个过程，前面建立连接等几个步骤就省略了，直接看执行器执行时的过程，如下图： 「执行器通过存储引擎查找【id=1】的记录」。 存储引擎查找到记录之后将这条记录所在的数据页全部从磁盘读入内存，然后返回给执行器。 「执行器获取到返回的记录后，修改【name=’星河之码’】，调用存储引擎修改数据」。 「存储引擎接收到name=’张三’的数据之后，将其更新到内存中对应的数据页中，同时写入redo log日志中，但是redo log写入之后未提交，处于准备（prepare）阶段」。 「执行器调用存储引擎修改数据后，会产生一个binlog，并将其写入磁盘中」。 每个写操作mysql在Server层都会生成一个binlog 「binlog写入完成，执行器调用存储引擎的提交事务接口」。 「存储引擎接收到提交事务请求后，会把写入的redo log状态改成提交（commit）状态，完成更新」。 「后台线程会定时将Buffer Pool中的修改过的缓存页加载到磁盘中」 通过以上执行过程分析图，写入操作就完成了，由此可见，虽然我们就写了一句update语句，但是实际上mysql还是帮助我们做了很多工作的。 对上图做了一个简化，其中比较重要的流程就是修改Buffer Pool 与日志同步的过程，如下图： 以上就是MySQ的InnoDB在写入的执行过程，其中涉及到很多细节，这里没有展开，比如数据页的读取，修改数据页之后Buffer Pool怎么刷脏，怎么保证Buffer Pool在有限的内存中加载到更多的热点数据，怎么提高Buffer Pool的命中率等，这些问题在以往的文章中都有详细介绍，有兴趣的可以看一下以下几篇文章： Change Buffer 36张图理解Buffer Pool 一文详解六大日志 2.6 两阶段提交 上述的写操作执行过程中，写入Redo log的时候有两个阶段：「准备阶段与提交阶段」，为什么不直接一步到位，而要分了两步，再调用一下提交事务的接口呢？是否是多此一举呢？ 实际上，在写Redo Log 与Binlog的时候采用「准备与提交」两个阶段的方式实现，是为了「保证数据一致性」。如果不用这个方式，而是两个日志都采用直接提交的方式，无论谁先谁后，都可能在数据路宕机时丢失数据导致不一致。 1update user set name='星河之码' where id=1; 还是以这条更新语句来看，如下： 「先写Redo log后写Binlog」 「假设先写Redo log，并且写入成功,Binlog还没有写完的时候，数据库宕机了」，那我们在重启数据库后，可以通过Redo log进行恢复，恢复后id=1的记录name为星河之码，但是Binlog没有写完就宕机，所以「Binlog里面是没有这条更新语句」的，当我们使用binlog做日志备份，数据同步或者恢复的时候，由于「binlog丢失就会导致数据与原库不一致」。 「先写Binlog后写Redo log」 「假设先写binlog，Redo log还没有写完的时候，数据库宕机了」，那数据库崩溃恢复后，「Redo log写入失败，事物无效，id=1的记录name为原值」。但是binlog已经记录了这条更新语句，当我们使用binlog做日志备份，数据同步或者恢复的时候，就会多这条更新事物，「导致恢复/备份的id=1的记录name为星河之码，最终导致与原库数据不一致」。 基于此，可以明确无论先写那个日志都会导致数据库不一致，因此，MySQL的设计了准备与提交的两阶段提交的方式。「Redo log和Binlog用于记录事物的行为状态，两阶段提交可以让这两个状态保持逻辑上的一致，以此保证数据的一致性」。","link":"/2021/08/06/MySQL%E5%8D%81%E5%85%AB%EF%BC%9A%E5%86%99%E8%AF%AD%E5%8F%A5%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/"},{"title":"MySQL十四：单表最大2000W行数据","text":"在互联网技术圈中有一个说法：「MySQL 单表数据量大于 2000 W行，性能会明显下降」。网传这个说法最早由百度传出，真假不得而知。但是却成为了行业内一个默认的标准。 单表超过2000W行数据一定会导致性能下降吗？我认为是不一定的，虽然说建议单表不超过2000W，但是我不接受它的建议可不可以？那必然也是可以的。 一、单表最大到底能存多少数据先来看看下面这张图，了解一下mysql各个类型的大小 我们知道在MySQL是支持主键自增长的，不考虑其他因素的前提下，理论上只有主键没有用完，表中的数据就可以一直增加。从上图可以中可以分析出： 「主键类型为Int时」 主键32位，数据最大为2^32-1，大约可以存储21亿的数据，远远大约2KW。 「主键类型为bigint时」 主键64位，数据最大为2^64-1，存储的数据远远大于了常用的计量单位了，磁盘都达不到这个数量级。 「主键类型为tinyint时」 主键8位，数据最大为255，Id自增超过255就会报错 「由此可見：MySQL能够存储的数据在一定程度上受限与主键的类型。但是数据量的大小却跟2000W没啥影响，既然百度大佬推荐单表最大2000W行数据，那肯定不会是空口白话，一定定会有其他影响行数的因素」。 二、数据存储的结构先不要着急，影响数据行数的因素肯定是有的，在此之前，先来看看数据在InnoDB中是怎么存储在磁盘的，又是怎么读取的。 2.1 数据存储的结构在MySQL中默认的存储引擎是InnoDB，在之前的《存储引擎》中有说过，InnoDB为每个表都生成了两个文件： .frm文件：表结构文件 .ibd文件：数据文件（聚簇索引包含数据与索引），又叫「表空间」。 我们表中的的数据其实都是存储在磁盘的.ibd文件中，而每次读取整个.ibd文件无疑是非常慢的，所以在《InnoDB数据文件》中又提到，「InnoDB将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般为 16KB」。如下 从上图中可以很清晰的看出，「一个.ibd文件文件是由多个数据页组成的，也就是说一个表的数据会被分散存储在多个数据页中」。当然数据页也不仅仅只是存储表中的数据，先来回顾一下页的组成 「页的组成」 如图所示，InnoDB数据页由以下七个部分组成， 从也得组成中我们知道，「数据页中还存储了除数据之外的东西，比如数据页的前后指针，页号，页目录等，因此虽然一个数据页一共16KB，但是能够用来存储数据的其实是不足16KB的」 。 通过页的组成，我们可以大致分析在数据页中一下「查找数据的整体过程」： 记录被分散在不同的数据页中，InnoDB通过「页号【表空间的地址偏移量】来标识数据具体在哪一页中」。 不通的数据页之间使用「前后指针」进行关联，避免检索消耗， 当找到数据在那个数据页之后，InnoDB为避免遍历检索而提供了一个「页目录」，页目录通过「二分查找」将查找效率「从O(n) 变成O(lgn)」，从而快速定位数据的位置。 2.2 索引的结构既然在.ibd文件中只要知道了页号，就可以快速定位数据行的位置，从而读取到相应的数据。那么问题来了，我咋知道我要找的数据在那个数据页里，咋知道页号是啥？ 万事都有解决的方式，要知道页号其实也简单，无非就两种方式： 「全表扫描」：简单粗暴，没那么多花花肠子，干就完事，但是「数据量大了，性能就会下降，非长久之计」。 「通过索引找到数据页」：重点了解一下这个 在《索引基本原理》中解释了InnoDB索引是基于B+tree实现的，InnoDB在构建B+tree结构时，一般会找出每个数据页中id最小（或者说索引最小， InnoDB主键即聚簇索引）的记录与其对应的页号，「将id与页号组成一个新的记录，存储在一个新生成的数据页中，其大小也为16K，为与存储数据的数据页区分，引入了数据页之间的上下层级关系」，也就是「页层级（page level）」。因此我们知道在B+tree中分为两部分： 「叶子节点」：真正存放表中的数据的数据页，page level = 0 「非叶子节点」：存放索引以及索引对应数据所在的页号的数据页 根据这张B+tree的图，我们知道数据页之间是有地址指向的，如果要找一条数据，最多只需要经历三次「磁盘IO」就可以将数据页都加载到内存中，从而找到数据，完成查询。 三、B+Tree能存储多少数据要知道一个B+Tree能存储多少数据其实也不难，B+Tree中的叶子节点存放的是数据，而一个数据页只有16K，我们「假设：数据页中页目录，页头，页尾加起来总共占用1KB，剩余15KB全部用了存放数据」。 如上图： 将B+tree的高度定义为N 非叶子节点的数据页存储数量为X，也就是有X个数据页的页号 叶子节点的数据页存储数据为Y 根据以上定义，B+tree存储的数据总量：「M ={X ^ (N-1)} * Y」 前文中我们说到主键类型会影响行数，那么此时我们「假设主键类型为bigint类型，占8个字节，而在InnoDB源码中页号（FIL_PAGE_OFFSET）被设置为4字节」。则此时非叶子节点能存储的数据量为 「X = 15 * 1024 / (8 +4) = 1280」 前面已经将目录，页头，页尾作为1KB排除，所以这里是15 「基于此：在来做一个假设，假设叶子节点中存储的数据，每条的大小都为1KB，即每个数据页存储15条数据。」 「Y = 15」 现在来看看B+tree的数据量 「两层B+tree的数据量（N=2）」 「M = {X ^ (N-1)} * Y = {1280 ^ (2-1)} * 15 = 19200 条」 「三层B+tree的数据量（N=3）」 「M = {X ^ (N-1)} * Y = {1280 ^ (3-1)} * 15 = 24579000条」 「四层B+tree的数据量（N=4）」 「M = {X ^ (N-1)} * Y = {1280 ^ (4-1)} * 15 = 计算器都算不清楚了」 可能还没有写到这么多数据，磁盘已经罢工了 「从这里的24579000条，我们就知道为啥单表不推荐超过2000W了，三层B+tree的时候最多只有三次磁盘IO，四层的时候数据量太大，磁盘可能都造不住了」。 四、啥时候能超过2000W的数据不知道大家有没有注意到一点，在上面计算中，我们都是做了很多假设，其中就有一条：「假设叶子节点中每条数据占用1KB，以此得出一个数据页的数据量Y=15」。 在实际中，要是我一行的数据非常小，仅仅只占用了100KB（比如一个中间表，记录的仅仅是ID），此时 「叶子节点数据页的数据量」 「Y = 15 * 1024 / 100 = 153」 「三层B+tree的数据量」 「M = {X ^ (N-1)} * Y = {1280 ^ (3-1)} * 153 = 250675200条」 「同样是三层B+tree，此时却可以存储2.5亿条数据，增长十倍，但是查找同样只需要三次磁盘IO，并不会对性能有太大影响」。 这里说的「是【叶子节点】数据页的数据行大小」影响了最终存储的数据总量，「实际上【非叶子节点】的数据页存储数量X的大小变化的时候」，也会影响数据总量，但是这种影响一般会在B-tree中体现。 我们知道B-tree跟B+tree最大的区别就是「B-tree的非叶子节点中存储的是真实的数据行，而数据页的大小是16KB固定的，因此相同数据下，B-tree需要更多数据页才能存储数据，数据页增多势必会造成非叶子节点的层级变高，造成更多的磁盘IO，导致性能下降」。这也是InnoDB使用B+tree作为索引结构，而不用B-tree的原因。 「总结」 这里总结一下前文中提的问题「其他影响行数的因素」？现在就很清晰了，除了主键大小和磁盘限制，最重要的就是索引的结构，即B+tree。","link":"/2021/08/06/MySQL%E5%8D%81%E5%9B%9B%EF%BC%9A%E5%8D%95%E8%A1%A8%E6%9C%80%E5%A4%A72000W%E8%A1%8C%E6%95%B0%E6%8D%AE/"},{"title":"MySQL十六：36张图理解Buffer Pool","text":"在应用系统中，我们为加速数据访问，会把高频的数据放在「缓存」(Redis、MongoDB)里，减轻数据库的压力。 在操作系统中，为了减少磁盘IO，引入了「缓冲池」(buffer pool)机制。 MySQL作为一个存储系统，为提高性能，减少磁盘IO，同样具有「缓冲池」(buffer pool)机制。 在之前的文章《InnoDB的存储结构》中介绍了InnoDB的存储结构的组成，结构图如下： 「上述结构图中展示了Buffer Pool作为InnoDB内存结构的四大组件之一，不属于MySQL的Server层，是InnoDB存储引擎层的缓冲池」。因此这个跟MySQL8.0删掉的【查询缓存】功能是不一样的。 一、什么是Buffer Pool「Buffer Pool即【缓冲池，简称BP】，BP以Page页为单位，缓存最热的数据页(data page)与索引页(index page)，Page页默认大小16K，BP的底层采用链表数据结构管理Page」。 上图描述了Buffer Pool在innoDB中的位置，通过它所在的位置我们可以大概知道它的工作流程： 所有数据页的读写操作都需要通过buffer pool进行， innodb 读操作，先从buffer_pool中查看数据的数据页是否存在，如果不存在，则将page从磁盘读取到buffer pool中。 innodb 写操作，先把数据和日志写入 buffer pool 和 log buffer，再由后台线程以一定频率将 buffer 中的内容刷到磁盘，「这个刷盘机制叫做Checkpoint」。 写操作的事务持久性由redo log 落盘保证，buffer pool只是为了提高读写效率。 「Buffer Pool缓存表数据与索引数据，把磁盘上的数据加载到缓冲池，避免每次访问都进行磁盘IO，起到加速访问的作用」。 Buffer Pool是一块内存区域，是一种「降低磁盘访问的机制」。 数据库的读写都是在buffer pool上进行，和undo log/redo log/redo log buffer/binlog一起使用，后续会把数据刷到硬盘上。 Buffer Pool默认大小 128M，用于缓存数据页（16KB）。 1show variables like 'innodb_buffer%'; Buffer Pool 是 innodb的数据缓存， 除了缓存「索引页」和「数据页」，还包括了 undo 页，插入缓存、自适应哈希索引、锁信息等。 「buffer pool绝大多数page都是 data page（包括index page）」。 「innodb 还有日志缓存 log buffer，保存redo log」。 二、Buffer Pool的控制块Buffer Pool中缓存的是数据页，数据页大小跟磁盘默认数据页大小一样（16K），为了更好管理的缓存页，Buffer Pool有一个「描述数据的区域」 ： 「InnoDB 为每一个缓存的数据页都创建了一个单独的区域，记录的数据页的元数据信息，包括数据页所属表空间、数据页编号、缓存页在Buffer Pool中的地址，链表节点信息、一些锁信息以及 LSN 信息等，这个区域被称之为控制块」。 「控制块和缓存页是一一对应的，它们都被存放到 Buffer Pool 中，其中控制块被存放到 Buffer Pool 的前边，缓存页被存放到 Buffer Pool 后边」， 控制块大概占缓存页大小的5%，16 * 1024 * 0.05 = 819个字节左右。 上图展示了控制块与数据页的对应关系，可以看到在控制块和数据页之间有一个碎片空间。 这里可能会有疑问，为什么会有碎片空间呢？ 上面说到，数据页大小为16KB，控制块大概为800字节，当我们划分好所有的控制块与数据页后，可能会有剩余的空间不够一对控制块和缓存页的大小，这部分就是多余的碎片空间。如果把 Buffer Pool 的大小设置的刚刚好的话，也可能不会产生碎片。 三、Buffer Pool的管理「Buffer Pool里有三个链表，LRU链表，free链表，flush链表，InnoDB正是通过这三个链表的使用来控制数据页的更新与淘汰的」。 3.1 Buffer Pool的初始化「当启动 Mysql 服务器的时候，需要完成对 Buffer Pool 的初始化过程，即分配 Buffer Pool 的内存空间，把它划分为若干对控制块和缓存页」。 「申请空间」 Mysql 服务器启动，就会根据设置的Buffer Pool大小（innodb_buffer_pool_size）超出一些，去操作系统「申请一块连续内存区域」作为Buffer Pool的内存区域。 这里之所以申请的内存空间会比innodb_buffer_pool_size大一些，主要是因为里面还要存放每个缓存页的控制块。 「划分空间」 当内存区域申请完毕之后，数据库就会按照默认的缓存页的16KB的大小以及对应的800个字节左右的控制块的大小，在Buffer Pool中划分「成若干个【控制块&amp;缓冲页】对」。 划分空间后Buffer Pool的缓存页是都是空的，里面什么都没有，当要对数据执行增删改查的操作的时候，才会把数据对应的页从磁盘文件里读取出来，放入Buffer Pool中的缓存页中。 3.2 Free链表「在Buffer pool刚被初始化出来的时候，里面的数据页以及控制块都是空的」，当执行读写的时候磁盘的数据页会加载到Buffer pool的数据页中，当BufferPool中间有的页数据持久化到硬盘后，这些数据页又会被空闲出来。 以上的过程中会有一个问题，如何知道那些数据页是空的，那些是有数据的，只有找到空的数据页，才能吧数据写进去，一种方式是遍历所有的数据页，根据经验，一般只要是全部遍历，对于一个有追求的码农肯定是不能忍的，innoDB的开发者无疑更加不能忍，所以就有了free链表。 3.2.1 Free链表是个啥「Free链表即空闲链表，是一个双向链表，由一个基础节点和若干个子节点组成，记录空闲的数据页对应的控制块信息」。如下 Free链表作用：帮助找到空闲的缓存页 「基节点」 「是一块单独申请的内存空间（约占40字节）。并不在Buffer Pool的连续内存空间里」。 包含链表中子节点中头节点地址，尾节点地址，以及当前链表中节点的数量等信息。 「子节点」 「每个节点就是个空闲缓存页的控制块，即只要一个缓存页空闲，那它的控制块就会被放入free链表」 每个控制块块里都有两个指针free_pre（指向上一个节点），free_next（指向下一个节点） Free链表存在的意义就是描述Buffer Pool中的数据页，所以Free链表跟数据页的是一一对应的关系，如下图所示： 上图就是Free链表记录空闲数据页的对应关系，这里可能会有一个误区，以为这个控制块，在Buffer Pool里有一份，在free链表里也有一份，似乎在内存里有两个一模一样的控制块，「如果这么想就大错特错了」。 「误区说明」 「free链表本身其实就是由Buffer Pool里的控制块组成的，前文中说到每个控制块里都有free_pre/free_next两个指针，分别指向自己的上一个free链表的节点，以及下一个free链表的节点。」 「Buffer Pool中的控制块通过两个指针，就可以把所有的控制块串成一个free链表。上面为了画图看起来更加清晰，所以把free链表单独画了一份出来，表示他们之间的指针引用关系。」 「基于此，真正的关系图应该下图」： 这里之所以会把两个图都画画出来，是因为网上很多博客画的图都是类似上面哪一种，「会给人产生在Buffer Pool和free链表各有一个控制块的误区」，我在开始的时候也产生了这样的疑问，所以在这里说明记录一下。 3.2.2 磁盘页加载到BufferPool的缓存也流程通过free链表只需要三步就可以将磁盘页加载到BufferPool的缓存中： 「步骤一」 「从free链表中取出一个空闲的控制块以及对应缓冲页」。 「步骤二」 「把磁盘上的数据页读取到对应的缓存页，同时把相关的一些描述数据写入缓存页的控制块（例如：页所在的表空间、页号之类的信息）」。 「步骤三」 「把该控制块对应的free链表节点从链表中移除，表示该缓冲页已经被使用了」。 下面用一个伪代码来描述一下控制块是如何在free链表节点中移除的，假设控制块的结构如下 1234567891011121314151617/** * 控制块 */publicclass CommandBlock { /** * 控制块id,也就是自己，可以理解为当前控制块的地址， */ private String blockId; /** * Free链表中当前控制块的上一个节点地址 */ private String freePre; /** * Free链表中当前控制块的下一个节点地址 */ private String freeNext;} 假设有一个控制块n-1，他的上一个节点是描述数据块n-2，下一个节点是描述数据块n，则它的数据结构如下： 1234567891011121314151617/** * 控制块 n-1 */publicclass CommandBlock { /** * 控制块id,也就是自己，可以理解为当前控制块的地址 block_n-1， */ blockId = block_n-1; /** * Free链表中当前控制块的上一个节点地址 block_n-2 */ freePre = block_n-2; /** * Free链表中当前控制块的下一个节点地址 block_n */ freeNext = block_n;} 上图我们使用了控制块N，要从free链表中移除，则只需要把block_n-1中的freeNext设置为null即可， block_n就失去了链表的引用了。 1234567891011121314151617/** * 控制块 n-1 */publicclass CommandBlock { /** * 控制块id,也就是自己，可以理解为当前控制块的地址 block_n-1， */ blockId = block_n-1; /** * Free链表中当前控制块的上一个节点地址 block_n-2 */ freePre = block_n-2; /** * Free链表中当前控制块的下一个节点地址 block_n */ freeNext = null;} 3.2.3 如何确定数据页是否被缓存 了解了磁盘页是通过Free加载到Buffer Pool 的缓存页的过程，不能所有的数据都去磁盘读取然后通过Free链表写入缓存页中，有可能在缓存页中已经有了这个数据页了，那么怎么确定应不应该去缓存数据页呢？ 「数据库提供了一个数据页缓存哈希表，以表空间号+数据页号作为key，缓存页控制块的地址作为value」。 12#注意：value是控制块的地址，不是缓存页地址{表空间号+数据页号:控制块的地址} 当使用数据页时，会先在数据页缓存哈希表中查找，如果找到了，则直接根据value定位控制块，然后根据控制块找到缓存页，如果没有找到，则读取磁盘数据页写入缓存，最后写入数据页缓存哈希表。 「在这个过程中一条语句要执行，大致会经历以下几个过程」： 通过sql语句中的数据库名和表名可以知道要加载的数据页处于哪个表空间。 「根据表空间号，表名称本身通过一致性算法得到索引根节点数据页号」。 进而根据根节点数据页号，找到下一层的数据页，可以从数据页缓存哈希表得到对应缓存页地址。 通过缓存页地址就可以在Buffer Pool池中定位到缓存页。 重点误区！！！重点误区！！！重点误区！！！ 重要的事情说三遍： 上面说的一致性哈希算法「指在数据字典中【根节点的页号，不是当前查找的数据的数据页号】」，当我们得到根节点页号后，通过B+tree一层一层往下找，在找下一层之前会通过数据缓存哈希表去buffer pool里面看看这个层的数据页存不存在，不存在则去磁盘加载。 「下文所有的图解都是查找buffer pool的过程，不包含索引的树状结构的查找」 「下文所有的图解都是查找buffer pool的过程，不包含索引的树状结构的查找」 「下文所有的图解都是查找buffer pool的过程，不包含索引的树状结构的查找」 流程图如下： 3.3 LRU链表了解LRU链表之前，我们先来考虑两个问题： 第一个问题：前面说到当从磁盘中读取数据页到Buffer Pool的时候，会将对应的控制块从Free链表中移除，那这个控制块移除之后被放到哪里去了呢？ 第二个问题：Buffer Pool的大小是128MB，当Buffer Pool中空闲数据页全部别加载数据之后，新的数据要怎么处理呢？ 以上两个问题都需要LRU链表来解决，下面带着这两个问题来看看LRU链表。 3.3.1 LRU链表是个啥Buffer pool 作为一个innodb自带的一个缓存池，数据的读写都是buffer pool中进行的，操作的都是Buffer pool中的数据页，但是Buffer Pool 的大小是有限的（默认128MB），所以对于一些频繁访问的数据是希望能够一直留在 Buffer Pool 中，而一些访问比较少的数据，我们希望能将它够释放掉，空出数据页缓存其他数据。 「基于此，InnoBD采用了LRU（Least recently used）算法，将频繁访问的数据放在链表头部，而不怎么访问的数据链表末尾，空间不够的时候就从尾部开始淘汰，从而腾出空间」。 「LRU链表本质上也是有控制块组成的」。 3.3.2 LRU链表的写入过程「当数据库从磁盘加载一个数据页到Buffer Pool中的时候，会将一些变动信息也写到控制块中，并且将控制块从Free链表中脱离加入到LRU链表中」。过程如下： 梳理一下整个过程： 「步骤一：根据表空间号，表名称本身通过一致性算法得到数据页号(这里省略了树状查找过程)」 「步骤二：通过数据页缓存哈希表判断数据页是否被加载」 「步骤三：从Free链表中获取一个控制块」 「步骤四：读取磁盘数据」 「步骤六：将数据写到空闲的缓存页中」 「步骤七：将缓存页的信息写回控制块」 「步骤八：将回控制块从Free链表中移除」 「步骤九：将从Free中移除的控制块节点加入到LRU链表中」 3.3.3 LRU链表的淘汰机制「LRU算法的设计思路就是：链表头部的节点是最近使用的，链表末尾的节点是最久没被使用的，当空间不够的时候就淘汰末尾最久没被使用的节点，从而腾出空间」。 「LRU算法的目的就是为让被访问的缓存页能够尽量排到靠前的位置」。 「LRU 算法的设计思路」 当访问的页在 Buffer Pool 里，就将该页对应的控制块移动到 LRU 链表的头部节点。 当访问的页不在 Buffer Pool 里，除了要把控制块放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的节点。 「LRU 的实现过程」 有一次数据访问，访问了数据页23，数据页23不在Buffer Pool 里，因此在磁盘加载之后会将末尾的22号页淘汰，然后将23加载到链表的头部。 此时，数据页7被访问了，因为数据页7就在链表中，也就是页在 Buffer Pool 里，所以直接将数据页7移动到链表的头部即可。 如下图， LRU 链表长度为 22，节点分别为1到22的数据页控制块，初始状态如下 以上就是LUR链表的实现过程，但是这种方式对于MySQL来说会有问题，所以MySQL并没有直接使用LRU链表的简单实现，而是对其做了一些改进，具体做了哪些改进，我们在下文中继续解释。 3.4 Flush链表前面解释了我们「对数据的读写都是先对Buffer Pool中的缓存页进行操作，然后在通过后台线程将脏页写入到磁盘，持久化到磁盘中，即刷脏」。 「脏页：当执行写入操作时，先更新的是缓存页，此时缓存页跟磁盘页的数据就会不一致，这就是常说的脏页」。 既然产生了脏页，那就是需要更新磁盘，也就是常说的刷脏，那如何确定那些缓存页需要刷脏呢？也不能吧所有的缓存页都重新刷新一百年磁盘，或者挨个遍历比对，这种方式肯定是不可取的，此时就需要Flush链表了。 3.4.1 Flush链表是个啥「Flush链表与Free链表的结构很类似，也由基节点与子节点组成」。 Flush链表是一个双向链表，链表结点是被修改过的缓存页对应的控制块（更新过的缓存页） Flush链表作用：帮助定位脏页，需要刷盘的缓存页 「基节点」：和free链表一样，链接首尾结点，并存储了有多少个描述信息块 「子节点」 「每个节点是脏页对应的控制块，即只要一个缓存页被修改，那它的控制块就会被放入Flush链表」 每个控制块块里都有两个指针pre（指向上一个节点），next（指向下一个节点） 「前面说了控制块其实是在Buffer Pool中的，控制块是通过上下节点的引用，组成一个链表，所以只需要通过基节点挨个遍历子节点，找到需要刷脏的数据页即可」。 3.4.2 Flush链表写入过程当我们在写入数据的时候，我们知道磁盘IO的效率很慢，所以MySQL不会直接更新直接更新磁盘，而是经过以下两个步骤： 第一步：更新Buffer Pool中的数据页，一次内存操作； 第二步：将更新操作顺序写Redo log，一次磁盘顺序写操作； 这样的效率是最高的。顺序写Redo log，每秒几万次，问题不大。 上图中描述了在更新数据页的时候，Flush链表的写入过程，其实这只是在被更新的数据已经别加载到Buffer Pool的前提下，如果我们要更新的数据没有别预先加载，那这个过程是不是会先去读取磁盘呢？实际上并不会，MySQL为了提高性能，减少磁盘IO，做了很多的优化，当数据页不存在Buffer Pool中的时候，会使用写缓冲(change buffer)来做更新操作，具体的实现原理下一篇文章再展开解释。 「当控制块被加入到Flush 链表后，后台线程就可以遍历 Flush 链表，将脏页写入到磁盘」。 3.5 Buffer Pool 的数据页上述了解了三种链表以及它们的使用方式，我们可以总结一下，「其实Buffer Pool 里有三种数据页页和链表来管理数据」。 Free Page（空闲页） 表示此数据页未被使用，是空的，其控制块位于 Free 链表； 「Clean Page（干净页）」 表示此数据页已被使用，缓存了数据， 其控制块位于LRU 链表。 「Dirty Page（脏页）」 表示此数据页【已被使用】且【已经被修改】，数据页中数据和磁盘上的数据已经不一致。 当脏页上的数据写入磁盘后，内存数据和磁盘数据一致，那么该页就变成了干净页。 「脏页的控制块同时存在于 LRU 链表和 Flush 链表」。 四、MySQL对LRU算法的改进在前文中我们说到了简单的LRU算法会对于MySQL来说会有问题，因此MySQL对LRU算法进行了改进，接下来就来看看LRU算法存在什么问题，MySQL又是怎么改进的。 先来说说LRU 算法存在的问题： 「预读失效」 「Buffer Pool 污染」 4.1 什么是预读既然LRU 算法存在预读失效的问题，先来看看什么是预读。 前面说到，为了减少磁盘IO，innoDB会把数据从磁盘读取到内存中使用，一般而言，数据的读取会遵循【集中读写】的原则，也就是当我们使用一些数据的时候，很大概率也会使用附件的数据，即【局部性原理】，它表明提前加载是有效的，能够减少磁盘IO。因此： 「磁盘数据读取到内存，并不是按需读取，而是按页读取，一次至少读一页数据（16K），如果未来要读取的数据就在页中，直接读取内存即可，不需要磁盘IO，提高效率」。这也就是常说的「预读」。 通过预读我们就可以事先先把数据读取放在内存中，下面来看一下「buffer pool的工作流程图」: buffer pool的工作流程图中以查询id为1的用户数据为例，大致可以分为三步： 第一步：先查询buffer pool是否存在对应的数据页，有的话则直接返回 第二步：buffer pool不存在对应的数据页，则去磁盘中查找，并把结果copy一份到buffer pool中，然后返回给客户端 第三步：下次有同样的查询，就可以直接查找buffer pool返回数据 例如：当id=1与id=2都在这个数据页中，那么下次查询Id=2的时候，就可以直接通过buffer pool返回。 这个过程看起来，感觉buffer pool跟缓存很类似，实际上它的缓存淘汰机制也跟Redis很类似。 4.2 什么是预读失效解释了什么是预读，那预读失效就很好理解了，「那些被提前加载进来的数据页并一直没有被访问，相当于预读是白费功夫，即预读失效」。 「通过简单的LRU链表的实现过程我们知道，预读的数据会被放到 LRU 链表头部，而当 Buffer Pool空间不够的时候，需要把末尾的页淘汰掉。如果这些预读的数据一直没有被使用，而把被使用的数据挤到了链表的尾部，进而被淘汰，那缓存的命中率就会大大降低」。这样的话，预读就适得其反了。 4.3 如何提高缓存的命中率预读的数据被使用到的时候，会减少磁盘IO，但是预读失效的时候，也会降低缓存的命中率，不能因为预读失效，而将预读机制去掉，所以我们要在保留预读这个机制的前提下提高缓存的命中率。 前面将在LRU链表的时候就解释了我们在读到数据之后，把对应的数据页放到LRU链表头部，因此想要提高缓存的命中率，只需要「让真正被访问的页才移动到 LRU 链表的头部，使其在 Buffer Pool 里停留的时间尽可能长，尽可能缩短预读的页停留在 Buffer Pool 里的时间」。 「提高缓存的命中率」 MySQL基于这种设计思路对LRU 算法进行了改进，将 LRU 划分了 2 个区域： 「划分old和young两个区域后，预读的页会被加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部」。 如果预读的页一直没有被访问，会一直存在old 区域，直到被移除，不会影响 young 区域中的热点数据。 「old 区域」：在LRU 链表的后半部分 「young 区域」：在 LRU 链表的前半部分 「old 区域占整个 LRU 链表长度的比例可以通过 innodb_old_blocks_pc 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37」。 1show variables like '%innodb_old_blocks_pc%'; 「提高缓存的命中率案例说明」 紧接着23号页被读取了，那么此时23号页就会被加入到young 区域的头部，而18号页则会被移动到Old区域的头部，这个过程不会有数据页被淘汰。 如果数据页23一直没有被读取，它就会一直存在于Old区，直到其他预读数据加载，慢慢将它淘汰。 假设现在有一个数据页23被预读到Buffer Pool中了，那23号页会被加载到old 区域头部，而old区域末尾的22号页会被淘汰掉 还是以刚才LRU链表长度为 22，节点分别为1到22的数据页控制块为案例，划分区域后初始状态如下： 由此可见，通过对LRU区域的划分，可以很多好的解决了预读失效的问题，提高了提高缓存的命中率。 4.4 什么是Buffer Pool 污染预读失效的问题解决了，接下来看看什么是Buffer Pool 污染。 我们知道当Sql执行的时候，「会数据加载到Buffer Pool ，而Buffer Pool的大小是有限的，如果加载大量数据，就会将Buffer Pool 里的所有页都替换出去，导致原本的热数据被淘汰」。下次访问的时候，又要重新去磁盘读取，导致数据库性能下降，这个过程就是「Buffer Pool 污染」。 「什么时候会加载大量数据呢」 SQL 语句扫描了大量的数据，并返回。 对大表进行全表扫描，比如： 12select * from user where name like &quot;%星河&quot;;select * from user where id+1 = 6 这两个sql 会导致索引失效而走全表扫描，导致全量加载数据到Buffer Pool中 「Buffer Pool加载大量数据」 从磁盘读取数据页加入到 LRU 链表的 old 区域头部 从数据页中读取行记录进行where进行匹配，这个过程会访问数据页，也就会将数据页加入到 young 区域头部。 由于是全表扫描，「因此所有数据都会被按照逐个加入young 区域头部，从而替换淘汰原有的 young 区域数据」。 4.5 如何解决Buffer Pool污染「Buffer Pool污染跟预读失效都是一样的会导致LRU的热点数据被替换和淘汰」，接下来看看如何解决Buffer Pool 污染而导致缓存命中率下降的问题？ 「问题分析」 其实我们可以针对以上全表扫描的情况进行分析， 全表扫描之所以会替换淘汰原有的LRU链表young 区域数据，主要是因为我们将原本只会访问一次的数据页加载到young 区。 这些数据实际上刚刚从磁盘被加载到Buffer Pool，然后就被访问，之后就不会用，基于此，我们是不是可以将数据放young 区的门槛提高有点，从而吧这种访问一次就不会用的数据过滤掉，把它挡在Old区，这样就不会污染young 区的热点数据了。 「解决Buffer Pool污染方案」 MySQL 解决方式就是提高了数据从Old区域进入到 young 区域门槛： 「先设定一个间隔时间innodb_old_blocks_time，然后将Old区域数据页的第一次访问时间在其对应的控制块中记录下来」。 这样看，其实「这个间隔时间innodb_old_blocks_time就是数据页必须在 old 区域停留的时间」。 如果后续的访问时间与第一次访问的时间「小于innodb_old_blocks_time」，则「不将该缓存页从 old 区域移动到 young 区域」。 如果后续的访问时间与第一次访问的时间「大于innodb_old_blocks_time」，才「会将该缓存页移动到 young 区域的头部」。 1show variables like '%innodb_old_blocks_time%'; 如上，innodb_old_blocks_time默认是 1s。 即：「当同时满足「数据页被访问」与「数据页在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部」。 通过这种方式，就过滤了上述那种全表扫描导致的将只会访问一次的数据页加载到young 区造成的Buffer Pool 污染的问题 。 「young 区域优化」 MySQL为了防止 young 区域节点频繁移动到头部，对 young 区域也做了一个优化： 「young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会」。 当访问前面4个数据页时（比如3号数据页），并不会将数据页移动到young 区的头部 当访问8号数据页，由于8数据页在后 3/4的young 区，所以8号会被移动到头部 比如访问如下LRU链表，young 区域一共有18个数据页，当我们访问young 区的数据页时： 五、脏页的刷盘时机通过对上述三种链表的描述，我们知道「当我们对数据进行修改时，其实修改的是Buffer Pool 中数据所在缓存页，修改后将其设置为脏页，并将脏页的控制块同时存在于 LRU 链表和 Flush 链表」。然后通过刷脏将修改同步至磁盘。 刷脏不是每次修改都进行的，那样性能会很差，因此刷脏是通过一定的时机触发进行批量刷盘的。 脏页的刷盘时机总的来说就分为以下种： redo log 日志满了的情况下，会主动触发脏页刷新到磁盘； MySQL 正常关闭之前，会把所有的脏页刷入到磁盘； Buffer Pool 空间不足时，会淘汰一部分数据页，如果淘汰的是脏页，需要先将其同步到磁盘。 MySQL 空闲时，后台线程会定期脏页刷盘 下面主要来看一下Buffer Pool 空间不足和后台线程的脏页刷盘过程 5.1 Buffer Pool内存不足触发刷脏「刷脏的目的是将修改的数据同步磁盘，释放Buffer Pool内存空间」。因此我们肯定是需要将访问的最少的数据页刷会磁盘，释放其数据页内存。 「基于这样的原则，我们只需要根据LRU链表，将其Old区域尾部节点输盘即可」。 我们在前面的描述中已经说了「对于修改的数据页的控制块同时存在于 LRU 链表和 Flush 链表，对于只有读取访问的数据页的控制块存在于 LRU 链表」。 如上图，Buffer Pool内存不足脏页刷盘分为两种情况： 「若缓存页同时在flush链表和lru链表中，说明数据被修改过，则需要刷脏，释放掉缓存页的内存，将控制块重新添加到free链表中」。 「若缓存页只是存在于LRU链表中，说明数据没有被修改过，则不需要刷脏，直接释放掉缓存页的内存，将控制块重新添加到free链表中」。 5.2 后台线程会定期脏页刷盘为了避免缓冲池内存不够，MySQL在后台有一个定时任务，通过单独的后台线程，不断从LRU链表Old区尾部的缓存页刷回至磁盘中并同时释放缓存页。 六、多实例Buffer Pool通过上述的描述我们知道：「Buffer Pool本质是InnoDB向操作系统申请的一块连续的内存空间」。 既然是内存空间，那么在多线程环境下，为保证数据的安全性，访问Buffer Pool中的数据都需要加锁处理。 6.1 什么是多实例Buffer Pool「当多线程并发访问量特别高时，单一的Buffer Pool可能会影响请求的处理速度。因此当Buffer Pool的内存空间很大的时候，可以将单一的Buffer Pool拆分成若干个小的Buffer Pool，每个Buffer Pool都称为一个独立的实例，各自去申请内存空间以及管理各种链表」。以此保证在多线程并发访问时不会相互影响，从而提高并发处理能力。 「innodb_buffer_pool_instances」 通过设置「innodb_buffer_pool_instances的值来修改Buffer Pool实例的个数，默认为1，最大可以设置为64」。 1show variables like '%innodb_buffer_pool_instances%'; 12[server]innodb_buffer_pool_instances = 2 如上配置表示创建2个buffer pool实例（缓冲池总量大小不变，即每个buffer pool的大小为原来的一半） 「每个Buffer Pool的内存空间」 「单个缓冲池实际占用内存空间 = 缓冲池大小 ÷ 缓冲池实例的个数，即」 : 「单个缓冲池实际占用内存空间 = innodb_buffer_pool_size ÷ innodb_buffer_pool_instances」 12[server]innodb_buffer_pool_instances = 2 如上配置表示创建2个buffer pool实例（缓冲池总量大小不变，即每个buffer pool的大小为原来的一半） 由于管理Buffer Pool需要性能开销，因此「并不是实例越多越好」。 在IInnoDB中，「当innodb_buffer_pool_size小于1GB时，innodb_buffer_pool_instances无效」。 当innodb_buffer_pool_size小于1GB，即使设置的innodb_buffer_pool_instances不为1，InnoDB默认也会把它改为1，这也是考虑到多实例管理的性能开销。 6.2 Buffer Pool的配置我们先来理解一下一个配置项：innodb_buffer_pool_chunk_size 「innodb_buffer_pool_chunk_size」 默认值128MB。可以按照1MB的单位进行增加或减小。 可以简单的理解成是Buffer Pool的总大小增加或缩小最小单位。 「innodb_buffer_pool_size的调整」 「Buffer Pool的总大小，必须是innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances的倍数」。 当innodb_buffer_pool_size不等于innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances的倍数时，服务器会自动把innodb_buffer_pool_size的值调整为【innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances】结果的整数倍 如果配置 123456# buffer_pool最小单位为128MBinnodb_buffer_pool_chunk_size=128MB# Buffer Pool实例的个数为16innodb_buffer_pool_instances=16# buffer_pool总大小为3GBinnodb_buffer_pool_size=3GB 由于 123innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances =128MB * 16 = 2GB而2GB 不等于 innodb_buffer_pool_size=3GB 则InnoDB会调整 12# InnoDB会调整buffer_pool总大小为4GBinnodb_buffer_pool_size = 4GB 「innodb_buffer_pool_chunk_size的调整」 在服务启动的时候，会进行如下计算，并判断结果调整innodb_buffer_pool_chunk_size的大小： 如果不等式成立： 「innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances &gt; innodb_buffer_pool_size」 则修改： 「innodb_buffer_pool_chunk_size = innodb_buffer_pool_size /innodb_buffer_pool_instances」 例如：如果配置 123456# buffer_pool最小单位为128MBinnodb_buffer_pool_chunk_size=256MB# Buffer Pool实例的个数为16innodb_buffer_pool_instances=16# buffer_pool总大小为3GBinnodb_buffer_pool_size=3GB 由于 123innodb_buffer_pool_chunk_size * innodb_buffer_pool_instances =256MB * 16 = 4GB而4GB 大于 innodb_buffer_pool_size=3GB 则InnoDB会调整 12# InnoDB会调整innodb_buffer_pool_chunk_size的大小为192MBinnodb_buffer_pool_chunk_size = innodb_buffer_pool_size / innodb_buffer_pool_instances = 3GB / 16 = 192MB","link":"/2021/08/06/MySQL%E5%8D%81%E5%85%AD%EF%BC%9A36%E5%BC%A0%E5%9B%BE%E7%90%86%E8%A7%A3Buffer%20Pool/"},{"title":"MySQL十：索引基础知识回顾","text":"1、索引简介1.1 什么是索引索引是对数据库表中一列或多列的值进行排序的一种结构，可以大大提高MySQL的检索速度。索引在MySQL中也叫做key，当表中的数据量越来越大时，索引对于查询性能的影响非常大。 那索引具体是什么呢，找几个生活中实例比较一下就清晰了： 新华字典：索引就相当于字典的音序表，我们可以通过音序表，快速在几百页中定位到我们要查找的字。 书店书架：索引就相当于书店里面的书架上的标签，可以通过标签，快速从成千上万本书中找到我们需要的书籍。 由此可知，其实索引就是一个目录，即数据库表的一个数据目录，帮助快速定位数据在磁盘中的位置，以此达到提高查询性能的目的。 1.2 索引的优缺点 优点 索引减小了需要扫描的数据量，从而大大加快数据的检索速度（创建索引的最主要的原因） 可以加速表与表的连接 可以显著的减少查询中分组和排序的时间 索引可以帮助服务器避免排序和创建临时表 索引可以将随机IO变成顺序IO 索引既然有这么多优点，那为什么不对表中每个列都建一个索引呢，这样不是更加能提升性能吗，实际上这是不可取的，索引虽然有诸多优点，但是也有很多缺点 缺点 对表中的数据进行增、删、改的时候，索引也要动态的维护，降低了数据的写入速度 随着数据量的增加，创建索引和维护索引要耗费时间也会越来越长，影响性能 索引的存储需要占物理空间，每一个索引都要占用一定的物理空间 2、创建索引准则基于以上索引的介绍，我们知道索引优缺点都很明显，我们不能在表数据中所有的列都添加索引，需要根据具体场景选择创建索引的列与类型。那么具体应该在那些列中添加索引，那些列中不能添加索引呢？ 能创建索引的列 主键索引，在MySQL中，主键列会默认的当成唯一性索引 在业务场景中被【当成条件查询的列】创建索引，可以提高查询效率 外键索引，比如需要【用于JOIN的列】创建索引，可以提高连接的速度 由于索引是已经排序的，所以在经常【用于范围查询的列】和需要【排序的列】创建索引，可以避免排序，提高查询效率 不能创建索引的列 以上几种情况的列，一般不建议创建索引，非但不能提高查询速度，反而增加索引后提高了数据的维护时间成本和空间成本。 经常用于计算的列 数据值很少或者大量重复的列 大字段的列 经常修改的列 很少使用的字段 3、MySQL索引的创建与分类3.1MySQL索引类型MySQL索引的类型其实只有五种，但是我们经常会听到很多种不同的索引，那其实是在不同维度划分的类型： 存储结构维度划分 B Tree索引、Hash索引、B + Tree索引 应用层次维度划分 普通索引、唯一索引、主键索引、全文索引，空间索引 空间索引基本不使用，这里不做介绍 索引键值类型维度划分 主键索引、辅助索引（二级索引） 数据存储和索引键值逻辑关系维度划分 聚集索引（聚簇索引）、非聚集索引（非聚簇索引） 索引组成维度划分 组合索引（复合索引）、单一索引 本文主要以应用层次维度来说明索引的分类，其他维度在后续文章中描述。 3.2MySQL索引的创建与删除 索引的创建 索引的创建方式有三种：建表时创建索引，已存在的表上直接创建索引，已存在的表上新增列并创建索引 建表时创建索引 123456CREATE TABLE 表名 ( 字段名1 数据类型 [完整性约束条件…], 字段名2 数据类型 [完整性约束条件…], [NORMAL | UNIQUE | FULLTEXT | SPATIAL ] INDEX | KEY [索引名] (字段名[(长度)] [ASC | DESC]) ); 已存在的表上直接创建索引 1CREATE [NORMAL | UNIQUE | FULLTEXT | SPATIAL ] INDEX 索引名 ON 表名 (字段名[(长度)] [ASC | DESC]) ; 已存在的表上新增列并创建索引（修改表结构） 1ALTER TABLE 表名 ADD [NORMAL | UNIQUE | FULLTEXT | SPATIAL ] INDEX 索引名 (字段名[(长度)] [ASC | DESC]) ; 名词解释 NORMAL | UNIQUE | FULLTEXT | SPATIAL 可选参数，Normal 普通索引，Unique 唯一索引，Full Text 全文索引，SPATIAL 空间索引 INDEX | KEY 同义词，作用相同，用来指定创建索引 ASC | DESC 指定升序或降序的索引值存储 索引的删除 1DROP INDEX 索引名 ON 表名字; 查看表结构 1desc table_name; 查看生成表的SQL 1show create table table_name; 查看索引结构信息 1show index from table_name; 查看SQL执行时间 123set profiling = 1;select * from user where id=1; show profiles; 3.3 普通索引最基本的索引类型，基于普通字段建立的索引，没有任何限制。 一张表可以创建多个普通索引，一个普通索引可以包含多个字段【组合索引】，允许数据重复，允许 NULL 值插入 建表时创建索引 123456789CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT , `name` char(255) CHARACTER NOT NULL , `idcard` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (name(length))) 已存在的表上直接创建索引 1CREATE INDEX index_name ON user (name(length)) 已存在的表上新增列并创建索引（修改表结构） 1ALTER TABLE user ADD INDEX index_name ON (name(length)) 通过以上三种方式为User表的name字段创建普通索引时，可以看到，并没有使用NORMAL关键字，这是因为在创建普通索引时，NORMAL关键字是可以省略的，直接使用Index即可。 3.4 唯一索引与普通索引基本相同类似，区别在于：唯一索引字段的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。在创建或修改表时追加唯一约束，就会自动创建对应的唯一索引。 建表时创建索引 123456789CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT , `name` char(255) CHARACTER NOT NULL , `idcard` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (name(length))) 已存在的表上直接创建索引 1CREATE UNIQUE INDEX index_name ON user (idcard(length)) 已存在的表上新增列并创建索引（修改表结构） 1ALTER TABLE user ADD UNIQUE index_name ON (idcard(length)) 通过以上三种方式为User表的idcard（身份证号码）字段创建唯一索引时，使用UNIQUE关键字。 3.5 主键索引是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。一般是在建表的时候同时创建主键索引，通过PRIMARY KEY关键字指定 12345678CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT , `name` char(255) CHARACTER NOT NULL , `idcard` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`)) 3.6 组合索引一个组合索引包含两个或两个以上的列。遵循 mysql 组合索引的【最左前缀原则】，即使用 where 时条件要按照索引建立时字段的排列方式放置索引才会生效。 1CREATE INDEX index_name ON user (name,idcard); 3.7 全文索引主要用来查找文本中的关键字，而不是直接与索引中的值相比较。 fulltext索引更像是一个搜索引擎，一般配合match against操作使用，而不是简单的where语句的like参数匹配。目前只有char、varchar，text 列上可以创建全文索引。 建表时创建索引 123456789CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT , `name` char(255) CHARACTER NOT NULL , `idcard` char(255) CHARACTER NOT NULL , `content` text CHARACTER NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), FULLTEXT (content)) 已存在的表上直接创建索引 1CREATE FULLTEXT INDEX index_name ON user(content) 已存在的表上新增列并创建索引（修改表结构） 1ALTER TABLE user ADD FULLTEXT index_name ON (content) 通过以上三种方式为user表的content字段创建全文索引时，使用FULLTEXT关键字。 在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用CREATE index创建fulltext索引，要比先为一张表建立fulltext然后再将数据写入的速度快很多。","link":"/2021/08/06/MySQL%E5%8D%81%EF%BC%9A%E7%B4%A2%E5%BC%95%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%9B%9E%E9%A1%BE/"},{"title":"MySQL四：InnoDB的存储结构","text":"「MySQL存储引擎最大的特点就是【插件化】，可以根据自己的需求使用不同的存储引擎，innodb存储引擎支持行级锁以及事务特性，也是多种场合使用较多的存储引擎。」 当官方的存储引擎不足以满足时，我们通过抽象的API接口实现自己的存储引擎。 抽象存储引擎API接口是通过抽象类handler来实现，handler类提供诸如打开/关闭table、扫表、查询Key数据、写记录、删除记录等基础操作方法。 每一个存储引擎通过继承handler类，实现以上提到的方法，在方法里面实现对底层存储引擎的读写接口的转调。 「InnoDB是为处理巨大数据量时的最大性能设计。它的CPU效率可能是任何其它基于磁盘的关系数据库引擎所不能匹敌的」。这是官网给出的一句话，可见InnoDB在mysql中的地位。 在MYSQL5.5版本，具体是在5.5.8版本之后,，「InnoDB代替MYISAM称为MYSQL的默认存储引擎」。 InnoDB存储引擎支持事务，具有自动崩溃恢复的特性，特点是行锁设计、支持外键，并支持类似于Oracle的非锁定读，即默认读取操作不会产生锁，在日常开发中使用非常广泛。 一、InnoDB架构组成InnoDB的存储结构分为「内存结构(左)和磁盘结构(右)两大部分」， 官方的InnoDB引擎架构图如下： MySQL 5.7以前的版本 MySQL 5.7 版本 由上面两张架构图可以看出，「InnoDB存储结构在MySQL 5.7 版本之后做了一些调整」 将 Undo日志表空间从共享表空间 ibdata 文件中分离出来，可以在安装MySQL 时由用户自行指定文件大小和数量。 增加了 temporary 临时表空间，里面存储着临时表或临时查询结果集的数据。 Buffer Pool 大小可以动态修改，无需重启数据库实例。 二、 InnoDB内存结构从架构图中可以看出【「内存结构主要包括Buffer Pool、Change Buffer、Adaptive Hash Index和Log Buffer四大组件」】。 2.1 Buffer Pool「即【缓冲池，简称BP】。BP以Page页为单位，默认大小16K，BP的底层采用链表数据结构管理Page」。 在InnoDB访问表记录和索引时会在Page页中缓存，以后使用可以减少磁 盘IO操作，提升效率。 「Page管理机制」 Page根据状态可以分为三种类型： 「InnoDB通过三种链表结构来维护和管理上述三种page类型」 【free list】：空闲缓冲区，管理free page 【flush list】：刷新到磁盘的缓冲区，管理dirty page 内部page按修改时间排序。脏页即存在于flush链表，也在LRU链表中，两种互不影响，「LRU链表负责管理page的可用性和释放，而flush链表负责管理脏页的刷盘操作」 。 【lru list】：正在使用的缓冲区，管理clean page和dirty page 缓冲区以midpoint为基点： 前面链表称为new列表区，存放经常访问的数据，占63%； 后面的链表称为old列表区，存放使用较少数据，占37%。 【free page】 ：空闲page，未被使用 【clean page】：被使用page，数据没有被修改过 【dirty page】：脏页，page的数据被修改过，页中数据和磁盘的数据产生了不 一致 「改进型LRU算法维护」 「每当有新的page数据读取到buffer pool时，InnoDb引擎会判断是否有空闲页，是否足够，如果有就将free page从free list列表删除，放入到LRU列表中。没有空闲页，就会根据LRU算法淘汰LRU链表默认的页，将内存空间释放分配给新的页。」 「普通LRU」 末尾淘汰法，新数据从链表头部加入，释放空间时从末尾淘汰 「改性LRU」 「链表分为new和old两个部分，加入元素时并不是从表头插入，而是从中间midpoint位置插入」，如果数据很快被访问，那么page就会向new列表头部移动，如果数据没有被访问，会逐步向old尾部移动，等待淘汰。 「Buffer Pool配置参数」 1show variables like '%innodb_page_size%'; 查看page页大小 查看lru list中old列表参数 1show variables like '%innodb_old%'; 查看buffer pool参数 1show variables like '%innodb_buffer%'; 「一般我们将innodb_buffer_pool_size设置为总内存大小的60%-80%， innodb_buffer_pool_instances可以设置为多个避免缓存争夺。」 2.2 Change Buffer「写【缓冲区，简称CB】。在进行DML操作时，如果BP没有其相应的Page数据， 并不会立刻将磁盘页加载到缓冲池，而是在CB记录缓冲变更，等未来数据被读取时，再将数据合并恢复到BP中。」 「ChangeBuffer占用BufferPool空间」，默认占25%，最大允许占50%，可以根据读写业务量来进行调整。 调整参数为：innodb_change_buffer_max_size 「当更新一条记录时，该记录在BufferPool存在，直接在BufferPool修改，一次内存操作。」 「如果该记录在BufferPool不存在（没有命中），会直接在ChangeBuffer进行一次内存操作，不用再去磁盘查询数据，避免一次磁盘IO。」 「当下次查询记录时，会先进行磁盘读取，然后再从 ChangeBuffer中读取信息合并，最终载入BufferPool中。」 「写缓冲区仅适用于非唯一普通索引页」 如果在索引设置唯一性，在进行修改时，InnoDB必须要做唯一性校验，因此必须查询磁盘， 做一次IO操作。会直接将记录查询到BufferPool中，然后在缓冲池修改，不会在 ChangeBuffer操作。 2.3 Adaptive Hash Index「即【自适应哈希索引】，用于优化对BP数据的查询」。 「InnoDB存储引擎会监控对表索引的查找，如果观察到建立哈希索引可以带来速度的提升，则建立自适应哈希索引，所以称之为自适应」。InnoDB存储引擎会自动根据访问的频率和模式来为某些页建立哈希索引。 实现本质上就是一个从某个检索条件到某个数据页的【哈希表】。 12345678mysql&gt; show variables like '%hash%';+----------------------------------+-------+| Variable_name | Value |+----------------------------------+-------+| innodb_adaptive_hash_index | ON || innodb_adaptive_hash_index_parts | 8 |+----------------------------------+-------+2 rows in set (0.00 sec) 「innodb_adaptive_hash_index」 控制innodb自适应哈希索引特性是否开启参数 「innodb_adaptive_hash_index_parts」 凡是缓存都会涉及多个缓存消费者间的锁竞争。 MySQL通过设立多个AHI分区，每个分区使用独立的锁，来减少锁竞争。 2.4 Log Buffer「即【日志缓冲区】，用来保存要写入磁盘上log文件（Redo/Undo）的数据」。 日志缓冲区刷盘时机： 日志缓冲区的内容「定期刷新」到磁盘log文件中。 「日志缓冲区满时会自动将其刷新」到磁盘，可以改变innodb_log_buffer_size参数大小，减少磁盘IO频率。 当遇到BLOB类型或多行更新的大事务操作时，增加日志缓冲区可以节省磁盘I/O。 LogBuffer主要是用于记录InnoDB引擎日志，在DML操作时会产生Redo和Undo日志。 三、 InnoDB磁盘结构InnoDB磁盘主要包含【Tablespaces，InnoDB Data Dictionary，Doublewrite Buffer、Redo Log 和Undo Logs】五部分组成。 3.1 表空间（Tablespaces）innodb存储引擎在存储设计上模仿了Oracle的存储结构，其数据是按照表空间进行管理的。「表空间用于存储表结构和数据」。表空间又分为系统表空间、独立表空间、 通用表空间、临时表空间、Undo表空间等多种类型。 3.1.1 表空间组成 「物理结构组成」 「在系统表空间由于所有的表公用一个.ibdatat1数据文件，所以针对每个表只有一个.frm表结构文件」。 「在独立表空间中，每个表分别都有一个.frm表结构文件，一个.ibd数据文件」。 innodb存储引擎物理组织形式可以理解为其在磁盘的存储形式，表现为各种文件，其分类大概为 文件 功能 描述 ibdatat1 共享表空间文件 系统/共享表空间，存储各种缓冲数据 .frm 表定义文件 记录表的定义，列名以及列的数据类型 .ibd 表数据存储文件 独立表空间，存储数据表的数据，按行存储 ib_logfile0/1 redo日志文件 重做日志文件，一共两个循环使用，一个写完即写另一个 「表空间是innodb存储引擎逻辑结构的最高层，所有的数据都存储在表空间中，默认innodb有一个共享表空间，所有的数据都存储在共享表空间中，可以通参数innodb_per_table设置每张表单独存放在一个表空间中」。 「独立表空间内存储的只是数据、索引、插入缓冲页」。 「回滚日志、插入缓冲索引页、事务信息、二次写缓冲等其他数据还是存放在共享表空间」。 3.1.2 表空间的五种类型 「系统表空间」（The System Tablespace） innodb_data_file_path用来指定innodb tablespace文件，如果我们不在My.cnf文件中指定innodb_data_home_dir和innodb_data_file_path那么默认会在datadir目录下创建ibdata1 作为innodb tablespace。 12345#默认值:innodb_data_file_path = ibdata1:12M:autoextend# ibdata1 : 文件名为ibdata1# 12M : 大小为12M# autoextend : 自动扩展 包含InnoDB数据字典，Doublewrite Buffer，Change Buffer，Undo Logs的「存储区域」。 系统表空间也默认包含任何用户在「系统表空间创建」的表数据和索引数据。 系统表空间是一个共享的表空间因为它是被多个表共享的。 「独立表空间」（File-Per-Table Tablespaces） 「默认开启，独立表空间是一个单表表空间，该表创建于自己的数据文件中，而非创建于系统表空间中」。 开启独立表空间参数为：innodb_file_per_table 12innodb_file_per_table = ON：独立表空间：tablename.ibdinnodb_file_per_table = OFF：系统表空间：ibdataX 【innodb_file_per_table = ON】 「新建表被创建于【表空间】中,每一个表建立ibd的扩展文件,文件名为：表名.ibd」，该文件默认被创建于数据库目录中,表空间的表文件支持动态和压缩行格式。 【innodb_file_per_table = OFF】 「innodb将被创建于【系统表空间】中，即ibdataX中」。X代表从1开始的一个数字 【查看表的存储表空间的存储方式值】 1show variables like 'innodb_file_per_table'; 【修改存储表空间的存储方式值为OFF】 1set global innodb_file_per_table=off; 「通用表空间」（General Tablespaces） 通用表空间为通过create tablespace语法创建的共享表空间。通用表空间可以创建于 mysql数据目录外的其他表空间，其可以容纳多张表，且其支持所有的行格式。 1234#创建表空间tablespaces1CREATE TABLESPACE tablespaces1 ADD DATAFILE tablespaces1.ibd Engine=InnoDB; #将表添加到test1表空间CREATE TABLE test1 (c1 INT PRIMARY KEY) TABLESPACE tablespaces1; 「撤销表空间」（Undo Tablespaces） 「撤销表空间由一个或多个包含Undo日志文件组成。」 在MySQL 5.7版本之前Undo占用的是System Tablespace共享区，从5.7开始将Undo从System Tablespace分离了出来。 【innodb_undo_tablespaces】 innodb_undo_tablespaces = 0 ：默认值，表示使用系统表空间ibdata1 innodb_undo_tablespaces = 1：大于0表示使用undo表空间undo_001、 undo_002等 「临时表空间」（Temporary Tablespaces） 「mysql服务器正常关闭或异常终止时，临时表空间将被移除，每次启动时会被重新创建」。 临时表空间分为两种： 【session temporary tablespaces】 存储的是用户创建的临时表和磁盘内部的临时表 【global temporary tablespace】 储存用户临时表的回滚段（rollback segments ） 3.1.3 系统表空间与独立表空间怎么选择系统表空间与独立表空间都是「存储表结构和数据」，只是存储位置和方法不同，那么应该怎么选择呢 系统表空间与独立表空间差异 系统表空间由于所有的数据都放ibdataX文件中，「容易产生IO瓶颈」 独立表空间单表存储，可以同时刷新多个文件数据 「基于此，我们一般都是使用独立的表空间进行管理，独立表空间也是默认的配置」 如何把系统表空间中的表转移到独立表空间中 使用mysqldump导出所有数据库表的数据(备份)。 停止MYSQL服务器，修改my.conf配置，删除原来innodb表空间的相关文件 12#修改my.conf配置innodb_file_per_table = ON 重启MYSQL服务,并重建Innodb系统表空间 重新导入备份的数据 3.2数据字典（InnoDB Data Dictionary）「InnoDB数据字典由内部系统表组成」。这些表包含用于查找表、索引和表字段等对象的元数据。 元数据物理上位于InnoDB系统表空间中。数据字典元数据在一定程度上 与InnoDB表元数据文件（.frm文件）中存储的信息重叠。 3.3 双写缓冲区（Doublewrite Buffer）「位于系统表空间，是一个存储区域」。 在BufferPage的page页刷新到磁盘真正的位置前，会先将数据存在Doublewrite 缓冲区。如果在page页写入过程中出现操作系统、存储子系统或 mysqld进程崩溃，InnoDB可以在崩溃恢复期间从Doublewrite缓冲区中找到page页备份。在大多数情况下，默认情况下启用双写缓冲区。 innodb_doublewrite = 0 ：禁用Doublewrite 缓冲区 innodb_flush_method = O_DIRECT ：数据文件写入操作会通知操作系统不要缓存数据，也不要用预读 「innodb_flush_method控制innodb数据文件及redo log的打开、 刷写模式」。 3.4 重做日志（Redo Log） 「重做日志是一种基于磁盘的数据结构，用于在崩溃恢复期间修正不完整事务写入的数据」。 MySQL以循环方式写入重做日志文件，记录InnoDB中所有对Buffer Pool修改的日志。 当出现实例故障，导致数据未能更新到数据文件，则数据库重启时须redo，重新把数据更新到数据文件。读写事务在执行的过程中，都会不断的产生redo log。默认情况下，重做日志在磁盘上由两个名为ib_logfile0和ib_logfile1的文件物理表示。 3.5 撤销日志（Undo Logs）「撤消日志是在事务开始之前保存的被修改数据的备份，用于回滚事务」。 撤消日志属于逻辑日志，根据每行记录进行记录。 撤消日志存在于系统表空间、撤消表空间和临时表空间中。","link":"/2021/08/06/MySQL%E5%9B%9B%EF%BC%9AInnoDB%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/"},{"title":"Mybatis Plus的分页插件的小问题","text":"一、前言在spring Boot环境下快速应用Mybatis plus，篇幅中我们使用了BaseMapper，从而可以直接使用selectPage这样的分页，但如果你够细心的话，返回的数据确实是分页后的数据，但在控制台打印的SQL语句其实并没有真正的物理分页，而是通过缓存来获得全部数据中再进行的分页，这样对于大数据量操作时是不可取的，那么接下来就叙述一下，真正实现物理分页的方法。 二、分页配置官方在分页插件上如是描述：自定义查询语句分页（自己写sql/mapper），也就是针对自己在Mapper中写的方法，但经过测试，如果不配置分页插件，其默认采用的分页为RowBounds的分页即逻辑分页，也就是先把数据记录全部查询出来,然在再根据offset和limit截断记录返回（数据量大的时候会造成内存溢出），故而不可取，而通过分页插件的配置即可达到物理分页效果。 新建一个MybatisPlusConfig配置类文件，代码如下所示： 123456789101112131415161718192021222324252627package com.szss.admin.config.mybatisplus; import com.baomidou.mybatisplus.plugins.PaginationInterceptor;import org.mybatis.spring.annotation.MapperScan;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration; /** * @author Allen * @date 2018/3/14 */@Configuration@MapperScan(&quot;com.szss.admin.dao.*&quot;)public class MybatisPlusConfig { /** * mybatis-plus分页插件&lt;br&gt; * 文档：http://mp.baomidou.com&lt;br&gt; */ @Bean public PaginationInterceptor paginationInterceptor() { PaginationInterceptor paginationInterceptor = new PaginationInterceptor(); return paginationInterceptor; } } 至此再次对之前代码进行测试，就会发现本次生成的SQL语句有所变化，不再是直接查询所有结果而进行的逻辑分页，而是会自动根据数据库生成对应的物理分页SQL语句，具体可自行在控制台查看SQL体现，本例示例如下： 1234562018-03-14 12:27:04.459 DEBUG 6008 --- [ XNIO-2 task-7] com.szss.admin.dao.RoleDAO.selectPage : ==&gt; Preparing: SELECT COUNT(1) FROM admin_role WHERE deleted = 0 2018-03-14 12:27:04.474 DEBUG 6008 --- [ XNIO-2 task-7] com.szss.admin.dao.RoleDAO.selectPage : ==&gt; Parameters: 2018-03-14 12:27:04.487 DEBUG 6008 --- [ XNIO-2 task-7] com.szss.admin.dao.RoleDAO.selectPage : ==&gt; Preparing: WITH query AS (SELECT TOP 100 PERCENT ROW_NUMBER() OVER (ORDER BY CURRENT_TIMESTAMP) as __row_number__, ID AS id,role,name,description,enabled,deleted,creator_id AS creatorId,creator,date_created AS dateCreated,modifier_id AS modifierId,modifier,last_modified AS lastModified FROM admin_role WHERE deleted=0) SELECT * FROM query WHERE __row_number__ BETWEEN 6 AND 10 ORDER BY __row_number__ 2018-03-14 12:27:04.488 DEBUG 6008 --- [ XNIO-2 task-7] com.szss.admin.dao.RoleDAO.selectPage : ==&gt; Parameters: 2018-03-14 12:27:04.499 DEBUG 6008 --- [ XNIO-2 task-7] com.szss.admin.dao.RoleDAO.selectPage : &lt;== Total: 3 而在没有配置分页属性时执行的SQL信息输出内容为： 12342018-03-14 15:03:50.525 DEBUG 6892 --- [ XNIO-2 task-5] com.szss.admin.dao.RoleDAO.selectPage : ==&gt; Preparing: SELECT ID AS id,role,name,description,enabled,deleted,creator_id AS creatorId,creator,date_created AS dateCreated,modifier_id AS modifierId,modifier,last_modified AS lastModified FROM admin_role WHERE deleted=0 2018-03-14 15:03:50.540 DEBUG 6892 --- [ XNIO-2 task-5] com.szss.admin.dao.RoleDAO.selectPage : ==&gt; Parameters: 2018-03-14 15:03:50.557 DEBUG 6892 --- [ XNIO-2 task-5] com.szss.admin.dao.RoleDAO.selectPage : &lt;== Total: 8 通过swagger进行数据测试时返回的展示结果是一样的，但是控制台输出的SQL语句明显有所区别，当启用分页插件时，首先会进行一个count的总记录条件查询，然后再进行物理分页操作，查询结果为3条记录，而默认是直接获取8条全部数据在由Mybatis进行逻辑分页，这个在大数据量操作时显然是不可取的。 三、分页应用由于我们在DAO层继承了BaseMapper接口，而我们所需要的就是通过Service层来继承ServiceImpl接口，具体代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.szss.admin.service; import java.util.Date; import org.springframework.beans.BeanUtils;import org.springframework.stereotype.Service; import com.baomidou.mybatisplus.mapper.EntityWrapper;import com.baomidou.mybatisplus.plugins.Page;import com.baomidou.mybatisplus.service.impl.ServiceImpl;import com.szss.admin.dao.RoleDAO;import com.szss.admin.model.domain.RoleDO;import com.szss.admin.model.param.ListRoleParam;import com.szss.admin.model.param.RoleParam; /** * 角色服务 * * @author Allen * @date 2018/3/7 */@Servicepublic class RoleService extends ServiceImpl&lt;RoleDAO, RoleDO&gt; { /** * 新增角色信息 * @param roleParam 角色参数 * @return 是否成功 */ public Boolean insert(RoleParam roleParam) { RoleDO roleDO = new RoleDO(); roleParam.setDateCreated(new Date()); BeanUtils.copyProperties(roleParam, roleDO); return insert(roleDO); } /** * 更新角色信息 * @param roleParam 角色参数 * @return 是否成功 */ public Boolean update(RoleParam roleParam) { RoleDO roleDO = selectById(roleParam.getId()); roleParam.setLastModified(new Date()); BeanUtils.copyProperties(roleParam, roleDO); return updateById(roleDO); } /** * 查询角色列表(分页) * * @param roleParam 角色参数 * @return 查询角色分页列表 */ public Page&lt;RoleDO&gt; selectListPage(ListRoleParam roleParam) { RoleDO roleDO = new RoleDO(); BeanUtils.copyProperties(roleParam, roleDO); Page&lt;RoleDO&gt; page = new Page&lt;RoleDO&gt;((int)roleParam.getPi(), (int)roleParam.getPs()); EntityWrapper&lt;RoleDO&gt; eWrapper = new EntityWrapper&lt;RoleDO&gt;(roleDO); Page&lt;RoleDO&gt; roleDOList = selectPage(page, eWrapper); return roleDOList; } } 这样我们就可以直接使用ServiceImpl实现类中的基本Insert、Update、Delete等操作了，而无须在Service层注入DAO来实现数据库的DML操作了。同时这里selectPage返回的是Page对象与DAO中返回的List对象是有所不同的，等于已经进行了翻页方法的封装，将需要的Page结果直接回传给页面，省去自己编写分页的操作。 四、注意事项 1、上述中在查询构造器EW中，我们采用的是new EntityWrapper(roleDO)方法，而不是大多数示例的new EntityWrapper().eq(“name”,”张三”)的这种方式，因为这样就意味着前台name参数是必须要传值的，而在我们大多数设置查询时，用户输入的查询条件是不固定及不明确的，所以本示例中采用如上方法来进行动态生成查询条件，当roleDO前台未传入值时，不会生成对应的where条件，而一旦使用了eq这样的方法，无论前台是否传值都会生成对应的where条件导致操作异常的错误。 2、在使用上述方法时domain中的RoleDO通过开启AR(ActiveRecord)模式（&lt;Spring Boot环境下Mybatis Plus的快速应用&gt;已有说明），即RoleDO需要继承Model方法，可直接使用roleDO来进行基本CRUD和分页查询操作。 3、当开启AR(ActiveRecord)模式后，对于service层就无须再继承ServiceImpl接口了，可直接进行DML和DQL操作，具体示例代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package com.szss.admin.service; import com.baomidou.mybatisplus.mapper.EntityWrapper;import com.baomidou.mybatisplus.plugins.Page;import com.baomidou.mybatisplus.service.impl.ServiceImpl;import com.szss.admin.dao.RoleDAO;import com.szss.admin.model.domain.RoleDO;import com.szss.admin.model.dto.ListRoleDTO;import com.szss.admin.model.dto.PageInfo;import com.szss.admin.model.dto.RestCodeEnum;import com.szss.admin.model.dto.RoleDTO;import com.szss.admin.model.param.ListRoleParam;import com.szss.admin.model.param.RoleParam;import java.sql.Timestamp;import java.util.ArrayList;import java.util.Date;import java.util.List;import org.springframework.beans.BeanUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.util.CollectionUtils; /** * 角色服务 * * @author Allen * @date 2018/3/7 */@Servicepublic class RoleService /**extends ServiceImpl&lt;RoleDAO, RoleDO&gt;*/ { /** * 新增角色信息 * * @param roleParam 角色信息 */ public void insert(RoleParam roleParam) { roleParam.setDateCreated(new Date()); RoleDO roleDO = new RoleDO(); BeanUtils.copyProperties(roleParam, roleDO); roleDO.insert(); } /** * 更新角色信息 * * @param roleParam 角色信息 */ public void update(RoleParam roleParam) { RoleDO roleDO = new RoleDO(); roleParam.setLastModified(new Date()); BeanUtils.copyProperties(roleParam, roleDO); roleDO.updateById(); } /** * 查询角色列表(分页) * * @param roleParam * @return */ public Page&lt;RoleDO&gt; selectPage(ListRoleParam roleParam) { RoleDO roleDO = new RoleDO(); BeanUtils.copyProperties(roleParam, roleDO); Page&lt;RoleDO&gt; page = new Page&lt;RoleDO&gt;(roleParam.getPi().intValue(), roleParam.getPs().intValue()); EntityWrapper&lt;RoleDO&gt; eWrapper = new EntityWrapper&lt;RoleDO&gt;(roleDO); Page&lt;RoleDO&gt; roleDOList = roleDO.selectPage(page,eWrapper); return roleDOList; } } 至此就可以看出AR与非AR之间的区别了，当你引用了Model以后就等于已经实现了相关CRUD和分页查询操作了。至于喜欢哪种方式或哪种更适合你，可自由根据情况进行自主选择。","link":"/2019/05/08/Mybatis-Plus%E7%9A%84%E5%88%86%E9%A1%B5%E6%8F%92%E4%BB%B6%E7%9A%84%E5%B0%8F%E9%97%AE%E9%A2%98/"},{"title":"OOM 的介绍","text":"OOM 分析Java 堆内存溢出在 Java 堆中只要不断的创建对象，并且 GC-Roots 到对象之间存在引用链，这样 JVM 就不会回收对象。 只要将-Xms(最小堆),-Xmx(最大堆) 设置为一样禁止自动扩展堆内存。 当使用一个 while(true) 循环来不断创建对象就会发生 OutOfMemory，还可以使用 -XX:+HeapDumpOnOutOfMemoryError 当发生 OOM 时会自动 dump 堆栈到文件中。 伪代码: 123456public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;&gt;(10) ; while (true){ list.add(&quot;1&quot;) ; }} 当出现 OOM 时可以通过工具来分析 GC-Roots 引用链 ，查看对象和 GC-Roots 是如何进行关联的，是否存在对象的生命周期过长，或者是这些对象确实改存在的，那就要考虑将堆内存调大了。 12345678910111213141516Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3210) at java.util.Arrays.copyOf(Arrays.java:3181) at java.util.ArrayList.grow(ArrayList.java:261) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227) at java.util.ArrayList.add(ArrayList.java:458) at com.crossoverjie.oom.HeapOOM.main(HeapOOM.java:18) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)Process finished with exit code 1 java.lang.OutOfMemoryError: Java heap space表示堆内存溢出。 更多内存溢出相关实战请看这里：强如 Disruptor 也发生内存溢出？ MetaSpace (元数据) 内存溢出 JDK8 中将永久代移除，使用 MetaSpace 来保存类加载之后的类信息，字符串常量池也被移动到 Java 堆。 PermSize 和 MaxPermSize 已经不能使用了，在 JDK8 中配置这两个参数将会发出警告。 JDK 8 中将类信息移到到了本地堆内存(Native Heap)中，将原有的永久代移动到了本地堆中成为 MetaSpace ,如果不指定该区域的大小，JVM 将会动态的调整。 可以使用 -XX:MaxMetaspaceSize=10M 来限制最大元数据。这样当不停的创建类时将会占满该区域并出现 OOM。 123456789101112131415public static void main(String[] args) { while (true){ Enhancer enhancer = new Enhancer() ; enhancer.setSuperclass(HeapOOM.class); enhancer.setUseCache(false) ; enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { return methodProxy.invoke(o,objects) ; } }); enhancer.create() ; }} 使用 cglib 不停的创建新类，最终会抛出: 1234567891011Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at net.sf.cglib.core.ReflectUtils.defineClass(ReflectUtils.java:459) at net.sf.cglib.core.AbstractClassGenerator.generate(AbstractClassGenerator.java:336) ... 11 moreCaused by: java.lang.OutOfMemoryError: Metaspace at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:763) ... 16 more 注意：这里的 OOM 伴随的是 java.lang.OutOfMemoryError: Metaspace 也就是元数据溢出。","link":"/2019/05/22/OOM-%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"title":"Netty-使用Netty传输POJO对象","text":"Netty-使用Netty传输POJO对象 使用Netty传输POJO对象，重点在于对象的序列化，序列化后的对象可以通过TCP流进行网络传输，结合Netty提供的对象编解码器，可以做到远程传输对象。 下面我们来看一个例子：模拟订票 首先Java序列化的POJO对象需要实现java.io.Serializable接口。 火车车次和余票量POJO：123456789101112131415161718192021222324252627282930package bookticket; import java.io.Serializable;/** * 火车pojo对象 * @author xwalker */public class Train implements Serializable { private static final long serialVersionUID = 1510326612440404416L; private String number;//火车车次 private int ticketCounts;//余票数量 public Train(String number,int ticketCounts){ this.number=number; this.ticketCounts=ticketCounts; } public String getNumber() { return number; } public void setNumber(String number) { this.number = number; } public int getTicketCounts() { return ticketCounts; } public void setTicketCounts(int ticketCounts) { this.ticketCounts = ticketCounts; } } 车票POJO：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package bookticket; import java.io.Serializable;import java.util.Date;/** * 订票POJO对象 * @author xwalker */public class Ticket implements Serializable { private static final long serialVersionUID = 4228051882802183587L; private String trainNumber;//火车车次 private int carriageNumber;//车厢编号 private String seatNumber;//座位编号 private String number;//车票编号 private User user;//订票用户 private Date bookTime;//订票时间 private Date startTime;//开车时间 public String getNumber() { return number; } public void setNumber(String number) { this.number = number; } public Date getBookTime() { return bookTime; } public void setBookTime(Date bookTime) { this.bookTime = bookTime; } public Date getStartTime() { return startTime; } public void setStartTime(Date startTime) { this.startTime = startTime; } public User getUser() { return user; } public void setUser(User user) { this.user = user; } public String getTrainNumber() { return trainNumber; } public void setTrainNumber(String trainNumber) { this.trainNumber = trainNumber; } public int getCarriageNumber() { return carriageNumber; } public void setCarriageNumber(int carriageNumber) { this.carriageNumber = carriageNumber; } public String getSeatNumber() { return seatNumber; } public void setSeatNumber(String seatNumber) { this.seatNumber = seatNumber; }} 用户POJO:123456789101112131415161718192021222324252627282930313233343536373839package bookticket; import java.io.Serializable;/** * 用户POJO对象 * @author xwalker */public class User implements Serializable { private static final long serialVersionUID = -3845514510571408376L; private String userId;//身份证 private String userName;//姓名 private String phone;//电话 private String email;//邮箱 public String getUserId() { return userId; } public void setUserId(String userId) { this.userId = userId; } public String getUserName() { return userName; } public void setUserName(String userName) { this.userName = userName; } public String getPhone() { return phone; } public void setPhone(String phone) { this.phone = phone; } public String getEmail() { return email; } public void setEmail(String email) { this.email = email; }} 请求指令集：12345678910111213package bookticket; /** * 指令集 * @author xwalker * */public class Code { public static final int CODE_SEARCH=1;//查询车票余量 public static final int CODE_BOOK=2;//订票 public static final int CODE_NONE=-1;//错误指令 无法处理} 客户端发送的请求信息：123456789101112131415161718192021222324252627282930313233343536373839404142package bookticket; import java.io.Serializable;import java.util.Date;/** * 订票人发送查询余票和订票使用的请求信息 * @author xwalker * */public class BookRequestMsg implements Serializable { private static final long serialVersionUID = -7335293929249462183L; private User user;//发送订票信息用户 private String trainNumber;//火车车次 private int code;//查询命令 private Date startTime;//开车时间 public User getUser() { return user; } public void setUser(User user) { this.user = user; } public String getTrainNumber() { return trainNumber; } public void setTrainNumber(String trainNumber) { this.trainNumber = trainNumber; } public Date getStartTime() { return startTime; } public void setStartTime(Date startTime) { this.startTime = startTime; } public int getCode() { return code; } public void setCode(int code) { this.code = code; } } 服务器接收订票和查票后处理完业务反馈客户端的信息：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package bookticket; import java.io.Serializable;import java.util.Date;/** * 订票成功与否反馈信息 * @author xwalker */public class BookResponseMsg implements Serializable { private static final long serialVersionUID = -4984721370227929766L; private boolean success;//是否操作成功 private User user;//请求用户 private String msg;//反馈信息 private int code;//请求指令 private Train train;//火车车次 private Date startTime;//出发时间 private Ticket ticket;//订票成功后具体出票票据 public boolean getSuccess() { return success; } public void setSuccess(boolean success) { this.success = success; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public Ticket getTicket() { return ticket; } public void setTicket(Ticket ticket) { this.ticket = ticket; } public int getCode() { return code; } public void setCode(int code) { this.code = code; } public Train getTrain() { return train; } public void setTrain(Train train) { this.train = train; } public Date getStartTime() { return startTime; } public void setStartTime(Date startTime) { this.startTime = startTime; } public User getUser() { return user; } public void setUser(User user) { this.user = user; } } 订票服务器：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package bookticket; import java.util.ArrayList;import java.util.List; import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioServerSocketChannel;import io.netty.handler.codec.serialization.ClassResolvers;import io.netty.handler.codec.serialization.ObjectDecoder;import io.netty.handler.codec.serialization.ObjectEncoder;import io.netty.handler.logging.LogLevel;import io.netty.handler.logging.LoggingHandler; /** * 订票服务器端 * @author xwalker * */public class BookTicketServer { public static List&lt;Train&gt; trains; /** * 初始化 构造车次和车票余数 */ public BookTicketServer() { trains=new ArrayList&lt;Train&gt;(); trains.add(new Train(&quot;G242&quot;,500)); trains.add(new Train(&quot;G243&quot;,200)); trains.add(new Train(&quot;D1025&quot;,100)); trains.add(new Train(&quot;D1235&quot;,0)); } public void bind(int port) throws Exception{ //配置NIO线程组 EventLoopGroup bossGroup=new NioEventLoopGroup(); EventLoopGroup workerGroup=new NioEventLoopGroup(); try{ //服务器辅助启动类配置 ServerBootstrap b=new ServerBootstrap(); b.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 100) .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { //添加对象解码器 负责对序列化POJO对象进行解码 设置对象序列化最大长度为1M 防止内存溢出 //设置线程安全的WeakReferenceMap对类加载器进行缓存 支持多线程并发访问 防止内存溢出 ch.pipeline().addLast(new ObjectDecoder(1024*1024,ClassResolvers.weakCachingConcurrentResolver(this.getClass().getClassLoader()))); //添加对象编码器 在服务器对外发送消息的时候自动将实现序列化的POJO对象编码 ch.pipeline().addLast(new ObjectEncoder()); ch.pipeline().addLast(new BookTicketServerhandler()); } }); //绑定端口 同步等待绑定成功 ChannelFuture f=b.bind(port).sync(); //等到服务端监听端口关闭 f.channel().closeFuture().sync(); }finally{ //优雅释放线程资源 bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } public static void main(String[] args) throws Exception { int port =8000; new BookTicketServer().bind(port); } } 服务器端网络IO处理器，查票订票业务处理和反馈：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package bookticket; import java.util.Date;import java.util.Random; import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext;/** * 订票server端处理器 * @author xwalker * */public class BookTicketServerhandler extends ChannelHandlerAdapter { @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { BookRequestMsg requestMsg=(BookRequestMsg) msg; BookResponseMsg responseMsg=null; switch (requestMsg.getCode()) { case Code.CODE_SEARCH://查询余票 for(Train train:BookTicketServer.trains){ //找到车次与请求车次相同的 返回车次余票 if(requestMsg.getTrainNumber().equals(train.getNumber())){ responseMsg=new BookResponseMsg(); responseMsg.setUser(requestMsg.getUser()); responseMsg.setCode(Code.CODE_SEARCH); responseMsg.setSuccess(true); responseMsg.setTrain(train); responseMsg.setStartTime(requestMsg.getStartTime()); responseMsg.setMsg(&quot;火车【&quot;+train.getNumber()+&quot;】余票数量为【&quot;+train.getTicketCounts()+&quot;】&quot;); break; } } if(responseMsg==null){ responseMsg=new BookResponseMsg(); responseMsg.setUser(requestMsg.getUser()); responseMsg.setCode(Code.CODE_SEARCH); responseMsg.setSuccess(false); responseMsg.setMsg(&quot;火车【&quot;+requestMsg.getTrainNumber()+&quot;】的信息不存在！&quot;); } break; case Code.CODE_BOOK://确认订票 for(Train train:BookTicketServer.trains){ //找到车次与请求车次相同的 返回车次余票 if(requestMsg.getTrainNumber().equals(train.getNumber())){ responseMsg=new BookResponseMsg(); responseMsg.setUser(requestMsg.getUser()); responseMsg.setSuccess(true); responseMsg.setCode(Code.CODE_BOOK); responseMsg.setMsg(&quot;恭喜您，订票成功！&quot;); Ticket ticket=new Ticket(); ticket.setBookTime(new Date()); ticket.setUser(requestMsg.getUser()); ticket.setStartTime(requestMsg.getStartTime()); ticket.setNumber(train.getNumber()+System.currentTimeMillis());//生成车票编号 ticket.setCarriageNumber(new Random().nextInt(15));//随机车厢 ticket.setUser(requestMsg.getUser());//设置订票人信息 String[] seat=new String[]{&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;}; Random seatRandom=new Random(); ticket.setSeatNumber(seat[seatRandom.nextInt(5)]+seatRandom.nextInt(100)); ticket.setTrainNumber(train.getNumber()); train.setTicketCounts(train.getTicketCounts()-1);//余票减去一张 responseMsg.setTrain(train); responseMsg.setTicket(ticket); break; } } if(responseMsg==null){ responseMsg=new BookResponseMsg(); responseMsg.setUser(requestMsg.getUser()); responseMsg.setCode(Code.CODE_BOOK); responseMsg.setSuccess(false); responseMsg.setMsg(&quot;火车【&quot;+requestMsg.getTrainNumber()+&quot;】的信息不存在！&quot;); } break; default://无法处理 responseMsg=new BookResponseMsg(); responseMsg.setUser(requestMsg.getUser()); responseMsg.setCode(Code.CODE_NONE); responseMsg.setSuccess(false); responseMsg.setMsg(&quot;指令无法处理！&quot;); break; } ctx.writeAndFlush(responseMsg); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); }} 客户端：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package bookticket; import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.ChannelOption;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;import io.netty.handler.codec.serialization.ClassResolvers;import io.netty.handler.codec.serialization.ObjectDecoder;import io.netty.handler.codec.serialization.ObjectEncoder; /** * 订票客户端 * @author xwalker */public class BookTicketClient { public void connect(int port,String host) throws Exception{ //配置客户端线程组 EventLoopGroup group=new NioEventLoopGroup(); try{ //配置客户端启动辅助类 Bootstrap b=new Bootstrap(); b.group(group).channel(NioSocketChannel.class) .option(ChannelOption.TCP_NODELAY, true) .handler(new ChannelInitializer&lt;SocketChannel&gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { //添加POJO对象解码器 禁止缓存类加载器 ch.pipeline().addLast(new ObjectDecoder(1024,ClassResolvers.cacheDisabled(this.getClass().getClassLoader()))); //设置发送消息编码器 ch.pipeline().addLast(new ObjectEncoder()); //设置网络IO处理器 ch.pipeline().addLast(new BookTicketClientHandler()); } }); //发起异步服务器连接请求 同步等待成功 ChannelFuture f=b.connect(host,port).sync(); //等到客户端链路关闭 f.channel().closeFuture().sync(); }finally{ //优雅释放线程资源 group.shutdownGracefully(); } } public static void main(String[] args) throws Exception{ new BookTicketClient().connect(8000, &quot;127.0.0.1&quot;); } } 客户端处理网络IO处理器 发送查票和订票请求：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package bookticket; import io.netty.channel.ChannelHandlerAdapter;import io.netty.channel.ChannelHandlerContext; import java.util.Calendar; /** * 客户端处理器 * * @author xwalker */public class BookTicketClientHandler extends ChannelHandlerAdapter { private User user; public BookTicketClientHandler() { user=new User(); user.setUserName(&quot;xwalker&quot;); user.setPhone(&quot;187667*****&quot;); user.setEmail(&quot;909854136@qq.com&quot;); user.setUserId(&quot;3705231988********&quot;); } /** * 链路链接成功 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { // 链接成功后发送查询某车次余票的请求 Calendar c = Calendar.getInstance(); c.set(Calendar.YEAR, 2015); c.set(Calendar.MONTH, 1); c.set(Calendar.DATE, 2); c.set(Calendar.HOUR, 11); c.set(Calendar.MINUTE, 30); // G242查询余票 BookRequestMsg requestMsg1 = new BookRequestMsg(); requestMsg1.setCode(Code.CODE_SEARCH); requestMsg1.setStartTime(c.getTime()); requestMsg1.setTrainNumber(&quot;G242&quot;);//设置查询车次 requestMsg1.setUser(user);//设置当前登陆用户 ctx.write(requestMsg1); // D1235查询余票 BookRequestMsg requestMsg2 = new BookRequestMsg(); requestMsg2.setCode(Code.CODE_SEARCH); requestMsg2.setStartTime(c.getTime()); requestMsg2.setTrainNumber(&quot;D1235&quot;);//设置查询车次 requestMsg2.setUser(user); ctx.write(requestMsg2); ctx.flush(); } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { BookResponseMsg responseMsg = (BookResponseMsg) msg; switch (responseMsg.getCode()) { case Code.CODE_SEARCH://收到查询结果 System.out.println(&quot;==========火车【&quot;+responseMsg.getTrain().getNumber()+&quot;】余票查询结果:【&quot;+(responseMsg.getSuccess()?&quot;成功&quot;:&quot;失败&quot;)+&quot;】=========&quot;); System.out.println(responseMsg.getMsg()); //查询发现有余票的话 需要发送订票指令 if(responseMsg.getTrain().getTicketCounts()&gt;0){ //构造查询有余票的火车的订票指令 BookRequestMsg requestMsg = new BookRequestMsg(); requestMsg.setCode(Code.CODE_BOOK); requestMsg.setUser(user); requestMsg.setStartTime(responseMsg.getStartTime()); requestMsg.setTrainNumber(responseMsg.getTrain().getNumber()); ctx.writeAndFlush(requestMsg); }else{ System.out.println(&quot;火车【&quot;+responseMsg.getTrain().getNumber()+&quot;】没有余票，不能订票了！&quot;); } break; case Code.CODE_BOOK://收到订票结果 System.out.println(&quot;==========火车【&quot;+responseMsg.getTrain().getNumber()+&quot;】订票结果:【&quot;+(responseMsg.getSuccess()?&quot;成功&quot;:&quot;失败&quot;)+&quot;】=========&quot;); System.out.println(responseMsg.getMsg()); System.out.println(&quot;========车票详情========&quot;); Ticket ticket=responseMsg.getTicket(); System.out.println(&quot;车票票号：【&quot;+ticket.getNumber()+&quot;】&quot;); System.out.println(&quot;火车车次：【&quot;+ticket.getTrainNumber()+&quot;】&quot;); System.out.println(&quot;火车车厢：【&quot;+ticket.getCarriageNumber()+&quot;】&quot;); System.out.println(&quot;车厢座位：【&quot;+ticket.getSeatNumber()+&quot;】&quot;); System.out.println(&quot;预定时间：【&quot;+ticket.getBookTime()+&quot;】&quot;); System.out.println(&quot;出发时间：【&quot;+ticket.getStartTime()+&quot;】&quot;); System.out.println(&quot;乘客信息：【&quot;+ticket.getUser().getUserName()+&quot;】&quot;); break; default: System.out.println(&quot;==========操作错误结果=========&quot;); System.out.println(responseMsg.getMsg()); break; } } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { ctx.flush(); } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); }} 最后测试结果： 文章来源于： https://blog.csdn.net/albertfly/article/details/51527488","link":"/2019/04/03/Netty-%E4%BD%BF%E7%94%A8Netty%E4%BC%A0%E8%BE%93POJO%E5%AF%B9%E8%B1%A1/"},{"title":"Spring 完美配置跨域请求","text":"在SpringBoot2.0 上的跨域 用以下代码配置 即可完美解决你的前后端跨域请求问题 在SpringBoot2.0 上的跨域 用以下代码配置 即可完美解决你的前后端跨域请求问题123456789101112131415161718192021222324252627282930313233import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.cors.CorsConfiguration;import org.springframework.web.cors.UrlBasedCorsConfigurationSource;import org.springframework.web.filter.CorsFilter;/** * 实现基本的跨域请求 * @author linhongcun * */@Configurationpublic class CorsConfig { @Bean public CorsFilter corsFilter() { final UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = new UrlBasedCorsConfigurationSource(); final CorsConfiguration corsConfiguration = new CorsConfiguration(); /*是否允许请求带有验证信息*/ corsConfiguration.setAllowCredentials(true); /*允许访问的客户端域名*/ corsConfiguration.addAllowedOrigin(&quot;*&quot;); /*允许服务端访问的客户端请求头*/ corsConfiguration.addAllowedHeader(&quot;*&quot;); /*允许访问的方法名,GET POST等*/ corsConfiguration.addAllowedMethod(&quot;*&quot;); urlBasedCorsConfigurationSource.registerCorsConfiguration(&quot;/**&quot;, corsConfiguration); return new CorsFilter(urlBasedCorsConfigurationSource); }}","link":"/2019/03/28/Spring%20%E5%AE%8C%E7%BE%8E%E9%85%8D%E7%BD%AE%E8%B7%A8%E5%9F%9F%E8%AF%B7%E6%B1%82/"},{"title":"Spring Bean 生命周期","text":"Spring Bean 生命周期前言Spring Bean 的生命周期在整个 Spring 中占有很重要的位置，掌握这些可以加深对 Spring 的理解。 首先看下生命周期图： 再谈生命周期之前有一点需要先明确： Spring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。 注解方式在 bean 初始化时会经历几个阶段，首先可以使用注解 @PostConstruct, @PreDestroy 来在 bean 的创建和销毁阶段进行调用: 1234567891011121314@Componentpublic class AnnotationBean { private final static Logger LOGGER = LoggerFactory.getLogger(AnnotationBean.class); @PostConstruct public void start(){ LOGGER.info(&quot;AnnotationBean start&quot;); } @PreDestroy public void destroy(){ LOGGER.info(&quot;AnnotationBean destroy&quot;); }} InitializingBean, DisposableBean 接口还可以实现 InitializingBean,DisposableBean 这两个接口，也是在初始化以及销毁阶段调用： 12345678910111213@Servicepublic class SpringLifeCycleService implements InitializingBean,DisposableBean{ private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleService.class); @Override public void afterPropertiesSet() throws Exception { LOGGER.info(&quot;SpringLifeCycleService start&quot;); } @Override public void destroy() throws Exception { LOGGER.info(&quot;SpringLifeCycleService destroy&quot;); }} 自定义初始化和销毁方法也可以自定义方法用于在初始化、销毁阶段调用: 123456789101112131415161718192021222324@Configurationpublic class LifeCycleConfig { @Bean(initMethod = &quot;start&quot;, destroyMethod = &quot;destroy&quot;) public SpringLifeCycle create(){ SpringLifeCycle springLifeCycle = new SpringLifeCycle() ; return springLifeCycle ; }}public class SpringLifeCycle{ private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycle.class); public void start(){ LOGGER.info(&quot;SpringLifeCycle start&quot;); } public void destroy(){ LOGGER.info(&quot;SpringLifeCycle destroy&quot;); }} 以上是在 SpringBoot 中可以这样配置，如果是原始的基于 XML 也是可以使用: 12&lt;bean class=&quot;com.crossoverjie.spring.SpringLifeCycle&quot; init-method=&quot;start&quot; destroy-method=&quot;destroy&quot;&gt;&lt;/bean&gt; 来达到同样的效果。 实现 *Aware 接口*Aware 接口可以用于在初始化 bean 时获得 Spring 中的一些对象，如获取 Spring 上下文等。 123456789101112@Componentpublic class SpringLifeCycleAware implements ApplicationContextAware { private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleAware.class); private ApplicationContext applicationContext ; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.applicationContext = applicationContext ; LOGGER.info(&quot;SpringLifeCycleAware start&quot;); }} 这样在 springLifeCycleAware 这个 bean 初始化会就会调用 setApplicationContext 方法，并可以获得 applicationContext 对象。 BeanPostProcessor 增强处理器实现 BeanPostProcessor 接口，Spring 中所有 bean 在做初始化时都会调用该接口中的两个方法，可以用于对一些特殊的 bean 进行处理： 12345678910111213141516171819202122232425262728293031323334@Componentpublic class SpringLifeCycleProcessor implements BeanPostProcessor { private final static Logger LOGGER = LoggerFactory.getLogger(SpringLifeCycleProcessor.class); /** * 预初始化 初始化之前调用 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { if (&quot;annotationBean&quot;.equals(beanName)){ LOGGER.info(&quot;SpringLifeCycleProcessor start beanName={}&quot;,beanName); } return bean; } /** * 后初始化 bean 初始化完成调用 * @param bean * @param beanName * @return * @throws BeansException */ @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (&quot;annotationBean&quot;.equals(beanName)){ LOGGER.info(&quot;SpringLifeCycleProcessor end beanName={}&quot;,beanName); } return bean; }} 执行之后观察结果： 123456789101112131415018-03-21 00:40:24.856 [restartedMain] INFO c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor start beanName=annotationBean2018-03-21 00:40:24.860 [restartedMain] INFO c.c.spring.annotation.AnnotationBean - AnnotationBean start2018-03-21 00:40:24.861 [restartedMain] INFO c.c.s.p.SpringLifeCycleProcessor - SpringLifeCycleProcessor end beanName=annotationBean2018-03-21 00:40:24.864 [restartedMain] INFO c.c.s.aware.SpringLifeCycleAware - SpringLifeCycleAware start2018-03-21 00:40:24.867 [restartedMain] INFO c.c.s.service.SpringLifeCycleService - SpringLifeCycleService start2018-03-21 00:40:24.887 [restartedMain] INFO c.c.spring.SpringLifeCycle - SpringLifeCycle start2018-03-21 00:40:25.062 [restartedMain] INFO o.s.b.d.a.OptionalLiveReloadServer - LiveReload server is running on port 357292018-03-21 00:40:25.122 [restartedMain] INFO o.s.j.e.a.AnnotationMBeanExporter - Registering beans for JMX exposure on startup2018-03-21 00:40:25.140 [restartedMain] INFO com.crossoverjie.Application - Started Application in 2.309 seconds (JVM running for 3.681)2018-03-21 00:40:25.143 [restartedMain] INFO com.crossoverjie.Application - start ok!2018-03-21 00:40:25.153 [Thread-8] INFO o.s.c.a.AnnotationConfigApplicationContext - Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@3913adad: startup date [Wed Mar 21 00:40:23 CST 2018]; root of context hierarchy2018-03-21 00:40:25.155 [Thread-8] INFO o.s.j.e.a.AnnotationMBeanExporter - Unregistering JMX-exposed beans on shutdown2018-03-21 00:40:25.156 [Thread-8] INFO c.c.spring.SpringLifeCycle - SpringLifeCycle destroy2018-03-21 00:40:25.156 [Thread-8] INFO c.c.s.service.SpringLifeCycleService - SpringLifeCycleService destroy2018-03-21 00:40:25.156 [Thread-8] INFO c.c.spring.annotation.AnnotationBean - AnnotationBean destroy 直到 Spring 上下文销毁时则会调用自定义的销毁方法以及实现了 DisposableBean 的 destroy() 方法。","link":"/2019/05/15/Spring-Bean-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/"},{"title":"Spring Boot使用@Async实现异步调用：自定义线程池","text":"Spring Boot使用@Async实现异步调用：自定义线程池 如果通过自定义线程池的方式来控制异步调用的并发。 定义线程池第一步，先在Spring Boot主类中定义一个线程池，比如： 123456789101112131415161718192021222324252627282930313233@SpringBootApplicationpublic class Application { public static void main(String[] args) { SpringApplication.run(Application.class, args); } @EnableAsync @Configuration class TaskPoolConfig { @Bean(&quot;asyncExecutor&quot;) public Executor taskExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); //设置核心线程数 executor.setCorePoolSize(10); //设置最大线程数 executor.setMaxPoolSize(20); //线程池所使用的缓冲队列 executor.setQueueCapacity(200); executor.setKeepAliveSeconds(60); // 线程名称前缀 executor.setThreadNamePrefix(&quot;asyncExecutor-&quot;); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); //用来设置线程池关闭的时候等待所有任务都完成再继续销毁其他的Bean，这样这些异步任务的销毁就会先于Redis线程池的销毁。 //等待任务在关机时完成--表明等待所有线程执行完 executor.setWaitForTasksToCompleteOnShutdown(true); //该方法用来设置线程池中任务的等待时间，如果超过这个时候还没有销毁就强制销毁，以确保应用最后能够被关闭，而不是阻塞住。 // 等待时间 （默认为0，此时立即停止），并没等待xx秒后强制停止 executor.setAwaitTerminationSeconds(180); // 初始化线程 executor.initialize(); return executor; }} 上面我们通过使用ThreadPoolTaskExecutor创建了一个线程池，同时设置了以下这些参数： 核心线程数10：线程池创建时候初始化的线程数 最大线程数20：线程池最大的线程数，只有在缓冲队列满了之后才会申请超过核心线程数的线程 缓冲队列200：用来缓冲执行任务的队列 允许线程的空闲时间60秒：当超过了核心线程出之外的线程在空闲时间到达之后会被销毁 线程池名的前缀：设置好了之后可以方便我们定位处理任务所在的线程池 线程池对拒绝任务的处理策略：这里采用了CallerRunsPolicy策略，当线程池没有处理能力的时候，该策略会直接在 execute 方法的调用线程中运行被拒绝的任务；如果执行程序已关闭，则会丢弃该任务 使用线程池在定义了线程池之后，我们如何让异步调用的执行任务使用这个线程池中的资源来运行呢？方法非常简单，我们只需要在@Async注解中指定线程池名即可，比如： 12345678910111213141516171819202122232425262728293031323334@Slf4j@Componentpublic class Task { public static Random random = new Random(); @Async(&quot;taskExecutor&quot;) public void doTaskOne() throws Exception { log.info(&quot;开始做任务一&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); log.info(&quot;完成任务一，耗时：&quot; + (end - start) + &quot;毫秒&quot;); } @Async(&quot;taskExecutor&quot;) public void doTaskTwo() throws Exception { log.info(&quot;开始做任务二&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); log.info(&quot;完成任务二，耗时：&quot; + (end - start) + &quot;毫秒&quot;); } @Async(&quot;taskExecutor&quot;) public void doTaskThree() throws Exception { log.info(&quot;开始做任务三&quot;); long start = System.currentTimeMillis(); Thread.sleep(random.nextInt(10000)); long end = System.currentTimeMillis(); log.info(&quot;完成任务三，耗时：&quot; + (end - start) + &quot;毫秒&quot;); }} 单元测试最后，我们来写个单元测试来验证一下 1234567891011121314151617@RunWith(SpringJUnit4ClassRunner.class)@SpringBootTestpublic class ApplicationTests { @Autowired private Task task; @Test public void test() throws Exception { task.doTaskOne(); task.doTaskTwo(); task.doTaskThree(); Thread.currentThread().join(); }} 执行上面的单元测试，我们可以在控制台中看到所有输出的线程名前都是之前我们定义的线程池前缀名开始的，说明我们使用线程池来执行异步任务的试验成功了！ 1234562018-03-27 22:01:15.620 INFO 73703 --- [ taskExecutor-1] com.didispace.async.Task : 开始做任务一2018-03-27 22:01:15.620 INFO 73703 --- [ taskExecutor-2] com.didispace.async.Task : 开始做任务二2018-03-27 22:01:15.620 INFO 73703 --- [ taskExecutor-3] com.didispace.async.Task : 开始做任务三2018-03-27 22:01:18.165 INFO 73703 --- [ taskExecutor-2] com.didispace.async.Task : 完成任务二，耗时：2545毫秒2018-03-27 22:01:22.149 INFO 73703 --- [ taskExecutor-3] com.didispace.async.Task : 完成任务三，耗时：6529毫秒2018-03-27 22:01:23.912 INFO 73703 --- [ taskExecutor-1] com.didispace.async.Task : 完成任务一，耗时：8292毫秒 完整示例：读者可以根据喜好选择下面的两个仓库中查看Chapter4-1-3项目： Github：https://github.com/dyc87112/SpringBoot-Learning/ Gitee：https://gitee.com/didispace/SpringBoot-Learning/","link":"/2019/08/21/Spring-Boot%E4%BD%BF%E7%94%A8-Async%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%AD%A5%E8%B0%83%E7%94%A8%EF%BC%9A%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BA%BF%E7%A8%8B%E6%B1%A0-1/"},{"title":"SpringBoot Jar包瘦身 - 跟大文件说再见！","text":"前言SpringBoot部署起来配置非常少，如果服务器部署在公司内网，上传速度还行，但是如果部署在公网（阿里云等云服务器上），部署起来实在头疼、就是 编译出来的 Jar 包很大，如果工程引入了许多开源组件（SpringCloud等），那就更大了。这个时候如果想要对线上运行工程有一些微调，则非常痛苦 可以用以下方法减少jar内容瘦身准备1、首先我们要对Jar包有一个初步认识，它的内部结构如下1234567891011121314151617example.jar | +-META-INF | +-MANIFEST.MF +-org | +-springframework | +-boot | +-loader | +-&lt;spring boot loader classes&gt; +-BOOT-INF +-classes | +-mycompany | +-project | +-YourClasses.class +-lib // 依赖库的包 +-dependency1.jar +-dependency2.jar 运行该Jar时默认从BOOT-INF/classes加载class，从BOOT-INF/lib加载所依赖的Jar包。如果想要加入外部的依赖Jar，可以通过设置环境变量LOADER_PATH来实现。 如此一来，就可以确认我们的思路了： 把那些不变的依赖Jar包（比如spring依赖、数据库Driver等，这些在不升级版本的情况下是不会更新的）从Flat Jar中抽离到单独的目录，如libs 在启动Jar时，设置LOADER_PATH使用上一步的libs 1java -Dloader.path=&quot;libs/&quot; -jar ht-ui-web.jar 这样，我们最终打包的jar包体积就大大减少，每次迭代后只需要更新这个精简版的Jar即可。 需要在pom文件配置忽略的依赖包。 关键需要配置MANIFEST.MF 文件中加入lib路径。 然后正常启动jar包就可以了。 123456789101112131415161718192021222324252627&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!--不打包资源文件--&gt; &lt;!--&lt;excludes&gt;--&gt; &lt;!--&lt;exclude&gt;*.**&lt;/exclude&gt;--&gt; &lt;!--&lt;exclude&gt;*/*.xml&lt;/exclude&gt;--&gt; &lt;!--&lt;/excludes&gt;--&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;!--MANIFEST.MF 中 Class-Path 加入前缀--&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;!--jar包不包含唯一版本标识--&gt; &lt;useUniqueVersions&gt;false&lt;/useUniqueVersions&gt; &lt;!--指定入口类--&gt; &lt;mainClass&gt;com.XProApplication&lt;/mainClass&gt; &lt;/manifest&gt; &lt;!--&lt;manifestEntries&gt;--&gt; &lt;!--&lt;!&amp;ndash;MANIFEST.MF 中 Class-Path 加入资源文件目录&amp;ndash;&gt;--&gt; &lt;!--&lt;Class-Path&gt;./resources/&lt;/Class-Path&gt;--&gt; &lt;!--&lt;/manifestEntries&gt;--&gt; &lt;/archive&gt; &lt;outputDirectory&gt;${project.build.directory}&lt;/outputDirectory&gt; &lt;/configuration&gt;&lt;/plugin&gt; 完整pom文件的内容如下。。配置完毕打包项目就会将lib包和项目包分开放到target中。然后分开上传内容。 以后就可以上传精简的jar包了```` org.apache.maven.plugins maven-compiler-plugin 3.1 org.apache.maven.plugins maven-jar-plugin true lib/ false com.XProApplication ${project.build.directory} &lt;!--拷贝依赖 copy-dependencies--&gt; &lt;!--也可以执行mvn copy-dependencies 命令打包依赖--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-dependencies&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt; ${project.build.directory}/lib/ &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--spring boot repackage，依赖 maven-jar-plugin 打包的jar包 重新打包成 spring boot 的jar包--&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!--重写包含依赖，包含不存在的依赖，jar里没有pom里的依赖--&gt; &lt;includes&gt; &lt;include&gt; &lt;groupId&gt;null&lt;/groupId&gt; &lt;artifactId&gt;null&lt;/artifactId&gt; &lt;/include&gt; &lt;/includes&gt; &lt;layout&gt;ZIP&lt;/layout&gt; &lt;!--使用外部配置文件，jar包里没有资源文件--&gt; &lt;addResources&gt;true&lt;/addResources&gt; &lt;outputDirectory&gt;${project.build.directory}&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!--配置jar包特殊标识 配置后，保留原文件，生成新文件 *-run.jar --&gt; &lt;!--配置jar包特殊标识 不配置，原文件命名为 *.jar.original，生成新文件 *.jar --&gt; &lt;!--&lt;classifier&gt;run&lt;/classifier&gt;--&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/build&gt; ```","link":"/2019/07/18/SpringBoot-Jar%E5%8C%85%E7%98%A6%E8%BA%AB-%E8%B7%9F%E5%A4%A7%E6%96%87%E4%BB%B6%E8%AF%B4%E5%86%8D%E8%A7%81%EF%BC%81/"},{"title":"SpringBoot中Shiro缓存使用Redis、Ehcache","text":"在SpringBoot中Shiro缓存使用Redis、Ehcache实现的两种方式实例SpringBoot 中配置redis作为session 缓存器。 让shiro引用 本文是建立在你是使用这shiro基础之上的补充内容 第一种：Redis缓存，将数据存储到redis 并且开启session存入redis中。引入pom12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 配置redisConfig12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configuration@EnableCachingpublic class RedisConfig extends CachingConfigurerSupport { @Bean public KeyGenerator keyGenerator() { return new KeyGenerator() { @Override public Object generate(Object target, Method method, Object... params) { StringBuilder sb = new StringBuilder(); sb.append(target.getClass().getName()); sb.append(method.getName()); for (Object obj : params) { sb.append(obj.toString()); } return sb.toString(); } }; } @Bean //在这里配置缓存reids配置 public RedisCacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) { RedisCacheConfiguration redisCacheConfiguration = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofHours(1)); // 设置缓存有效期一小时 System.out.println(&quot;《========【开启redis】 ======== 》 &quot;); return RedisCacheManager .builder(RedisCacheWriter.nonLockingRedisCacheWriter(redisConnectionFactory)) .cacheDefaults(redisCacheConfiguration).build(); } @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory factory) { StringRedisTemplate template = new StringRedisTemplate(factory); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); template.setValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; }} 配置自定义缓存管理器，引入redis缓存管理器 定义自己的CacheManager 123456789101112131415161718192021222324252627282930313233343536373839/** * &lt;p&gt; 自定义cacheManage 扩张shiro里面的缓存 使用reids作缓存 &lt;/p&gt; * &lt;description&gt; * 引入自己定义的CacheManager * 关于CacheManager的配置文件在spring-redis-cache.xml中 * &lt;/description&gt; */@Componentpublic class ShiroSpringCacheManager implements CacheManager ,Destroyable{ /** * 将之上的RedisCacheManager的Bean拿出来 注入于此 */ @Autowired private org.springframework.cache.CacheManager cacheManager; public org.springframework.cache.CacheManager getCacheManager() { return cacheManager; } public void setCacheManager(org.springframework.cache.CacheManager cacheManager) { this.cacheManager = cacheManager; } @Override public void destroy() throws Exception { cacheManager = null; } @Override public &lt;K, V&gt; Cache&lt;K, V&gt; getCache(String name) { if (name == null ){ return null; } // 新建一个ShiroSpringCache 将Bean放入并实例化 return new ShiroSpringCache&lt;K,V&gt;(name,getCacheManager()); }} 定义自己实现的Shiro的Cache,实现了Shiro包里的Cache 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/** * &lt;p&gt; 自定义缓存 将数据存入到redis中 &lt;/p&gt; */@SuppressWarnings(&quot;unchecked&quot;)public class ShiroSpringCache&lt;K,V&gt; implements org.apache.shiro.cache.Cache&lt;K, V&gt;{ private static final Logger log = LoggerFactory.getLogger(ShiroSpringCache.class); private CacheManager cacheManager; private Cache cache; public ShiroSpringCache(String name, CacheManager cacheManager) { if(name==null || cacheManager==null){ throw new IllegalArgumentException(&quot;cacheManager or CacheName cannot be null.&quot;); } this.cacheManager = cacheManager; //这里首先是从父类中获取这个cache,如果没有会创建一个redisCache,初始化这个redisCache的时候 //会设置它的过期时间如果没有配置过这个缓存的，那么默认的缓存时间是为0的，如果配置了，就会把配置的时间赋予给这个RedisCache //如果从缓存的过期时间为0，就表示这个RedisCache不存在了，这个redisCache实现了spring中的cache this.cache= cacheManager.getCache(name); } @Override public V get(K key) throws CacheException { log.info(&quot;从缓存中获取key为{}的缓存信息&quot;,key); if(key == null){ return null; } ValueWrapper valueWrapper = cache.get(key); if(valueWrapper==null){ return null; } return (V) valueWrapper.get(); } @Override public V put(K key, V value) throws CacheException { log.info(&quot;创建新的缓存，信息为：{}={}&quot;,key,value); cache.put(key, value); return get(key); } @Override public V remove(K key) throws CacheException { log.info(&quot;干掉key为{}的缓存&quot;,key); V v = get(key); cache.evict(key);//干掉这个名字为key的缓存 return v; } @Override public void clear() throws CacheException { log.info(&quot;清空所有的缓存&quot;); cache.clear(); } @Override public int size() { return cacheManager.getCacheNames().size(); } /** * 获取缓存中所的key值 */ @Override public Set&lt;K&gt; keys() { return (Set&lt;K&gt;) cacheManager.getCacheNames(); } /** * 获取缓存中所有的values值 */ @Override public Collection&lt;V&gt; values() { return (Collection&lt;V&gt;) cache.get(cacheManager.getCacheNames()).get(); } @Override public String toString() { return &quot;ShiroSpringCache [cache=&quot; + cache + &quot;]&quot;; }} 到此为止，使用redis做缓存，和spring的集成就完成了。 可以使用以下注解将缓存放入redis 1@Cacheable(value = Cache.CONSTANT, key = &quot;'&quot; + CacheKey.DICT_NAME + &quot;'+#name+'_'+#val&quot;) 配置spring session管理器12345@Bean@ConditionalOnProperty(prefix = &quot;xpro&quot;, name = &quot;spring-session-open&quot;, havingValue = &quot;true&quot;)public ServletContainerSessionManager servletContainerSessionManager() { return new ServletContainerSessionManager();} 新建类 spring session设置session过期时间123456789101112/** * spring session配置 * * @author xingri * @date 2017-07-13 21:05 */@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 900) //session过期时间 如果部署多机环境,需要打开注释@ConditionalOnProperty(prefix = &quot;xpro&quot;, name = &quot;spring-session-open&quot;, havingValue = &quot;true&quot;)public class SpringSessionConfig {} 第一种：Ehcache做缓存，可以将数据存储到磁盘中，也可以存到内存中 新建ehcache.xml 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;ehcache updateCheck=&quot;false&quot; dynamicConfig=&quot;false&quot;&gt; &lt;diskStore path=&quot;java.io.tmpdir&quot;/&gt; &lt;!--授权信息缓存--&gt; &lt;cache name=&quot;authorizationCache&quot; maxEntriesLocalHeap=&quot;2000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;1800&quot; timeToLiveSeconds=&quot;1800&quot; overflowToDisk=&quot;false&quot; statistics=&quot;true&quot;&gt; &lt;/cache&gt;&lt;!--身份信息缓存--&gt; &lt;cache name=&quot;authenticationCache&quot; maxEntriesLocalHeap=&quot;2000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;1800&quot; timeToLiveSeconds=&quot;1800&quot; overflowToDisk=&quot;false&quot; statistics=&quot;true&quot;&gt; &lt;/cache&gt;&lt;!--session缓存--&gt; &lt;cache name=&quot;activeSessionCache&quot; maxEntriesLocalHeap=&quot;2000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;1800&quot; timeToLiveSeconds=&quot;1800&quot; overflowToDisk=&quot;false&quot; statistics=&quot;true&quot;&gt; &lt;/cache&gt; &lt;!-- 缓存半小时 --&gt; &lt;cache name=&quot;halfHour&quot; maxElementsInMemory=&quot;10000&quot; maxElementsOnDisk=&quot;100000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;1800&quot; timeToLiveSeconds=&quot;1800&quot; overflowToDisk=&quot;false&quot; diskPersistent=&quot;false&quot; /&gt; &lt;!-- 缓存一小时 --&gt; &lt;cache name=&quot;hour&quot; maxElementsInMemory=&quot;10000&quot; maxElementsOnDisk=&quot;100000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;3600&quot; timeToLiveSeconds=&quot;3600&quot; overflowToDisk=&quot;false&quot; diskPersistent=&quot;false&quot; /&gt; &lt;!-- 缓存一天 --&gt; &lt;cache name=&quot;oneDay&quot; maxElementsInMemory=&quot;10000&quot; maxElementsOnDisk=&quot;100000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;86400&quot; timeToLiveSeconds=&quot;86400&quot; overflowToDisk=&quot;false&quot; diskPersistent=&quot;false&quot; /&gt; &lt;!-- name:缓存名称。 maxElementsInMemory：缓存最大个数。 eternal:对象是否永久有效，一但设置了，timeout将不起作用。 timeToIdleSeconds：设置对象在失效前的允许闲置时间（单位：秒）。仅当eternal=false对象不是永久有效时使用，可选属性，默认值是0，也就是可闲置时间无穷大。 timeToLiveSeconds：设置对象在失效前允许存活时间（单位：秒）。最大时间介于创建时间和失效时间之间。仅当eternal=false对象不是永久有效时使用，默认是0.，也就是对象存活时间无穷大。 overflowToDisk：当内存中对象数量达到maxElementsInMemory时，Ehcache将会对象写到磁盘中。 diskSpoolBufferSizeMB：这个参数设置DiskStore（磁盘缓存）的缓存区大小。默认是30MB。每个Cache都应该有自己的一个缓冲区。 maxElementsOnDisk：硬盘最大缓存个数。 diskPersistent：是否缓存虚拟机重启期数据 Whether the disk store persists between restarts of the Virtual Machine. The default value is false. diskExpiryThreadIntervalSeconds：磁盘失效线程运行时间间隔，默认是120秒。 memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。默认策略是LRU（最近最少使用）。你可以设置为FIFO（先进先出）或是LFU（较少使用）。 clearOnFlush：内存数量最大时是否清除。 --&gt; &lt;defaultCache name=&quot;defaultCache&quot; maxElementsInMemory=&quot;10000&quot; eternal=&quot;false&quot; timeToIdleSeconds=&quot;600&quot; timeToLiveSeconds=&quot;600&quot; overflowToDisk=&quot;false&quot; maxElementsOnDisk=&quot;100000&quot; diskPersistent=&quot;false&quot; diskExpiryThreadIntervalSeconds=&quot;120&quot; memoryStoreEvictionPolicy=&quot;LRU&quot;/&gt;&lt;/ehcache&gt; 配置自定义缓存管理器，引入ehcache缓存管理器1234567891011121314151617181920212223242526272829303132333435363738/** * ehcache配置 * */@Configuration@EnableCachingpublic class EhCacheConfig { /** * EhCache的配置 */ @Bean public EhCacheCacheManager cacheManager(CacheManager cacheManager) { MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer(); ManagementService.registerMBeans(cacheManager, mBeanServer, true, true, true, true); return new EhCacheCacheManager(cacheManager); } /** * EhCache的配置 */ @Bean public EhCacheManagerFactoryBean ehcache() { System.out.println(&quot;《========【开启ehcache】 ======== 》 &quot;); EhCacheManagerFactoryBean ehCacheManagerFactoryBean = new EhCacheManagerFactoryBean(); ehCacheManagerFactoryBean.setConfigLocation(new ClassPathResource(&quot;ehcache.xml&quot;)); return ehCacheManagerFactoryBean; } @Bean public org.apache.shiro.cache.CacheManager getCacheShiroManager(EhCacheManagerFactoryBean ehcache) { EhCacheManager ehCacheManager = new EhCacheManager(); ehCacheManager.setCacheManager(ehcache.getObject()); return ehCacheManager; }} 最后 最重要的是引入shriro 中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155/** * shiro权限管理的配置 * */@Configurationpublic class ShiroConfig { /** * 安全管理器 */ @Bean public DefaultWebSecurityManager securityManager(CookieRememberMeManager rememberMeManager, CacheManager cacheShiroManager, SessionManager sessionManager) { DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(); securityManager.setAuthenticator(modularRealmAuthenticator()); List&lt;Realm&gt; realms=new ArrayList&lt;&gt;(); securityManager.setRealms(realms); securityManager.setCacheManager(cacheShiroManager); securityManager.setRememberMeManager(rememberMeManager); securityManager.setSessionManager(sessionManager); return securityManager; } /** * spring session管理器（多机环境） */ @Bean public ServletContainerSessionManager servletContainerSessionManager() { return new ServletContainerSessionManager(); } /** * session管理器(单机环境) 使用cookie存储缓存。。如果多级请注释 */ @Bean public DefaultWebSessionManager defaultWebSessionManager(CacheManager cacheShiroManager, XProProperties xProProperties) { DefaultWebSessionManager sessionManager = new DefaultWebSessionManager(); sessionManager.setCacheManager(cacheShiroManager); sessionManager.setSessionValidationInterval(xProProperties.getSessionValidationInterval() * 1000); sessionManager.setGlobalSessionTimeout(xProProperties.getSessionInvalidateTime() * 1000); sessionManager.setDeleteInvalidSessions(true); sessionManager.setSessionValidationSchedulerEnabled(true); Cookie cookie = new SimpleCookie(ShiroHttpSession.DEFAULT_SESSION_ID_NAME); cookie.setName(&quot;shiroCookie&quot;); cookie.setHttpOnly(true); sessionManager.setSessionIdCookie(cookie); return sessionManager; } /** * 缓存管理器 使用Ehcache实现 如果使用redis则注释下面内容！！！！ */ @Bean public CacheManager getCacheShiroManager(EhCacheManagerFactoryBean ehcache) { EhCacheManager ehCacheManager = new EhCacheManager(); ehCacheManager.setCacheManager(ehcache.getObject()); return ehCacheManager; } /** * 项目自定义的Realm */ @Bean public ShiroDbRealm shiroDbRealm() { return new ShiroDbRealm(); } @Bean public ShiroTockenRealm shiroTockenRealm( ) { return new ShiroTockenRealm(); } @Bean public ShiroJwtRealm shiroJwtRealm( ) { return new ShiroJwtRealm(); } /** * 系统自带的Realm管理，主要针对多realm * */ @Bean public ModularRealmAuthenticator modularRealmAuthenticator(){ ModularRealmAuthenticator modularRealmAuthenticator=new ModularRealmAuthenticator(); modularRealmAuthenticator.setAuthenticationStrategy(new AtLeastOneSuccessfulStrategy()); return modularRealmAuthenticator; } /** * rememberMe管理器, cipherKey生成见{@code Base64Test.java} */ @Bean public CookieRememberMeManager rememberMeManager(SimpleCookie rememberMeCookie) { CookieRememberMeManager manager = new CookieRememberMeManager(); manager.setCipherKey(Base64.decode(&quot;Z3VucwAAAAAAAAAAAAAAAA==&quot;)); manager.setCookie(rememberMeCookie); return manager; } /** * 记住密码Cookie */ @Bean public SimpleCookie rememberMeCookie() { SimpleCookie simpleCookie = new SimpleCookie(&quot;rememberMe&quot;); simpleCookie.setHttpOnly(true); simpleCookie.setMaxAge(7 * 24 * 60 * 60);//7天 return simpleCookie; } /** * 在方法中 注入 securityManager,进行代理控制 */ @Bean public MethodInvokingFactoryBean methodInvokingFactoryBean(DefaultWebSecurityManager securityManager) { MethodInvokingFactoryBean bean = new MethodInvokingFactoryBean(); bean.setStaticMethod(&quot;org.apache.shiro.SecurityUtils.setSecurityManager&quot;); bean.setArguments(new Object[]{securityManager}); return bean; } /** * 保证实现了Shiro内部lifecycle函数的bean执行 */ @Bean public LifecycleBeanPostProcessor lifecycleBeanPostProcessor() { return new LifecycleBeanPostProcessor(); } /** * 启用shrio授权注解拦截方式，AOP式方法级权限检查 */ @Bean @DependsOn(value = &quot;lifecycleBeanPostProcessor&quot;) //依赖其他bean的初始化 public DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() { return new DefaultAdvisorAutoProxyCreator(); } @Bean public AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor(DefaultWebSecurityManager securityManager) { AuthorizationAttributeSourceAdvisor authorizationAttributeSourceAdvisor = new AuthorizationAttributeSourceAdvisor(); authorizationAttributeSourceAdvisor.setSecurityManager(securityManager); return authorizationAttributeSourceAdvisor; }}","link":"/2019/08/15/SpringBoot%E4%B8%ADShiro%E7%BC%93%E5%AD%98%E4%BD%BF%E7%94%A8Redis%E3%80%81Ehcache/"},{"title":"SpringBoot是如何动起来的","text":"程序入口 SpringBoot是如何动起来的程序入口1SpringApplication.run(BeautyApplication.class, args); 执行此方法来加载整个SpringBoot的环境。1. 从哪儿开始？ SpringApplication.java123456789 /** * Run the Spring application, creating and refreshing a new * {@link ApplicationContext}. * @param args the application arguments (usually passed from a Java main method) * @return a running {@link ApplicationContext} */public ConfigurableApplicationContext run(String... args) { //... } 调用SpringApplication.java 中的 run 方法，目的是加载Spring Application，同时返回 ApplicationContext。 2. 执行了什么？2.1 计时记录整个Spring Application的加载时间！12345678StopWatch stopWatch = new StopWatch();stopWatch.start();// ...stopWatch.stop();if (this.logStartupInfo) { new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch);} 2.2 声明指定 java.awt.headless，默认是true 一般是在程序开始激活headless模式，告诉程序，现在你要工作在Headless mode下，就不要指望硬件帮忙了，你得自力更生，依靠系统的计算能力模拟出这些特性来。1234private void configureHeadlessProperty() { System.setProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, System.getProperty( SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, Boolean.toString(this.headless)));} 2.4 配置监听并发布应用启动事件SpringApplicationRunListener 负责加载 ApplicationListener事件。123456789101112131415SpringApplicationRunListeners listeners = getRunListeners(args);// 开始listeners.starting();// 处理所有 property sources 配置和 profiles 配置，准备环境，分为标准 Servlet 环境和标准环境ConfigurableEnvironment environment = prepareEnvironment(listeners,applicationArguments);// 准备应用上下文prepareContext(context, environment, listeners, applicationArguments,printedBanner);// 完成listeners.started(context);// 异常handleRunFailure(context, ex, exceptionReporters, listeners);// 执行listeners.running(context); getRunListeners 中根据 type = SpringApplicationRunListener.class 去拿到了所有的 Listener 并根据优先级排序。对应的就是 META-INF/spring.factories 文件中的 org.springframework.boot.SpringApplicationRunListener=org.springframework.boot.context.event.EventPublishingRunListener 123456789101112private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) { ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); return instances; }复制代码 在 ApplicationListener 中 , 可以针对任何一个阶段插入处理代码。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public interface SpringApplicationRunListener { /** * Called immediately when the run method has first started. Can be used for very * early initialization. */ void starting(); /** * Called once the environment has been prepared, but before the * {@link ApplicationContext} has been created. * @param environment the environment */ void environmentPrepared(ConfigurableEnvironment environment); /** * Called once the {@link ApplicationContext} has been created and prepared, but * before sources have been loaded. * @param context the application context */ void contextPrepared(ConfigurableApplicationContext context); /** * Called once the application context has been loaded but before it has been * refreshed. * @param context the application context */ void contextLoaded(ConfigurableApplicationContext context); /** * The context has been refreshed and the application has started but * {@link CommandLineRunner CommandLineRunners} and {@link ApplicationRunner * ApplicationRunners} have not been called. * @param context the application context. * @since 2.0.0 */ void started(ConfigurableApplicationContext context); /** * Called immediately before the run method finishes, when the application context has * been refreshed and all {@link CommandLineRunner CommandLineRunners} and * {@link ApplicationRunner ApplicationRunners} have been called. * @param context the application context. * @since 2.0.0 */ void running(ConfigurableApplicationContext context); /** * Called when a failure occurs when running the application. * @param context the application context or {@code null} if a failure occurred before * the context was created * @param exception the failure * @since 2.0.0 */ void failed(ConfigurableApplicationContext context, Throwable exception);} 3. 每个阶段执行的内容3.1 listeners.starting();在加载Spring Application之前执行，所有资源和环境未被加载。3.2 prepareEnvironment(listeners, applicationArguments);创建 ConfigurableEnvironment； 将配置的环境绑定到Spring Application中；123456789101112131415private ConfigurableEnvironment prepareEnvironment( SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) { // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); listeners.environmentPrepared(environment); bindToSpringApplication(environment); if (this.webApplicationType == WebApplicationType.NONE) { environment = new EnvironmentConverter(getClassLoader()) .convertToStandardEnvironmentIfNecessary(environment); } ConfigurationPropertySources.attach(environment); return environment; } 3.3 prepareContext配置忽略的Bean；123456789private void configureIgnoreBeanInfo(ConfigurableEnvironment environment) { if (System.getProperty( CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME) == null) { Boolean ignore = environment.getProperty(&quot;spring.beaninfo.ignore&quot;, Boolean.class, Boolean.TRUE); System.setProperty(CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME, ignore.toString()); } } 打印日志-加载的资源1Banner printedBanner = printBanner(environment); 根据不同的WebApplicationType创建Context1context = createApplicationContext(); 3.4 refreshContext支持定制刷新123456789/** * Register a shutdown hook with the JVM runtime, closing this context * on JVM shutdown unless it has already been closed at that time. * &lt;p&gt;This method can be called multiple times. Only one shutdown hook * (at max) will be registered for each context instance. * @see java.lang.Runtime#addShutdownHook * @see #close() */ void registerShutdownHook(); 3.5 afterRefresh刷新后的实现方法暂未实现12345678/** * Called after the context has been refreshed. * @param context the application context * @param args the application arguments */ protected void afterRefresh(ConfigurableApplicationContext context, ApplicationArguments args) { } 3.6 listeners.started(context);到此为止， Spring Application的环境和资源都加载完毕了； 发布应用上下文启动完成事件； 执行所有 Runner 运行器 - 执行所有 ApplicationRunner 和 CommandLineRunner 这两种运行器12// 启动callRunners(context, applicationArguments); 3.7 listeners.running(context);触发所有 SpringApplicationRunListener 监听器的 running 事件方法","link":"/2019/03/28/SpringBoot%E6%98%AF%E5%A6%82%E4%BD%95%E5%8A%A8%E8%B5%B7%E6%9D%A5%E7%9A%84/"},{"title":"SpringBoot是怎么在实例化时候将bean加载进入容器中","text":"之前写过的很多spring文章，都是基于应用方面的，这次的话，就带大家来一次对spring的源码追踪，看一看spring到底是怎么进行的初始化，如何创建的bean，相信很多刚刚接触spring的朋友，或者没什么时间的朋友都很想知道spring到底是如何工作的。 首先，按照博主一贯的作风，当然是使用最新的spring版本，这次就使用spring4.2.5…其次，也是为了方便，采用spring-boot-1.3.3进行追踪，和spring 4.2.5是相同的。 不用担心框架不同，大家如果是使用的xml方式进行配置的话，可以去你的ContextListener里面进行追踪，spring-boot只是对 spring所有框架进行了一个集成，如果实在进行不了前面几个步骤的话，可以从文章第6步的AbstractApplicationContext开始看起， 这里就是spring最最重要的部分。 1、默认的spring启动器，DemoApplication： 该方法是spring-boot的启动器，我们进入。 2、进入了SpringApplication.java： 这里创建了一个SpringApplication，执行run方法，返回的是一个ConfigurableApplicationContext，这只是一个接口而已，根据名称来看，称作可配置的应用程序上下文。 3、我们不看new SpringApplication(Sources)过程了，有兴趣可以自己研究一下，里面主要是判断了当前的运行环境是否为web，当然，博主这次的环境是web，然后看run： try语句块内的内容最为重要，因为创建了我们的context对象，此时需要进入的方法为 context = createAndRefreshContext(listeners, applicationArguments) 4、 接着往下看，看到context = createApplicationContext这行，进入，因为我们刚刚在创建SpringApplication时并没有给 this.applicationContextClass赋值，所以此时this.applicationContextClass = null，那么便会创建指定的两个applicationContext中的一个，返回一个刚刚创建的context，这个context便是我们的基 础，因为门现在为web环境，所以创建的context为 AnnotationConfigEmbeddedWebApplicationContext。 5、第4步创建了一个context，需要指出的是，context里面默认带有一个beanFactory，而这个beanFactory的类型为DefaultListableBeanFactory。 然后继续看我们的createAndRefreshContext方法，忽略别的代码，最重要的地方为refresh(context)： 6、进入refresh(context)，不管你进入那个实现类，最终进入的都是AbstractApplicationContext.java： 该方法中，我们这次需要注意的地方有两个： 1、invokeBeanFactoryPostProcessors(beanFactory); 2、finishBeanFactoryInitialization(beanFactory); 两处传入的beanFactory为上面的context中的DefaultListableBeanFactory。 7、进入invokeBeanFactoryPostProcessors(beanFactory)： 然后找到第98行的invokeBeanDefinitionRegistryPostProcessors(priorityOrderedPostProcessors, registry)，该方法看名字就是注册bean，进入。 8、 该方法内部有一个for循环，进入内部方法 postProcessor.postProcesBeanDefinitionRegistry(registry)，此时传入的registry就是我们context中的beanfactory，因为其实现了BeanDefinitionRegistry接口。而此时的postProcessor实现类为ConfigurationClassPostProcessor.java。 9、进入之后直接看最后面的一个方法，名称为processConfigBeanDefinitions(registry)，翻译过来就是配置beanDefinitions的流程。 10、在processConfigBeanDefinitions(registry)里，314行创建了一个parser解析器，用来解析bean。并在第321行进行了调用，那么我们进入parse方法。 11、进入parse方法之后，会发现内层还有parse方法，不要紧，继续进入内层的parse，然后会发现它们均调用了processConfigurationClass(ConfigurationClass configClass)方法： 12、 在processConfigurationClass(ConfigurationClass configClass)方法内，找到do循环，然后进入doProcessConfigurationClass方法，此时，便会出现许多我们常用的注 解了，spring会找到这些注解，并对它们进行解析。例如第268行的componentScanParser.parse方法，在这里会扫描我们的注 解类，并将带有@bean注解的类进行registry。 13、进入 componentScanParser.parse，直接进入结尾的scannner.doScan，然后便会扫描basepackages，并将扫描 到的bean生成一个一个BeanDefinitionHolder，BeanDefinitionHolder中包含有我们bean的一些相关信息、以 及spring赋予其的额外信息，例如别名： 14、 虽然已经创建了BeanDefinitionHolder，但并没有添加到我们的beanFactory中，所以需要执行263行的 registerBeanDefinition(definitionHolder, this.registry)，进入后继续跳转： 然后看registry.registerBeanDefinition方法，因为我们的beanFactory为DefaultListableBeanFactory，所以进入对应的实现类。 15、在进入的registry.registerBeanDefinition方法中，关键点在851行或871行： this.beanDefinitionMap.put(beanName, beanDefinition); 这个方法将扫描到的bean存放到了一个beanName为key、beanDefinition为value的map中，以便执行DI(dependency inject)。 16、现在我们回到第6步的第二条分支，此处是非懒加载的bean初始化位置，注意，我们之前只是对bean的信息进行了获取，然后创建的对象为BeanDefinition，却不是bean的实例，而现在则是创建bean的实例。 进入方法后找到829行的getBean(weaverAwareName)： 17、getBean =&gt; getBeanFactory.getBean =&gt; doGetBean，然后找到306行的createBean，这里不讲语法，不要奇怪为什么这个createBean不能进入实现代码。 18、这之后的代码都比较容易追踪，直接给一条调用链： doCreateBean(482) =&gt; createBeanInstance(510) =&gt; autowireConstructor(1034,1046) =&gt; autowireConstructor(1143) =&gt; instantiate(267) =&gt; instantiateClass(122) =&gt; newInstance(147) 括号内的数字代表行号，方便大家进行追踪，最后看到是反射newInstance取得的对象实例： 平时总说spring反射获取bean，其实也就是听别人这么说而已，还是自己见到才踏实，万一别人问你是不是通过Class.forName获取的呢？ 19、属性注入，位于第18条的doCreateBean方法内，找到第543行，populateBean便译为填充Bean，进入后便能看到和我们平时代码对应的条件了，例如byType注入、byName注入： 这里还没有进行依赖注入，仅仅是准备一些必要的信息，找到1214行的ibp.postProcessPropertyValues方法 20、这里有很多实现类可以选择，因为博主平时是使用@Autowired注解，所以这里选择AutowiredAnnotationBeanPostProcessor，如果你使用@Resource的话，就选择CommonBeanPostProcessor： 21、进入该方法后，首先获取一些元信息metadata，通过findAutowiringMetadata获取，然后调用metadata.inject进行注入： 22、继续进入inject方法后，继续找到88行的element.inject方法并进入，实现类选择AutowiredFieldElement，该类是一个内部类： 在这个方法中，最重要的内容在第567~570行内，我们可以看到，这里其实也就是jdk的反射特性。至此，spring的 bean初始化-&gt;注入 便完成了。 这次的博客内容很长(其实是自己追踪代码时间太久)，感谢大家耐心看完，能有所收获的话便最好不过了。另外，若是有什么补充的话欢迎进行回复。","link":"/2019/04/23/SpringBoot%E6%98%AF%E6%80%8E%E4%B9%88%E5%9C%A8%E5%AE%9E%E4%BE%8B%E5%8C%96%E6%97%B6%E5%80%99%E5%B0%86bean%E5%8A%A0%E8%BD%BD%E8%BF%9B%E5%85%A5%E5%AE%B9%E5%99%A8%E4%B8%AD/"},{"title":"SpringCloud","text":"https://cloud.tencent.com/developer/article/1154457 https://cloud.tencent.com/developer/article/1350910","link":"/2019/05/05/SpringCloud/"},{"title":"个人完善的springboot拦截器","text":"使用Sping Boot2.0 搭建权限拦截的时候 发现 与之前的版本不一样了 123456789101112131415161718192021222324252627282930313233343536373839404142434445## 所有功能完成 配置登录认证### 配置拦截器###### 在spring boot2.0 之后 通过继承这个WebMvcConfigurer类 就可以完成拦截- 新建包com.example.interceptor;- 创建login拦截类```javapackage com.example.interceptor;import org.springframework.web.servlet.HandlerInterceptor;import org.springframework.web.servlet.ModelAndView;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpSession;public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //请求进入这个拦截器 HttpSession session = request.getSession(); if(session.getAttribute(&quot;user&quot;) == null){ //判断session中有没有user信息// System.out.println(&quot;进入拦截器&quot;); if(&quot;XMLHttpRequest&quot;.equalsIgnoreCase(request.getHeader(&quot;X-Requested-With&quot;))){ response.sendError(401); } response.sendRedirect(&quot;/&quot;); //没有user信息的话进行路由重定向 return false; } return true; //有的话就继续操作 } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { }} 在com.example包中添加拦截控制器 1234567891011121314151617181920212223242526package com.example;import com.example.interceptor.LoginInterceptor;import com.example.interceptor.RightsInterceptor;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.*;@Configuration //使用注解 实现拦截public class WebAppConfigurer implements WebMvcConfigurer { @Autowired RightsInterceptor rightsInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { //登录拦截的管理器 InterceptorRegistration registration = registry.addInterceptor(new LoginInterceptor()); //拦截的对象会进入这个类中进行判断 registration.addPathPatterns(&quot;/**&quot;); //所有路径都被拦截 registration.excludePathPatterns(&quot;/&quot;,&quot;/login&quot;,&quot;/error&quot;,&quot;/static/**&quot;,&quot;/logout&quot;); //添加不拦截路径 }} 在WebAppConfigurer.java中增加内容123456789101112131415161718192021222324252627282930313233package com.example;import com.example.interceptor.LoginInterceptor;import com.example.interceptor.RightsInterceptor;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.*;@Configuration //使用注解 实现拦截public class WebAppConfigurer implements WebMvcConfigurer { @Autowired RightsInterceptor rightsInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { //登录拦截的管理器 InterceptorRegistration registration = registry.addInterceptor(new LoginInterceptor()); //拦截的对象会进入这个类中进行判断 registration.addPathPatterns(&quot;/**&quot;); //所有路径都被拦截 registration.excludePathPatterns(&quot;/&quot;,&quot;/login&quot;,&quot;/error&quot;,&quot;/static/**&quot;,&quot;/logout&quot;); //添加不拦截路径// super.addInterceptors(registry); //权限拦截的管理器 InterceptorRegistration registration1 = registry.addInterceptor(rightsInterceptor); registration1.addPathPatterns(&quot;/**&quot;); //所有路径都被拦截 registration1.excludePathPatterns(&quot;/&quot;,&quot;/login&quot;,&quot;/error&quot;,&quot;/static/**&quot;,&quot;/logout&quot;); //添加不拦截路径 }}","link":"/2019/03/27/Springboot2.0%E6%8B%A6%E6%88%AA%E5%99%A8/"},{"title":"Swagger使用指南","text":"Swagger使用指南 现代化的研发组织架构中，一个研发团队基本包括了产品组、后端组、前端组、APP端研发、测试组、UI组等，各个细分组织人员各司其职，共同完成产品的全周期工作。如何进行组织架构内的有效高效沟通就显得尤其重要。其中，如何构建一份合理高效的接口文档更显重要。 接口文档横贯各个端的研发人员，但是由于接口众多，细节不一，有时候理解起来并不是那么容易，引起‘内战’也在所难免， 并且维护也是一大难题。 类似RAP文档管理系统，将接口文档进行在线维护，方便了前端和APP端人员查看进行对接开发，但是还是存在以下几点问题： 文档是接口提供方手动导入的，是静态文档，没有提供接口测试功能； 维护的难度不小 Swagger的出现可以完美解决以上传统接口管理方式存在的痛点。本文介绍Spring Boot整合Swagger2的流程，连带填坑。 使用流程如下： 1）引入相应的maven包： 1234567891011&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt; 2）编写Swagger2的配置类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.trace.configuration;import io.swagger.annotations.Api;import org.springframework.beans.factory.annotation.Value;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import springfox.documentation.builders.ApiInfoBuilder;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;import springfox.documentation.swagger2.annotations.EnableSwagger2;/*** Created by Trace on 2018-05-16.&lt;br/&gt;* Desc: swagger2配置类*/@SuppressWarnings({&quot;unused&quot;})@Configuration @EnableSwagger2public class Swagger2Config { @Value(&quot;${swagger2.enable}&quot;) private boolean enable; @Bean(&quot;UserApis&quot;) public Docket userApis() { return new Docket(DocumentationType.SWAGGER_2) .groupName(&quot;用户模块&quot;) .select() .apis(RequestHandlerSelectors.withClassAnnotation(Api.class)) .paths(PathSelectors.regex(&quot;/user.*&quot;)) .build() .apiInfo(apiInfo()) .enable(enable); } @Bean(&quot;CustomApis&quot;) public Docket customApis() { return new Docket(DocumentationType.SWAGGER_2) .groupName(&quot;客户模块&quot;) .select() .apis(RequestHandlerSelectors.withClassAnnotation(Api.class)) .paths(PathSelectors.regex(&quot;/custom.*&quot;)) .build() .apiInfo(apiInfo()) .enable(enable); } private ApiInfo apiInfo() { return new ApiInfoBuilder() .title(&quot;XXXXX系统平台接口文档&quot;) .description(&quot;提供子模块1/子模块2/子模块3的文档, 更多请关注公众号: 随行享阅&quot;) .termsOfServiceUrl(&quot;https://xingtian.github.io/trace.github.io/&quot;) .version(&quot;1.0&quot;) .build(); }} 如上可见：通过注解@EnableSwagger2开启swagger2，apiInfo是接口文档的基本说明信息，包括标题、描述、服务网址、联系人、版本等信息； 在Docket创建中，通过groupName进行分组，paths属性进行过滤，apis属性可以设置扫描包，或者通过注解的方式标识；通过enable属性，可以在application-{profile}.properties文件中设置相应值，主要用于控制生产环境不生成接口文档。 3）controller层类和方法添加相关注解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110package com.trace.controller;import com.trace.bind.ResultModel;import com.trace.entity.po.Area;import com.trace.entity.po.User;import com.trace.service.UserService;import io.swagger.annotations.Api;import io.swagger.annotations.ApiImplicitParam;import io.swagger.annotations.ApiImplicitParams;import io.swagger.annotations.ApiOperation;import org.springframework.web.bind.annotation.*;import javax.annotation.Resource;import java.util.List;/** * Created by Trace on 2017-12-01.&lt;br/&gt; * Desc: 用户管理controller */@SuppressWarnings(&quot;unused&quot;)@RestController @RequestMapping(&quot;/user&quot;)@Api(tags = &quot;用户管理&quot;)public class UserController { @Resource private UserService userService; @GetMapping(&quot;/query/{id}&quot;) @ApiOperation(&quot;通过ID查询&quot;) @ApiImplicitParam(name = &quot;id&quot;, value = &quot;用户ID&quot;, required = true, dataType = &quot;int&quot;, paramType = &quot;path&quot;) public ResultModel&lt;User&gt; findById(@PathVariable int id) { User user = userService.findById(id); return ResultModel.success(&quot;id查询成功&quot;, user); } @GetMapping(&quot;/query/ids&quot;) @ApiOperation(&quot;通过ID列表查询&quot;) public ResultModel&lt;List&lt;User&gt;&gt; findByIdIn(int[] ids) { List&lt;User&gt; users = userService.findByIdIn(ids); return ResultModel.success(&quot;in查询成功&quot;, users); } @GetMapping(&quot;/query/user&quot;) @ApiOperation(&quot;通过用户实体查询&quot;) public ResultModel&lt;List&lt;User&gt;&gt; findByUser(User user) { List&lt;User&gt; users = userService.findByUser(user); return ResultModel.success(&quot;通过实体查询成功&quot;, users); } @GetMapping(&quot;/query/all&quot;) @ApiOperation(&quot;查询所有用户&quot;) public ResultModel&lt;List&lt;User&gt;&gt; findAll() { List&lt;User&gt; users = userService.findAll(); return ResultModel.success(&quot;全体查找成功&quot;, users); } @GetMapping(&quot;/query/username&quot;) @ApiOperation(&quot;通过用户名称模糊查询&quot;) @ApiImplicitParam(name = &quot;userName&quot;, value = &quot;用户名称&quot;) public ResultModel&lt;List&lt;User&gt;&gt; findByUserName(String userName) { List&lt;User&gt; users = userService.findByUserName(userName); return ResultModel.success(users); } @PostMapping(&quot;/insert&quot;) @ApiOperation(&quot;新增默认用户&quot;) public ResultModel&lt;Integer&gt; insert() { User user = new User(); user.setUserName(&quot;zhongshiwen&quot;); user.setNickName(&quot;zsw&quot;); user.setRealName(&quot;钟仕文&quot;); user.setPassword(&quot;zsw123456&quot;); user.setGender(&quot;男&quot;); Area area = new Area(); area.setLevel((byte) 5); user.setArea(area); userService.save(user); return ResultModel.success(&quot;新增用户成功&quot;, user.getId()); } @PutMapping(&quot;/update&quot;) @ApiOperation(&quot;更新用户信息&quot;) public ResultModel&lt;Integer&gt; update(User user) { int row = userService.update(user); return ResultModel.success(row); } @PutMapping(&quot;/update/status&quot;) @ApiOperation(&quot;更新单个用户状态&quot;) @ApiImplicitParams({ @ApiImplicitParam(name = &quot;id&quot;, value = &quot;用户ID&quot;, required = true), @ApiImplicitParam(name = &quot;status&quot;, value = &quot;状态&quot;, required = true) }) public ResultModel&lt;User&gt; updateStatus(int id, byte status) { User user = userService.updateStatus(id, status); return ResultModel.success(user); } @DeleteMapping(&quot;/delete&quot;) @ApiOperation(&quot;删除单个用户&quot;) @ApiImplicitParam(value = &quot;用户ID&quot;, required = true) public ResultModel&lt;Integer&gt; delete(int id) { return ResultModel.success(userService.delete(id)); }} 4）返回对象ResultModel 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.trace.bind;import io.swagger.annotations.ApiModel;import io.swagger.annotations.ApiModelProperty;import lombok.Getter;import lombok.Setter;/*** Created by Trace on 2017-12-01.&lt;br/&gt;* Desc: 接口返回结果对象*/@SuppressWarnings(&quot;unused&quot;)@Getter @Setter @ApiModel(description = &quot;返回结果&quot;)public final class ResultModel&lt;T&gt; { @ApiModelProperty(&quot;是否成功: true or false&quot;) private boolean result; @ApiModelProperty(&quot;描述性原因&quot;) private String message; @ApiModelProperty(&quot;业务数据&quot;) private T data; private ResultModel(boolean result, String message, T data) { this.result = result; this.message = message; this.data = data; } public static&lt;T&gt; ResultModel&lt;T&gt; success(T data) { return new ResultModel&lt;&gt;(true, &quot;SUCCESS&quot;, data); } public static&lt;T&gt; ResultModel&lt;T&gt; success(String message, T data) { return new ResultModel&lt;&gt;(true, message, data); } public static ResultModel failure() { return new ResultModel&lt;&gt;(false, &quot;FAILURE&quot;, null); } public static ResultModel failure(String message) { return new ResultModel&lt;&gt;(false, message, null); }} 5）ApiModel属性对象 – User实体 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.trace.entity.po;import com.trace.mapper.base.NotPersistent;import io.swagger.annotations.ApiModel;import io.swagger.annotations.ApiModelProperty;import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import java.time.LocalDate;import java.time.LocalDateTime;import java.util.List;/** * Created by Trace on 2017-12-01.&lt;br/&gt; * Desc: 用户表tb_user */@SuppressWarnings(&quot;unused&quot;)@Data @NoArgsConstructor @AllArgsConstructor@ApiModelpublic class User { @ApiModelProperty(&quot;用户ID&quot;) private Integer id; @ApiModelProperty(&quot;账户名&quot;) private String userName; @ApiModelProperty(&quot;用户昵称&quot;) private String nickName; @ApiModelProperty(&quot;真实姓名&quot;) private String realName; @ApiModelProperty(&quot;身份证号码&quot;) private String identityCard; @ApiModelProperty(&quot;性别&quot;) private String gender; @ApiModelProperty(&quot;出生日期&quot;) private LocalDate birth; @ApiModelProperty(&quot;手机号码&quot;) private String phone; @ApiModelProperty(&quot;邮箱&quot;) private String email; @ApiModelProperty(&quot;密码&quot;) private String password; @ApiModelProperty(&quot;用户头像地址&quot;) private String logo; @ApiModelProperty(&quot;账户状态 0:正常; 1:冻结; 2:注销&quot;) private Byte status; @ApiModelProperty(&quot;个性签名&quot;) private String summary; @ApiModelProperty(&quot;用户所在区域码&quot;) private String areaCode; @ApiModelProperty(&quot;注册时间&quot;) private LocalDateTime registerTime; @ApiModelProperty(&quot;最近登录时间&quot;) private LocalDateTime lastLoginTime; @NotPersistent @ApiModelProperty(hidden = true) private transient Area area; //用户所在地区 @NotPersistent @ApiModelProperty(hidden = true) private transient List&lt;Role&gt; roles; //用户角色列表} 简单说下Swagger2几个重要注解： @Api：用在请求的类上，表示对类的说明 tags “说明该类的作用，可以在UI界面上看到的注解” value “该参数没什么意义，在UI界面上也看到，所以不需要配置” @ApiOperation：用在请求的方法上，说明方法的用途、作用 value=”说明方法的用途、作用” notes=”方法的备注说明” @ApiImplicitParams：用在请求的方法上，表示一组参数说明 @ApiImplicitParam：用在@ApiImplicitParams注解中，指定一个请求参数的各个方面 value：参数的汉字说明、解释 required：参数是否必须传 paramType：参数放在哪个地方 header –&gt; 请求参数的获取：@RequestHeader query –&gt; 请求参数的获取：@RequestParam path（用于restful接口）–&gt; 请求参数的获取：@PathVariable body（不常用） form（不常用） dataType：参数类型，默认String，其它值dataType=”Integer” defaultValue：参数的默认值 @ApiResponses：用在请求的方法上，表示一组响应 @ApiResponse：用在@ApiResponses中，一般用于表达一个错误的响应信息 code：数字，例如400 message：信息，例如”请求参数没填好” response：抛出异常的类 @ApiModel：主要有两种用途： 用于响应类上，表示一个返回响应数据的信息 入参实体：使用@RequestBody这样的场景，请求参数无法使用@ApiImplicitParam注解进行描述的时候 @ApiModelProperty：用在属性上，描述响应类的属性 **最终呈现结果： ** 如前所述：通过maven导入了swagger-ui： 12345&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt;&lt;/dependency&gt; 那么，启动应用后，会自动生成http://{root-path}/swagger-ui.html页面，访问后，效果如下所示： 可以在线测试接口，如通过ID查询的接口/user/query/{id}","link":"/2019/04/19/Swagger%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"},{"title":"Spring中重要的注解","text":"现在大部分的Spring项目都会用到注解。使用注解来替换xml，一行简单的注解就可以解决很多事情。但是你真的懂其中的原理吗。 本文翻译于 https://docs.spring.io/spring-framework/docs and https://docs.spring.io/spring-framework/docs 在面试的时候 多少会问道 你了解过Spring注解吗。 先来谈谈@Configuration定义：指示一个类声明一个或者多个@Bean 声明的方法并且由Spring容器统一管理，以便在运行时为这些bean生成bean的定义和服务请求的类。 例如：12345678910@Configurationpublic class AppConfig { @Bean public MyBean myBean(){ return new MyBean(); }} 上述AppConfig 加入@Configuration 注解，表明这就是一个配置类。有一个myBean()的方法，返回一个MyBean()的实例，并用@Bean 进行注释，表明这个方法是需要被Spring进行管理的bean。@Bean 如果不指定名称的话，默认使用myBean名称，也就是小写的名称。 通过注解启动： 通过启动一个AnnotationConfigApplicationContext 来引导这个@Configuration 注解的类，比如： 123AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.register(AppConfig.class);ctx.refresh(); 在web项目中，也可以使用AnnotationContextWebApplicationContext或者其他变体来启动。 **使用SpringBoot项目的例子如下： ** pom.xml 文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.spring.configuration&lt;/groupId&gt; &lt;artifactId&gt;spring-configuration&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;spring-configuration&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 在config 包下新建一个MyConfiguration环境配置，和上面的示例代码相似，完整的代码如下： 123456789@Configurationpublic class MyConfiguration { @Bean public MyBean myBean(){ System.out.println(&quot;myBean Initialized&quot;); return new MyBean(); }} 说明MyConfiguration 是一个配置类，能够在此类下面声明管理多个Bean，我们声明了一个MyBean的bean，希望它被容器加载和管理。 新建一个 MyBean的类，具体代码如下 1234567891011public class MyBean { public MyBean(){ System.out.println(&quot;generate MyBean Instance&quot;); } public void init(){ System.out.println(&quot;MyBean Resources Initialized&quot;); }} 新建一个SpringConfigurationApplication类，用来测试MyConfiguration类，具体代码如下： 1234567891011121314public class SpringConfigurationApplication { public static void main(String[] args) { // AnnotationConfigApplicationContext context = = new AnnotationConfigApplicationContext(MyConfiguration.class) // 因为我们加载的@Configuration 是基于注解形式的，所以需要创建AnnotationConfigApplicationContext AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); // 注册MyConfiguration 类并刷新bean 容器。 context.register(MyConfiguration.class); context.refresh(); }} 输出：myBean Initialized generate MyBean Instance 从输出的结果可以看到，默认名称为myBean 的bean随着容器的加载而加载，因为myBean方法返回一个myBean的构造方法，所以myBean被初始化了。 通过XML 的方式来启动例子如下 通过XML 的方式来启动 可以通过使用XML方式定义的&lt;context:annotation-config /&gt;开启基于注解的启动，然后再定义一个MyConfiguration的bean，在/resources 目录下新建 application-context.xml 代码如下 123456789101112131415&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd&quot;&gt; &lt;!-- 相当于基于注解的启动类 AnnotationConfigApplicationContext--&gt; &lt;context:annotation-config /&gt; &lt;bean class=&quot;com.spring.configuration.config.MyConfiguration&quot;/&gt;&lt;/beans&gt; 需要引入applicationContext.xml ，在SpringConfigurationApplication 需要进行引入，修改后的SpringConfigurationApplication如下： 12345678public class SpringConfigurationApplication { public static void main(String[] args) { ApplicationContext context = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); }} 输出：myBean Initialized generate MyBean Instance 再谈谈ComponentScan （扫包范围）@Configuration 使用@Component 进行原注解，因此@Configuration 类也可以被组件扫描到（特别是使用XML context:component-scan 元素）。 在这里认识几个注解: @Controller,** @Service**,** @Repository**, @Component @Controller: 表明一个注解的类是一个”Controller”，也就是控制器，可以把它理解为MVC 模式的Controller 这个角色。这个注解是一个特殊的@Component，允许实现类通过类路径的扫描扫描到。它通常与@RequestMapping 注解一起使用。 @Service: 表明这个带注解的类是一个”Service”，也就是服务层，可以把它理解为MVC 模式中的Service层这个角色，这个注解也是一个特殊的@Component，允许实现类通过类路径的扫描扫描到 @Repository: 表明这个注解的类是一个”Repository”,团队实现了JavaEE 模式中像是作为”Data Access Object” 可能作为DAO来使用，当与 PersistenceExceptionTranslationPostProcessor 结合使用时，这样注释的类有资格获得Spring转换的目的。这个注解也是@Component 的一个特殊实现，允许实现类能够被自动扫描到 @Component: 表明这个注释的类是一个组件，当使用基于注释的配置和类路径扫描时，这些类被视为自动检测的候选者。 也就是说，上面四个注解标记的类都能够通过@ComponentScan 扫描到，上面四个注解最大的区别就是使用的场景和语义不一样. 比如你定义一个Service类想要被Spring进行管理，你应该把它定义为@Service 而不是@Controller因为我们从语义上讲，@Service更像是一个服务的类，而不是一个控制器的类，@Component通常被称作组件，它可以标注任何你没有严格予以说明的类，比如说是一个配置类，它不属于MVC模式的任何一层，这个时候你更习惯于把它定义为 @Component。@Controller，@Service，@Repository 的注解上都有@Component，所以这三个注解都可以用@Component进行替换。 来看一下代码进行理解： 定义五个类，类上分别用@Controller, @Service, @Repository, @Component, @Configuration 进行标注，分别如下 123456789101112131415@Componentpublic class UserBean {}@Configurationpublic class UserConfiguration {}@Controllerpublic class UserController {}@Repositorypublic class UserDao {}@Servicepublic class UserService {} 在MyConfiguration上加上@ComponentScan 注解，扫描上面5个类所在的包位置。代码如下： 12345678910@Configuration@ComponentScan(basePackages = &quot;com.spring.configuration.pojo&quot;)public class MyConfiguration { @Bean public MyBean myBean(){ System.out.println(&quot;myBean Initialized&quot;); return new MyBean(); }} 修改 SpringConfigurationApplication 中的代码，如下： 1234567891011121314151617181920public class SpringConfigurationApplication { public static void main(String[] args) {// AnnotationConfigApplicationContext context = = new AnnotationConfigApplicationContext(MyConfiguration.class)// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(); context.register(MyConfiguration.class); context.refresh(); // 获取启动过程中的bean 定义的名称 for(String str : context.getBeanDefinitionNames()){ System.out.println(&quot;str = &quot; + str); } context.close(); }} 输出： myBean Initializedgenerate MyBean Instancestr = org.springframework.context.annotation.internalConfigurationAnnotationProcessorstr = org.springframework.context.annotation.internalAutowiredAnnotationProcessorstr = org.springframework.context.annotation.internalRequiredAnnotationProcessorstr = org.springframework.context.annotation.internalCommonAnnotationProcessorstr = org.springframework.context.event.internalEventListenerProcessorstr = org.springframework.context.event.internalEventListenerFactorystr = myConfigurationstr = userBeanstr = userConfigurationstr = userControllerstr = userDaostr = userServicestr = myBean 由输出可以清楚的看到，上述定义的五个类成功被@ComponentScan 扫描到，并在程序启动的时候进行加载。 @Configuration 和 Environment@Configuration 通常和Environment 一起使用，通过@Environment 解析的属性驻留在一个或多个”属性源”对象中，@Configuration类可以使用@PropertySource，像Environment 对象提供属性源 为了便于测试，我们引入junit4和spring-test 的依赖，完整的配置文件如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.4.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.spring.configuration&lt;/groupId&gt; &lt;artifactId&gt;spring-configuration&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;spring-configuration&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring.version&gt;5.0.6.RELEASE&lt;/spring.version&gt; &lt;spring.test.version&gt;4.3.13.RELEASE&lt;/spring.test.version&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;${spring.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;${spring.test.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;${junit.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 在config 包下定义一个 EnvironmentConfig 类，注入Environment 属性，完整代码如下： 1234567891011121314151617181920@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = EnvironmentConfig.class)@Configuration@PropertySource(&quot;classpath:beanName.properties&quot;)public class EnvironmentConfig { @Autowired Environment env; @Test public void testReadProperty(){ // 获取bean.name.controller 的属性 System.out.println(env.getProperty(&quot;bean.name.controller&quot;)); // 判断是否包含bean.name.component System.out.println(env.containsProperty(&quot;bean.name.component&quot;)); // 返回与给定键关联的属性值 System.out.println(env.getRequiredProperty(&quot;bean.name.service&quot;)); }} 在/resources 目录下新建beanName.properties 文件，如下： 123456bean.name.configuration=beanNameConfigurationbean.name.controller=beanNameControllerbean.name.service=beanNameServicebean.name.component=beanNameComponentbean.name.repository=beanNameRepository 启动并进行Junit测试，输出如下： beanNameControllertruebeanNameService @Autowired 、 @Inject、@Resource 的区别@Inject: 这是jsr330 的规范，通过AutowiredAnnotationBeanPostProcessor 类实现的依赖注入。位于javax.inject包内，是Java自带的注解。 如下是@Inject的使用，不加@Named注解，需要配置与变量名一致即可。 123@Inject@Named(&quot;mongo&quot;)private Mongo mongo; @Autowired: @Autowired是Spring提供的注解，通过AutowiredAnnotationBeanPostProcessor类实现的依赖注入，与@inject二者具有可互换性。位于org.springframework.beans.factory.annotation 包内，是Spring 中的注解 @Autowired默认是按照byType进行注入的，但是当byType方式找到了多个符合的bean，又是怎么处理的？Autowired默认先按byType，如果发现找到多个bean，则又按照byName方式比对，如果还有多个，则报出异常。 12345678910public class TestServiceImpl {// 下面两种@Autowired只要使用一种即可@Autowiredprivate UserDao userDao; // 用于字段上 @Autowiredpublic void setUserDao(UserDao userDao) { // 用于属性的方法上this.userDao = userDao;}} @Autowired 注解是按照类型（byType）装配依赖对象，默认情况下它要求依赖对象必须存在，如果允许null值，可以设置它的required属性为false。如果我们想使用按照名称（byName）来装配，可以结合@Qualifier注解一起使用。如下： 12345public class TestServiceImpl {@Autowired@Qualifier(&quot;userDao&quot;)private UserDao userDao;} @Resource: @Resource 是jsr250规范的实现，@Resource通过CommonAnnotationBeanPostProcessor 类实现注入。@Resource 一般会指定一个name属性，如下： @Resource默认按照ByName自动注入，由J2EE提供，需要导入包javax.annotation.Resource。@Resource有两个重要的属性：name和type，而Spring将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。所以，如果使用name属性，则使用byName的自动注入策略，而使用type属性时则使用byType自动注入策略。如果既不制定name也不制定type属性，这时将通过反射机制使用byName自动注入策略。 12345678910public class TestServiceImpl {// 下面两种@Resource只要使用一种即可@Resource(name=&quot;userDao&quot;)private UserDao userDao; // 用于字段上 @Resource(name=&quot;userDao&quot;)public void setUserDao(UserDao userDao) { // 用于属性的setter方法上this.userDao = userDao;}} 注： @Autowired和@Inject基本是一样的，因为两者都是使用AutowiredAnnotationBeanPostProcessor来处理依赖注入。但是@Resource是个例外，它使用的是CommonAnnotationBeanPostProcessor来处理依赖注入。当然，两者都是BeanPostProcessor。 @Autowired和@Inject默认autowired by type，可以通过@Qualifier显式指定autowired by qualifier name。 @Resource默认autowired by field name，如果autowired by field name失败，会退化为autowired by type，可以通过@Qualifier显式指定autowired by qualifier name，如果autowired by qualifier name失败，会退化为autowired by field name。但是这时候如果autowired by field name失败，就不会再退化为autowired by type。 @Value、@PropertySource 和 @Configuration@Configuration 可以和@Value 和@PropertySource 一起使用读取外部配置文件，具体用法如下： 在config 包下新建一个ReadValueFromPropertySource类，代码如下 12345678910111213@PropertySource(&quot;classpath:beanName.properties&quot;)@Configurationpublic class ReadValueFromPropertySource { @Value(&quot;bean.name.component&quot;) String beanName; @Bean(&quot;myTestBean&quot;) public MyBean myBean(){ return new MyBean(beanName); }} 通过@PropertySource引入的配置文件，使@Value 能够获取到属性值，在给myBean()方法指定了一个名称叫做myTestBean。 修改MyBean类，增加一个name属性和一个构造器，再生成其toString() 方法 1234567891011121314151617181920212223public class MyBean { String name; public MyBean(String name) { this.name = name; } public MyBean(){ System.out.println(&quot;generate MyBean Instance&quot;); } public void init(){ System.out.println(&quot;MyBean Resources Initialized&quot;); } @Override public String toString() { return &quot;MyBean{&quot; + &quot;name='&quot; + name + '\\'' + '}'; }} 通过@PropertySource引入的配置文件，使@Value 能够获取到属性值，在给myBean()方法指定了一个名称叫做myTestBean 修改MyBean类，增加一个name属性和一个构造器，再生成其toString() 方法 1234567891011121314151617181920212223public class MyBean { String name; public MyBean(String name) { this.name = name; } public MyBean(){ System.out.println(&quot;generate MyBean Instance&quot;); } public void init(){ System.out.println(&quot;MyBean Resources Initialized&quot;); } @Override public String toString() { return &quot;MyBean{&quot; + &quot;name='&quot; + name + '\\'' + '}'; }} 在SpringConfigurationApplication中进行测试，如下 12345678910111213141516171819202122232425public class SpringConfigurationApplication { public static void main(String[] args) { // 为了展示配置文件的完整性，之前的代码没有删除。// AnnotationConfigApplicationContext context = = new AnnotationConfigApplicationContext(MyConfiguration.class)// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);// AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();// context.register(MyConfiguration.class);// context.refresh();//// // 获取启动过程中的bean 定义的名称// for(String str : context.getBeanDefinitionNames()){// System.out.println(&quot;str = &quot; + str);// }// context.close(); ApplicationContext context = new AnnotationConfigApplicationContext(ReadValueFromPropertySource.class); MyBean myBean = (MyBean) context.getBean(&quot;myTestBean&quot;); System.out.println(&quot;myBean = &quot; + myBean); }} 使用Applicatio@InConntext 就能够获取myTestBean 这个bean，再生成myBean的实例。 输出：myBean = MyBean{name=’bean.name.component’} @Import 和 @Configuration@Import的定义(来自于JavaDoc)：表明一个或者多个配置类需要导入，提供与Spring XML中相等的功能，允许导入@Configuration 、@ImportSelector、@ImportBeanDefinitionRegistar的实现，以及常规组件类似于AnnotationConfigApplicationContext。可能用于类级别或者是原注解。如果XML或者其他非@Configuration标记的Bean资源需要被导入的话，使用@ImportResource。下面是一个示例代码： 在pojo 包下新建两个配置类，分别是CustomerBo, SchedualBo 12345678910111213141516171819202122232425@Configurationpublic class CustomerBo { public void printMsg(String msg){ System.out.println(&quot;CustomerBo : &quot; + msg); } @Bean public CustomerBo testCustomerBo(){ return new CustomerBo(); }}@Configurationpublic class SchedulerBo { public void printMsg(String msg){ System.out.println(&quot;SchedulerBo : &quot; + msg); } @Bean public SchedulerBo testSchedulerBo(){ return new SchedulerBo(); }} 在config 包下新建一个AppConfig，导入CustomerBo 和 SchedulerBo 。 123@Configuration@Import(value = {CustomerBo.class,SchedulerBo.class})public class AppConfig {} 在config 包下新建一个ImportWithConfiguration ，用于测试@Import 和 @Configuration 的使用 1234567891011public class ImportWithConfiguration { public static void main(String[] args) { ApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class); CustomerBo customerBo = (CustomerBo) context.getBean(&quot;testCustomerBo&quot;); customerBo.printMsg(&quot;System out println('get from customerBo')&quot;); SchedulerBo schedulerBo = (SchedulerBo) context.getBean(&quot;testSchedulerBo&quot;); schedulerBo.printMsg(&quot;System out println('get from schedulerBo')&quot;); }} 输出 : CustomerBo : System out println(‘get from customerBo’) SchedulerBo : System out println(‘get from schedulerBo’) @Profile @Profile: 表示当一个或多个@Value 指定的配置文件处于可用状态时，组件符合注册条件，可以进行注册。 ** 三种设置方式：** 可以通过ConfigurableEnvironment.setActiveProfiles()以编程的方式激活 可以通过AbstractEnvironment.ACTIVE_PROFILES_PROPERTY_NAME (spring.profiles.active )属性设置为JVM属性 作为环境变量，或作为web.xml 应用程序的Servlet 上下文参数。也可以通过@ActiveProfiles 注解在集成测试中以声明方式激活配置文件。 ** 作用域** 作为类级别的注释在任意类或者直接与@Component 进行关联，包括@Configuration 类 作为原注解，可以自定义注解 作为方法的注解作用在任何方法 注意: 如果一个配置类使用了Profile 标签或者@Profile 作用在任何类中都必须进行启用才会生效，如果@Profile({“p1”,”!p2”}) 标识两个属性，那么p1 是启用状态 而p2 是非启用状态的。 @ImportResource 和 @Configuration@ImportResource: 这个注解提供了与@Import 功能相似作用，通常与@Configuration 一起使用，通过AnnotationConfigApplicationContext 进行启动，下面以一个示例来看一下具体用法： 在config下新建TestService 类，声明一个构造函数，类初始化时调用 123456public class TestService { public TestService(){ System.out.println(&quot;test @importResource success&quot;); }} 在/resources 目录下新建 importResources.xml ，为了导入TestService 123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:util=&quot;http://www.springframework.org/schema/util&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd&quot;&gt; &lt;bean id = &quot;testService&quot; class=&quot;com.spring.configuration.config.TestService&quot; /&gt;&lt;/beans&gt; 然后在config 下新建一个ImportResourceWithConfiguration， 用于读取配置文件 123456789101112131415161718@Configuration@ImportResource(&quot;classpath:importResources.xml&quot;)public class ImportResourceWithConfiguration { @Autowired private TestService service; public void getImportResource(){ new TestService(); } public static void main(String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(ImportResourceWithConfiguration.class); context.getBean(&quot;testService&quot;); }} 输出：test @importResource success @Configuration 嵌套 @Configuration注解作用在类上，就和普通类一样能够进行相互嵌套，定义内部类 1234567891011121314151617181920// 来自JavaDoc@Configurationpublic class AppConfig{ @Inject DataSource dataSource; @Bean public MyBean myBean(){ return new MyBean(dataSource); } @Configuration static class DataConfig(){ @Bean DataSource dataSource(){ return new EmbeddedDatabaseBuilder().build() } }} 在上述代码中，只需要在应用程序的上下文中注册 AppConfig 。由于是嵌套的@Configuration 类，DatabaseConfig 将自动注册。当AppConfig 、DatabaseConfig 之间的关系已经隐含清楚时，这就避免了使用@Import 注解的需要。 @Lazy 延迟初始化@Lazy : 表明一个bean 是否延迟加载，可以作用在方法上，表示这个方法被延迟加载；可以作用在@Component (或者由@Component 作为原注解) 注释的类上，表明这个类中所有的bean 都被延迟加载。如果没有@Lazy注释，或者@Lazy 被设置为false，那么该bean 就会急切渴望被加载；除了上面两种作用域，@Lazy 还可以作用在@Autowired和@Inject注释的属性上，在这种情况下，它将为该字段创建一个惰性代理，作为使用ObjectFactory或Provider的默认方法。下面来演示一下： 修改MyConfiguration类，在该类上添加@Lazy 注解，新增一个IfLazyInit()方法，检验是否被初始化。 1234567891011121314151617@Lazy@Configuration@ComponentScan(basePackages = &quot;com.spring.configuration.pojo&quot;)public class MyConfiguration { @Bean public MyBean myBean(){ System.out.println(&quot;myBean Initialized&quot;); return new MyBean(); } @Bean public MyBean IfLazyInit(){ System.out.println(&quot;initialized&quot;); return new MyBean(); }} 修改SpringConfigurationApplication 启动类，放开之前MyConfiguration 的启动类 123456789101112131415161718192021222324public class SpringConfigurationApplication { public static void main(String[] args) { AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(MyConfiguration.class);// ApplicationContext context = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;);// AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();// context.register(MyConfiguration.class);// context.refresh();//// // 获取启动过程中的bean 定义的名称 for(String str : context.getBeanDefinitionNames()){ System.out.println(&quot;str = &quot; + str); }// context.close();// ApplicationContext context =// new AnnotationConfigApplicationContext(ReadValueFromPropertySource.class);// MyBean myBean = (MyBean) context.getBean(&quot;myTestBean&quot;);// System.out.println(&quot;myBean = &quot; + myBean); }} 输出你会发现没有关于bean的定义信息，但是当吧@Lazy 注释拿掉，你会发现输出了关于bean的初始化信息：myBean Initializedgenerate MyBean Instanceinitializedgenerate MyBean Instance @RunWith 和 @ContextConfigurationJunit4 测试类，用于注解在类上表示通过Junit4 进行测试，可以省略编写启动类代码，是ApplicationContext 等启动类的替换。一般用@RunWith 和 @Configuration 进行单元测试，这是软件开发过程中非常必要而且具有专业性的一部分，上面EnvironmentConfig 类证实了这一点： 12345678910111213141516171819202122@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = EnvironmentConfig.class)@Configuration@PropertySource(&quot;classpath:beanName.properties&quot;)public class EnvironmentConfig {// @Autowired// Environment env; @Inject Environment env; @Test public void testReadProperty(){ // 获取bean.name.controller 的属性 System.out.println(env.getProperty(&quot;bean.name.controller&quot;)); // 判断是否包含bean.name.component System.out.println(env.containsProperty(&quot;bean.name.component&quot;)); // 返回与给定键关联的属性值 System.out.println(env.getRequiredProperty(&quot;bean.name.service&quot;)); }} @Enable 启动Spring内置功能 详情查阅 @EnableAsync,@EnableScheduling,@EnableTransactionManagement,@EnableAspectJAutoProxy,@EnableWebMvc官方文档 ** @Configuration 使用约束** 必须以类的方式提供(即不是从工厂方法返回的实例) @Configuration 注解的类必须是非final的 配置类必须是非本地的（即可能不在方法中声明）,native 标注的方法 任何嵌套的@Configuration 都必须是static 的。 @Bean 方法可能不会反过来创建更多配置类","link":"/2019/03/13/Spring%E4%B8%AD%E9%87%8D%E8%A6%81%E7%9A%84%E6%B3%A8%E8%A7%A3/"},{"title":"docker命令","text":"docker基本命令 docker logs 检查排错。如果启动不起容器，可以试着检查排错 docker安装jenkins及其相关问题解决 https://www.cnblogs.com/youcong/p/10182091.html systemctl stop firewalld.service 关闭防火墙 docker inspect 容器id 查询容器信息 docker stop 容器id 停止容器id docker rm 容器id 删除容器id systemctl restart docker 重启docker容器 docker exec -it 容器ID /bin/bash 进入容器 docker rm $(sudo docker ps -a -q) 删除所有未运行的容器 docker search elasticsearch 搜索镜像文件 docker run 创建并启动一个容器，在run后面加上-d参数，则会创建一个守护式容器在后台运行。 docker ps -a 查看已经创建的容器 docker ps -s 查看已经启动的容器 docker start con_name 启动容器名为con_name的容器 docker stop con_name 停止容器名为con_name的容器 docker rm con_name 删除容器名为con_name的容器 docker rename old_name new_name 重命名一个容器 docker attach con_name 将终端附着到正在运行的容器名为con_name的容器的终端上面去，前提是创建该容器时指定了相应的sh docker logs –tail=”10” 容器名称 查询容器日志信息","link":"/2019/04/25/docker%E5%91%BD%E4%BB%A4/"},{"title":"dubbo 支持哪些通信协议？支持哪些序列化协议？","text":"面试题dubbo 支持哪些通信协议？支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？ 面试官心理分析上一个问题，说说 dubbo 的基本工作原理，那是你必须知道的，至少要知道 dubbo 分成哪些层，然后平时怎么发起 rpc 请求的，注册、发现、调用，这些是基本的。接着就可以针对底层进行深入的问问了，比如第一步就可以先问问序列化协议这块，就是平时 RPC 的时候怎么走的？ 面试题剖析序列化，就是把数据结构或者是一些对象，转换为二进制串的过程，而反序列化是将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。###dubbo 支持不同的通信协议 dubbo 协议 默认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。为了要支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次！此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接就可以，可能总共就 100 个连接。然后后面直接基于长连接 NIO 异步通信，可以支撑高并发请求。长连接，通俗点说，就是建立连接过后可以持续发送请求，无须再建立连接。 dubbo-keep-connection而短连接，每次要发送请求之前，需要先重新建立一次连接。 dubbo-not-keep-connection rmi 协议走 Java 二进制序列化，多个短连接，适合消费者和提供者数量差不多的情况，适用于文件的传输，一般较少用。 hessian 协议走 hessian 序列化协议，多个短连接，适用于提供者数量比消费者数量还多的情况，适用于文件的传输，一般较少用。 http 协议走 json 序列化。 webservice走 SOAP 文本序列化。 dubbo 支持的序列化协议dubbo 支持 hession、Java 二进制序列化、json、SOAP 文本序列化多种序列化协议。但是 hessian 是其默认的序列化协议。 说一下 Hessian 的数据结构Hessian 的对象序列化机制有 8 种原始类型： 原始二进制数据 boolean 64-bit date（64 位毫秒值的日期） 64-bit double32-bit int 64-bit long null UTF-8 编码的 string 另外还包括 3 种递归类型： list for lists and arraysmap for maps and dictionaries object for objects 还有一种特殊的类型： ref：用来表示对共享对象的引用。 为什么 PB 的效率是最高的？可能有一些同学比较习惯于 JSON or XML 数据存储格式，对于 Protocal Buffer 还比较陌生。Protocal Buffer 其实是 Google 出品的一种轻量并且高效的结构化数据存储格式，性能比 JSON、XML 要高很多。其实 PB 之所以性能如此好，主要得益于两个：第一，它使用 proto 编译器，自动进行序列化和反序列化，速度非常快，应该比 XML 和 JSON 快上了 20~100 倍；第二，它的数据压缩效果好，就是说它序列化后的数据量体积小。因为体积小，传输起来带宽和速度上会有优化。","link":"/2019/01/18/dubbo-%E6%94%AF%E6%8C%81%E5%93%AA%E4%BA%9B%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE%EF%BC%9F%E6%94%AF%E6%8C%81%E5%93%AA%E4%BA%9B%E5%BA%8F%E5%88%97%E5%8C%96%E5%8D%8F%E8%AE%AE%EF%BC%9F/"},{"title":"java防止接口被篡改--接口签名(Signature）","text":"前言 在为第三方系统提供接口的时候，肯定要考虑接口数据的安全问题，比如数据是否被篡改，数据是否已经过时，数据是否可以重复提交等问题。其中我认为最终要的还是数据是否被篡改。在此分享一下我的关于接口签名的实践方案。 签名规则 1、线下分配appid和appsecret，针对不同的调用方分配不同的appid和appsecret 2、加入timestamp（时间戳），10分钟内数据有效 3、加入流水号nonce（防止重复提交），至少为10位。针对查询接口，流水号只用于日志落地，便于后期日志核查。 针对办理类接口需校验流水号在有效期内的唯一性，以避免重复请求。 4、加入signature，所有数据的签名信息。以上红色字段放在请求头中。 签名的生成 signature 字段生成规则如下。 数据部分 Path：按照path中的顺序将所有value进行拼接 Query：按照key字典序排序，将所有key=value进行拼接 Form：按照key字典序排序，将所有key=value进行拼接 Body： Json: 按照key字典序排序，将所有key=value进行拼接（例如{“a”:”a”,”c”:”c”,”b”:{“e”:”e”}} =&gt; a=ab=e=ec=c） String: 整个字符串作为一个拼接 如果存在多种数据形式，则按照path、query、form、body的顺序进行再拼接，得到所有数据的拼接值。 上述拼接的值记作 Y。 请求头部分 X=”appid=xxxnonce=xxxtimestamp=xxx” 生成签名 最终拼接值=XY 最后将最终拼接值按照如下方法进行加密得到签名。 signature=org.apache.commons.codec.digest.HmacUtils.hmacSha256Hex(app secret, 拼接的值); 签名算法实现指定哪些接口或者哪些实体需要进行签名1234567891011121314151617import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.METHOD;import static java.lang.annotation.ElementType.TYPE;import static java.lang.annotation.RetentionPolicy.RUNTIME;@Target({TYPE, METHOD})@Retention(RUNTIME)@Documentedpublic @interface Signature { String ORDER_SORT = &quot;ORDER_SORT&quot;;//按照order值排序 String ALPHA_SORT = &quot;ALPHA_SORT&quot;;//字典序排序 boolean resubmit() default true;//允许重复请求 String sort() default Signature.ALPHA_SORT;} 指定哪些字段需要进行签名1234567891011121314151617181920import java.lang.annotation.Documented;import java.lang.annotation.Retention;import java.lang.annotation.Target;import static java.lang.annotation.ElementType.FIELD;import static java.lang.annotation.RetentionPolicy.RUNTIME;@Target({FIELD})@Retention(RUNTIME)@Documentedpublic @interface SignatureField { //签名顺序 int order() default 0; //字段name自定义值 String customName() default &quot;&quot;; //字段value自定义值 String customValue() default &quot;&quot;;} 核心算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * 生成所有注有 SignatureField属性 key=value的 拼接 */public static String toSplice(Object object) { if (Objects.isNull(object)) { return StringUtils.EMPTY; } if (isAnnotated(object.getClass(), Signature.class)) { Signature sg = findAnnotation(object.getClass(), Signature.class); switch (sg.sort()) { case Signature.ALPHA_SORT: return alphaSignature(object); case Signature.ORDER_SORT: return orderSignature(object); default: return alphaSignature(object); } } return toString(object);}private static String alphaSignature(Object object) { StringBuilder result = new StringBuilder(); Map&lt;String, String&gt; map = new TreeMap&lt;&gt;(); for (Field field : getAllFields(object.getClass())) { if (field.isAnnotationPresent(SignatureField.class)) { field.setAccessible(true); try { if (isAnnotated(field.getType(), Signature.class)) { if (!Objects.isNull(field.get(object))) { map.put(field.getName(), toSplice(field.get(object))); } } else { SignatureField sgf = field.getAnnotation(SignatureField.class); if (StringUtils.isNotEmpty(sgf.customValue()) || !Objects.isNull(field.get(object))) { map.put(StringUtils.isNotBlank(sgf.customName()) ? sgf.customName() : field.getName() , StringUtils.isNotEmpty(sgf.customValue()) ? sgf.customValue() : toString(field.get(object))); } } } catch (Exception e) { LOGGER.error(&quot;签名拼接(alphaSignature)异常&quot;, e); } } } for (Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = map.entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;String, String&gt; entry = iterator.next(); result.append(entry.getKey()).append(&quot;=&quot;).append(entry.getValue()); if (iterator.hasNext()) { result.append(DELIMETER); } } return result.toString();}/** * 针对array, collection, simple property, map做处理 */private static String toString(Object object) { Class&lt;?&gt; type = object.getClass(); if (BeanUtils.isSimpleProperty(type)) { return object.toString(); } if (type.isArray()) { StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; Array.getLength(object); ++i) { sb.append(toSplice(Array.get(object, i))); } return sb.toString(); } if (ClassUtils.isAssignable(Collection.class, type)) { StringBuilder sb = new StringBuilder(); for (Iterator&lt;?&gt; iterator = ((Collection&lt;?&gt;) object).iterator(); iterator.hasNext(); ) { sb.append(toSplice(iterator.next())); if (iterator.hasNext()) { sb.append(DELIMETER); } } return sb.toString(); } if (ClassUtils.isAssignable(Map.class, type)) { StringBuilder sb = new StringBuilder(); for (Iterator&lt;? extends Map.Entry&lt;String, ?&gt;&gt; iterator = ((Map&lt;String, ?&gt;) object).entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;String, ?&gt; entry = iterator.next(); if (Objects.isNull(entry.getValue())) { continue; } sb.append(entry.getKey()).append(&quot;=&quot;).append(toSplice(entry.getValue())); if (iterator.hasNext()) { sb.append(DELIMETER); } } return sb.toString(); } return NOT_FOUND;} 签名的校验header中的参数如下 签名实体123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import com.google.common.base.MoreObjects;import com.google.common.collect.Sets;import org.hibernate.validator.constraints.NotBlank;import java.util.Set;@ConfigurationProperties(prefix = &quot;wmhopenapi.validate&quot;, exceptionIfInvalid = false)@Signaturepublic class SignatureHeaders { public static final String SIGNATURE_HEADERS_PREFIX = &quot;wmhopenapi-validate&quot;; public static final Set&lt;String&gt; HEADER_NAME_SET = Sets.newHashSet(); private static final String HEADER_APPID = SIGNATURE_HEADERS_PREFIX + &quot;-appid&quot;; private static final String HEADER_TIMESTAMP = SIGNATURE_HEADERS_PREFIX + &quot;-timestamp&quot;; private static final String HEADER_NONCE = SIGNATURE_HEADERS_PREFIX + &quot;-nonce&quot;; private static final String HEADER_SIGNATURE = SIGNATURE_HEADERS_PREFIX + &quot;-signature&quot;; static { HEADER_NAME_SET.add(HEADER_APPID); HEADER_NAME_SET.add(HEADER_TIMESTAMP); HEADER_NAME_SET.add(HEADER_NONCE); HEADER_NAME_SET.add(HEADER_SIGNATURE); } /** * 线下分配的值 * 客户端和服务端各自保存appId对应的appSecret */ @NotBlank(message = &quot;Header中缺少&quot; + HEADER_APPID) @SignatureField private String appid; /** * 线下分配的值 * 客户端和服务端各自保存，与appId对应 */ @SignatureField private String appsecret; /** * 时间戳，单位: ms */ @NotBlank(message = &quot;Header中缺少&quot; + HEADER_TIMESTAMP) @SignatureField private String timestamp; /** * 流水号【防止重复提交】; (备注：针对查询接口，流水号只用于日志落地，便于后期日志核查； 针对办理类接口需校验流水号在有效期内的唯一性，以避免重复请求) */ @NotBlank(message = &quot;Header中缺少&quot; + HEADER_NONCE) @SignatureField private String nonce; /** * 签名 */ @NotBlank(message = &quot;Header中缺少&quot; + HEADER_SIGNATURE) private String signature; public String getAppid() { return appid; } public void setAppid(String appid) { this.appid = appid; } public String getAppsecret() { return appsecret; } public void setAppsecret(String appsecret) { this.appsecret = appsecret; } public String getTimestamp() { return timestamp; } public void setTimestamp(String timestamp) { this.timestamp = timestamp; } public String getNonce() { return nonce; } public void setNonce(String nonce) { this.nonce = nonce; } public String getSignature() { return signature; } public void setSignature(String signature) { this.signature = signature; } @Override public String toString() { return MoreObjects.toStringHelper(this) .add(&quot;appid&quot;, appid) .add(&quot;appsecret&quot;, appsecret) .add(&quot;timestamp&quot;, timestamp) .add(&quot;nonce&quot;, nonce) .add(&quot;signature&quot;, signature) .toString(); }} 根据request 中 header值生成SignatureHeaders实体12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private SignatureHeaders generateSignatureHeaders(Signature signature, HttpServletRequest request) throws Exception {//NOSONAR Map&lt;String, Object&gt; headerMap = Collections.list(request.getHeaderNames()) .stream() .filter(headerName -&gt; SignatureHeaders.HEADER_NAME_SET.contains(headerName)) .collect(Collectors.toMap(headerName -&gt; headerName.replaceAll(&quot;-&quot;, &quot;.&quot;), headerName -&gt; request.getHeader(headerName))); PropertySource propertySource = new MapPropertySource(&quot;signatureHeaders&quot;, headerMap); SignatureHeaders signatureHeaders = RelaxedConfigurationBinder.with(SignatureHeaders.class) .setPropertySources(propertySource) .doBind(); Optional&lt;String&gt; result = ValidatorUtils.validateResultProcess(signatureHeaders); if (result.isPresent()) { throw new ServiceException(&quot;WMH5000&quot;, result.get()); } //从配置中拿到appid对应的appsecret String appSecret = limitConstants.getSignatureLimit().get(signatureHeaders.getAppid()); if (StringUtils.isBlank(appSecret)) { LOGGER.error(&quot;未找到appId对应的appSecret, appId=&quot; + signatureHeaders.getAppid()); throw new ServiceException(&quot;WMH5002&quot;); } //其他合法性校验 Long now = System.currentTimeMillis(); Long requestTimestamp = Long.parseLong(signatureHeaders.getTimestamp()); if ((now - requestTimestamp) &gt; EXPIRE_TIME) { String errMsg = &quot;请求时间超过规定范围时间10分钟, signature=&quot; + signatureHeaders.getSignature(); LOGGER.error(errMsg); throw new ServiceException(&quot;WMH5000&quot;, errMsg); } String nonce = signatureHeaders.getNonce(); if (nonce.length() &lt; 10) { String errMsg = &quot;随机串nonce长度最少为10位, nonce=&quot; + nonce; LOGGER.error(errMsg); throw new ServiceException(&quot;WMH5000&quot;, errMsg); } if (!signature.resubmit()) { String existNonce = redisCacheService.getString(nonce); if (StringUtils.isBlank(existNonce)) { redisCacheService.setString(nonce, nonce); redisCacheService.expire(nonce, (int) TimeUnit.MILLISECONDS.toSeconds(RESUBMIT_DURATION)); } else { String errMsg = &quot;不允许重复请求, nonce=&quot; + nonce; LOGGER.error(errMsg); throw new ServiceException(&quot;WMH5000&quot;, errMsg); } } //设置appsecret signatureHeaders.setAppsecret(appSecret); return signatureHeaders;} 生成签名前需要几个步骤，如下。 （1）、appid是否合法 （2）、根据appid从配置中心中拿到appsecret （3）、请求是否已经过时，默认10分钟 （4）、随机串是否合法 （5）、是否允许重复请求 生成header信息参数拼接1String headersToSplice = SignatureUtils.toSplice(signatureHeaders); 生成header中的参数，mehtod中的参数的拼接1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private List&lt;String&gt; generateAllSplice(Method method, Object[] args, String headersToSplice) { List&lt;String&gt; pathVariables = Lists.newArrayList(), requestParams = Lists.newArrayList(); String beanParams = StringUtils.EMPTY; for (int i = 0; i &lt; method.getParameterCount(); ++i) { MethodParameter mp = new MethodParameter(method, i); boolean findSignature = false; for (Annotation anno : mp.getParameterAnnotations()) { if (anno instanceof PathVariable) { if (!Objects.isNull(args[i])) { pathVariables.add(args[i].toString()); } findSignature = true; } else if (anno instanceof RequestParam) { RequestParam rp = (RequestParam) anno; String name = mp.getParameterName(); if (StringUtils.isNotBlank(rp.name())) { name = rp.name(); } if (!Objects.isNull(args[i])) { List&lt;String&gt; values = Lists.newArrayList(); if (args[i].getClass().isArray()) { //数组 for (int j = 0; j &lt; Array.getLength(args[i]); ++j) { values.add(Array.get(args[i], j).toString()); } } else if (ClassUtils.isAssignable(Collection.class, args[i].getClass())) { //集合 for (Object o : (Collection&lt;?&gt;) args[i]) { values.add(o.toString()); } } else { //单个值 values.add(args[i].toString()); } values.sort(Comparator.naturalOrder()); requestParams.add(name + &quot;=&quot; + StringUtils.join(values)); } findSignature = true; } else if (anno instanceof RequestBody || anno instanceof ModelAttribute) { beanParams = SignatureUtils.toSplice(args[i]); findSignature = true; } if (findSignature) { break; } } if (!findSignature) { LOGGER.info(String.format(&quot;签名未识别的注解, method=%s, parameter=%s, annotations=%s&quot;, method.getName(), mp.getParameterName(), StringUtils.join(mp.getMethodAnnotations()))); } } List&lt;String&gt; toSplices = Lists.newArrayList(); toSplices.add(headersToSplice); toSplices.addAll(pathVariables); requestParams.sort(Comparator.naturalOrder()); toSplices.addAll(requestParams); toSplices.add(beanParams); return toSplices;} 对最终的拼接结果重新生成签名信息1SignatureUtils.signature(allSplice.toArray(new String[]{}), signatureHeaders.getAppsecret()); 依赖第三方工具包12345678&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt;&lt;/dependency&gt; 使用示例生成签名12345678910111213//初始化请求头信息SignatureHeaders signatureHeaders = new SignatureHeaders();signatureHeaders.setAppid(&quot;111&quot;);signatureHeaders.setAppsecret(&quot;222&quot;);signatureHeaders.setNonce(SignatureUtils.generateNonce());signatureHeaders.setTimestamp(String.valueOf(System.currentTimeMillis()));List&lt;String&gt; pathParams = new ArrayList&lt;&gt;();//初始化path中的数据pathParams.add(SignatureUtils.encode(&quot;18237172801&quot;, signatureHeaders.getAppsecret()));//调用签名工具生成签名signatureHeaders.setSignature(SignatureUtils.signature(signatureHeaders, pathParams, null, null));System.out.println(&quot;签名数据: &quot; + signatureHeaders);System.out.println(&quot;请求数据: &quot; + pathParams); 输出结果123拼接结果: appid=111^_^appsecret=222^_^nonce=c9e778ba668c8f6fedf35634eb493af6304d54392d990262d9e7c1960b475b67^_^timestamp=1538207443910^_^w8rAwcXDxcDKwsM=^_^签名数据: SignatureHeaders{appid=111, appsecret=222, timestamp=1538207443910, nonce=c9e778ba668c8f6fedf35634eb493af6304d54392d990262d9e7c1960b475b67, signature=0a7d0b5e802eb5e52ac0cfcd6311b0faba6e2503a9a8d1e2364b38617877574d}请求数据: [w8rAwcXDxcDKwsM=] 此文引自 https://www.cnblogs.com/hujunzheng/p/9725168.html","link":"/2019/05/14/java%E9%98%B2%E6%AD%A2%E6%8E%A5%E5%8F%A3%E8%A2%AB%E7%AF%A1%E6%94%B9-%E6%8E%A5%E5%8F%A3%E7%AD%BE%E5%90%8D-Signature%EF%BC%89/"},{"title":"java防止接口被篡改--接口签名(简单版本）续","text":"一、前言 此次来说一下另一种简单粗暴的签名方案。相对于之前的签名方案，对body、paramenter、path variable的获取都做了简化的处理。也就是说这种方式针所有数据进行了签名，并不能指定某些数据进行签名。 二、签名规则 1、线下分配appid和appsecret，针对不同的调用方分配不同的appid和appsecret 2、加入timestamp（时间戳），10分钟内数据有效 3、加入流水号nonce（防止重复提交），至少为10位。针对查询接口，流水号只用于日志落地，便于后期日志核查。 针对办理类接口需校验流水号在有效期内的唯一性，以避免重复请求。 4、加入signature，所有数据的签名信息。 以上红色字段放在请求头中。 三、签名的生成 signature字段生成规则如下。 1、数据部分 Path Variable：按照path中的字典顺序将所有value进行拼接 Parameter：按照key=values（多个value按照字典顺序拼接）字典顺序进行拼接 Body：从request inputstream中获取保存为String形式 如果存在多种数据形式，则按照body、parameter、path variable的顺序进行再拼接，得到所有数据的拼接值。 上述拼接的值记作 Y。 2、请求头部分 X=”appid=xxxnonce=xxxtimestamp=xxx” 3、生成签名 最终拼接值=XY 最后将最终拼接值按照如下方法进行加密得到签名。 signature=org.apache.commons.codec.digest.HmacUtils.AesEncodeUtil(app secret, 拼接的值); 四、签名算法实现 注：省去了X=”appid=xxxnonce=xxxtimestamp=xxx”这部分。1、自定义Request对象为什么要自定义request对象，因为我们要获取request inputstream（默认只能获取一次） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class BufferedHttpServletRequest extends HttpServletRequestWrapper { private ByteBuf buffer; private final AtomicBoolean isCached = new AtomicBoolean(); public BufferedHttpServletRequest(HttpServletRequest request, int initialCapacity) { super(request); int contentLength = request.getContentLength(); int min = Math.min(initialCapacity, contentLength); if (min &lt; 0) { buffer = Unpooled.buffer(0); } else { buffer = Unpooled.buffer(min, contentLength); } } @Override public ServletInputStream getInputStream() throws IOException { //Only returning data from buffer if it is readonly, which means the underlying stream is EOF or closed. if (isCached.get()) { return new NettyServletInputStream(buffer); } return new ContentCachingInputStream(super.getInputStream()); } public void release() { buffer.release(); } private class ContentCachingInputStream extends ServletInputStream { private final ServletInputStream is; public ContentCachingInputStream(ServletInputStream is) { this.is = is; } @Override public int read() throws IOException { int ch = this.is.read(); if (ch != -1) { //Stream is EOF, set this buffer to readonly state buffer.writeByte(ch); } else { isCached.compareAndSet(false, true); } return ch; } @Override public void close() throws IOException { //Stream is closed, set this buffer to readonly state try { is.close(); } finally { isCached.compareAndSet(false, true); } } @Override public boolean isFinished() { throw new UnsupportedOperationException(&quot;Not yet implemented!&quot;); } @Override public boolean isReady() { throw new UnsupportedOperationException(&quot;Not yet implemented!&quot;); } @Override public void setReadListener(ReadListener readListener) { throw new UnsupportedOperationException(&quot;Not yet implemented!&quot;); } }} 替换默认的request对象123456789101112131415@Configurationpublic class FilterConfig { @Bean public RequestCachingFilter requestCachingFilter() { return new RequestCachingFilter(); } @Bean public FilterRegistrationBean requestCachingFilterRegistration( RequestCachingFilter requestCachingFilter) { FilterRegistrationBean bean = new FilterRegistrationBean(requestCachingFilter); bean.setOrder(1); return bean; }} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class RequestCachingFilter extends OncePerRequestFilter { private static Logger LOGGER = LoggerFactory.getLogger(RequestCachingFilter.class); @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { boolean isFirstRequest = !isAsyncDispatch(request); HttpServletRequest requestToUse = request; if (isFirstRequest &amp;&amp; !(request instanceof BufferedHttpServletRequest)) { requestToUse = new BufferedHttpServletRequest(request, 1024); } try { filterChain.doFilter(requestToUse, response); } catch (Exception e) { LOGGER.error(&quot;RequestCachingFilter&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;, e); } finally { this.printRequest(requestToUse); if (requestToUse instanceof BufferedHttpServletRequest) { ((BufferedHttpServletRequest) requestToUse).release(); } } } private void printRequest(HttpServletRequest request) { String body = StringUtils.EMPTY; try { if (request instanceof BufferedHttpServletRequest) { body = IOUtils.toString(request.getInputStream(), StandardCharsets.UTF_8); } } catch (IOException e) { LOGGER.error(&quot;printRequest 获取body异常...&quot;, e); } JSONObject requestJ = new JSONObject(); JSONObject headers = new JSONObject(); Collections.list(request.getHeaderNames()) .stream() .forEach(name -&gt; headers.put(name, request.getHeader(name))); requestJ.put(&quot;headers&quot;, headers); requestJ.put(&quot;parameters&quot;, request.getParameterMap()); requestJ.put(&quot;body&quot;, body); requestJ.put(&quot;remote-user&quot;, request.getRemoteUser()); requestJ.put(&quot;remote-addr&quot;, request.getRemoteAddr()); requestJ.put(&quot;remote-host&quot;, request.getRemoteHost()); requestJ.put(&quot;remote-port&quot;, request.getRemotePort()); requestJ.put(&quot;uri&quot;, request.getRequestURI()); requestJ.put(&quot;url&quot;, request.getRequestURL()); requestJ.put(&quot;servlet-path&quot;, request.getServletPath()); requestJ.put(&quot;method&quot;, request.getMethod()); requestJ.put(&quot;query&quot;, request.getQueryString()); requestJ.put(&quot;path-info&quot;, request.getPathInfo()); requestJ.put(&quot;context-path&quot;, request.getContextPath()); LOGGER.info(&quot;Request-Info: &quot; + JSON.toJSONString(requestJ, SerializerFeature.PrettyFormat)); }} 2、签名切面1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Aspect@Componentpublic class SignatureAspect { private static final Logger LOGGER = LoggerFactory.getLogger(StringUtils.class); @Around(&quot;execution(* com..controller..*.*(..)) &quot; + &quot;&amp;&amp; (@annotation(org.springframework.web.bind.annotation.RequestMapping)&quot; + &quot;|| @annotation(org.springframework.web.bind.annotation.GetMapping)&quot; + &quot;|| @annotation(org.springframework.web.bind.annotation.PostMapping)&quot; + &quot;|| @annotation(org.springframework.web.bind.annotation.DeleteMapping)&quot; + &quot;|| @annotation(org.springframework.web.bind.annotation.PatchMapping))&quot; ) public Object doAround(ProceedingJoinPoint pjp) throws Throwable { try { this.checkSign(); return pjp.proceed(); } catch (Throwable e) { LOGGER.error(&quot;SignatureAspect&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;, e); throw e; } } private void checkSign() throws Exception { HttpServletRequest request = ((ServletRequestAttributes) (RequestContextHolder.currentRequestAttributes())).getRequest(); String oldSign = request.getHeader(&quot;X-SIGN&quot;); if (StringUtils.isBlank(oldSign)) { throw new RuntimeException(&quot;取消签名Header[X-SIGN]信息&quot;); } //获取body（对应@RequestBody） String body = null; if (request instanceof BufferedHttpServletRequest) { body = IOUtils.toString(request.getInputStream(), StandardCharsets.UTF_8); } //获取parameters（对应@RequestParam） Map&lt;String, String[]&gt; params = null; if (!CollectionUtils.isEmpty(request.getParameterMap())) { params = request.getParameterMap(); } //获取path variable（对应@PathVariable） String[] paths = null; ServletWebRequest webRequest = new ServletWebRequest(request, null); Map&lt;String, String&gt; uriTemplateVars = (Map&lt;String, String&gt;) webRequest.getAttribute( HandlerMapping.URI_TEMPLATE_VARIABLES_ATTRIBUTE, RequestAttributes.SCOPE_REQUEST); if (!CollectionUtils.isEmpty(uriTemplateVars)) { paths = uriTemplateVars.values().toArray(new String[]{}); } try { String newSign = SignUtil.sign(body, params, paths); if (!newSign.equals(oldSign)) { throw new RuntimeException(&quot;签名不一致...&quot;); } } catch (Exception e) { throw new RuntimeException(&quot;验签出错...&quot;, e); } }} 分别获取了request inputstream中的body信息、parameter信息、path variable信息。签名核心工具类 12345678910111213141516171819202122232425262728293031323334353637383940414243public class SignUtil { private static final String DEFAULT_SECRET = &quot;1qaz@WSX#$%&amp;&quot;; public static String sign(String body, Map&lt;String, String[]&gt; params, String[] paths) { StringBuilder sb = new StringBuilder(); if (StringUtils.isNotBlank(body)) { sb.append(body).append('#'); } if (!CollectionUtils.isEmpty(params)) { params.entrySet() .stream() .sorted(Map.Entry.comparingByKey()) .forEach(paramEntry -&gt; { String paramValue = String.join(&quot;,&quot;, Arrays.stream(paramEntry.getValue()).sorted().toArray(String[]::new)); sb.append(paramEntry.getKey()).append(&quot;=&quot;).append(paramValue).append('#'); }); } if (ArrayUtils.isNotEmpty(paths)) { String pathValues = String.join(&quot;,&quot;, Arrays.stream(paths).sorted().toArray(String[]::new)); sb.append(pathValues); } String createSign = HmacUtils.hmacSha256Hex(DEFAULT_SECRET, sb.toString()); return createSign; } public static void main(String[] args) { String body = &quot;{\\n&quot; + &quot;\\t\\&quot;name\\&quot;: \\&quot;hjzgg\\&quot;,\\n&quot; + &quot;\\t\\&quot;age\\&quot;: 26\\n&quot; + &quot;}&quot;; Map&lt;String, String[]&gt; params = new HashMap&lt;&gt;(); params.put(&quot;var3&quot;, new String[]{&quot;3&quot;}); params.put(&quot;var4&quot;, new String[]{&quot;4&quot;}); String[] paths = new String[]{&quot;1&quot;, &quot;2&quot;}; System.out.println(sign(body, params, paths)); }} 五、签名验证 简单写了一个包含body参数，parameter参数，path variable参数的controller 123456789101112131415161718192021222324252627282930313233343536373839404142@RestController@RequestMapping(&quot;example&quot;)public class ExampleController { @PostMapping(value = &quot;test/{var1}/{var2}&quot;, produces = MediaType.ALL_VALUE) public String myController(@PathVariable String var1 , @PathVariable String var2 , @RequestParam String var3 , @RequestParam String var4 , @RequestBody User user) { return String.join(&quot;,&quot;, var1, var2, var3, var4, user.toString()); } private static class User { private String name; private int age; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } @Override public String toString() { return new ToStringBuilder(this) .append(&quot;name&quot;, name) .append(&quot;age&quot;, age) .toString(); } }} 通过 签名核心工具类SignUtil 的main方法生成一个签名，通过如下命令验证 12345678curl -X POST \\ 'http://localhost:8080/example/test/1/2?var3=3&amp;var4=4' \\ -H 'Content-Type: application/json' \\ -H 'X-SIGN: 4955125a3aa2782ab3def51dc958a34ca46e5dbb345d8808590fb53e81cc2687' \\ -d '{ &quot;name&quot;: &quot;hjzgg&quot;, &quot;age&quot;: 26}' 此文引自 https://www.cnblogs.com/hujunzheng/p/10178584.html","link":"/2019/05/14/java%E9%98%B2%E6%AD%A2%E6%8E%A5%E5%8F%A3%E8%A2%AB%E7%AF%A1%E6%94%B9-%E6%8E%A5%E5%8F%A3%E7%AD%BE%E5%90%8D-%E7%AE%80%E5%8D%95%E7%89%88%E6%9C%AC%EF%BC%89%E7%BB%AD/"},{"title":"jenkins","text":"Jenkins 是 Devops 神器，本篇文章介绍如何安装和使用 Jenkins 部署 Spring Boot 项目 Jenkins 搭建、部署分为四个步骤； 第一步，Jenkins 安装 第二步，插件安装和配置 第三步，Push SSH 第四步，部署项目 第一步 ，Jenkins 安装准备环境： JDK:1.8Jenkins:2.83Centos:7.3maven 3.5 Jdk 默认已经安装完成 配置 Maven版本要求 Maven3.5.0 软件下载 1wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.5.0/binaries/apache-maven-3.5.0-bin.tar.gz 安装 1234## 解压tar vxf apache-maven-3.5.0-bin.tar.gz## 移动mv apache-maven-3.5.0 /usr/local/maven3 修改环境变量，在/etc/profile中添加以下几行 123MAVEN_HOME=/usr/local/maven3export MAVEN_HOMEexport PATH=${PATH}:${MAVEN_HOME}/bin 记得执行source /etc/profile使环境变量生效。 验证最后运行mvn -v验证maven是否安装成功 配置防护墙关闭防护墙 12345678#centos7systemctl stop firewalld.service==============================#以下为：centOS 6.5关闭防火墙步骤#关闭命令： service iptables stop #永久关闭防火墙：chkconfig iptables off 两个命令同时运行，运行完成后查看防火墙关闭状态 1service iptables status Jenkins 安装下载 12cd /optwget http://mirrors.jenkins.io/war/2.83/jenkins.war 启动服务 1java -jar jenkins.war &amp; Jenkins 就启动成功了！它的war包自带Jetty服务器 第一次启动 Jenkins 时，出于安全考虑，Jenkins 会自动生成一个随机的按照口令。注意控制台输出的口令，复制下来，然后在浏览器输入密码： 12345678910111213141516INFO: ***************************************************************************************************************************************************************************************Jenkins initial setup is required. An admin user has been created and a password generated.Please use the following password to proceed to installation:0cca37389e6540c08ce6e4c96f46da0fThis may also be found at: /root/.jenkins/secrets/initialAdminPassword*************************************************************************************************************************************************************************************** 访问浏览器访问：http://localhost:8080/ 输入：0cca37389e6540c08ce6e4c96f46da0f 进入用户自定义插件界面，建议选择安装官方推荐插件，因为安装后自己也得安装: 接下来是进入插件安装进度界面: 插件一次可能不会完全安装成功，可以点击Retry再次安装。直到全部安装成功 等待一段时间之后，插件安装完成，配置用户名密码: 输入：admin/admin 系统管理-》全局工具配置 jdk路径， 第二步，插件安装和配置有很多插件都是选择的默认的安装的，所以现在需要我们安装的插件不多，Git plugin 和 Maven Integration plugin，publish over SSH。 插件安装：系统管理 &gt; 插件管理 &gt; 可选插件,勾选需要安装的插件，点击直接安装或者下载重启后安装 配置全局变量系统管理 &gt; 全局工具配置 JDK 配置本地 JDK 的路径，去掉勾选自动安装 Maven 配置本地maven的路径，去掉勾选自动安装 其它内容可以根据自己的情况选择安装。 使用密钥方式登录目标发布服务器ssh 的配置可使用密钥，也可以使用密码，这里我们使用密钥来配置，在配置之前先配置好jenkins服务器和应用服务器的密钥认证Jenkins服务器上生成密钥对，使用ssh-keygen -t rsa命令 输入下面命令 一直回车，一个矩形图形出现就说明成功，在~/.ssh/下会有私钥id_rsa和公钥id_rsa.pub 1ssh-keygen -t rsa 将jenkins服务器的公钥id_rsa.pub中的内容复制到应用服务器 的~/.ssh/下的 authorized_keys文件 12ssh-copy-id -i id_rsa.pub 192.168.0.xxchmod 644 authorized_keys 在应用服务器上重启 ssh 服务，service sshd restart现在 Jenkins 服务器可免密码直接登陆应用服务器 之后在用 ssh B尝试能否免密登录 B 服务器，如果还是提示需要输入密码，则有以下原因 a. 非 root 账户可能不支持 ssh 公钥认证（看服务器是否有限制） b. 传过来的公钥文件权限不够，可以给这个文件授权下 chmod 644 authorized_keys c. 使用 root 账户执行 ssh-copy-id -i ~/.ssh/id_rsa.pub 这个指令的时候如果需要输入密码则要配置sshd_config 123vi /etc/ssh/sshd_config#内容PermitRootLogin no 修改完后要重启 sshd 服务 1service sshd restart 最后，如果可以 SSH IP 免密登录成功说明 SSH 公钥认证成功。 上面这种方式比较复杂，其实在 Jenkins 后台直接添加操作即可，参考下面方式 使用用户名+密码方式登录目标发布服务器(1)点击”高级”展开配置 (2)配置SSH的登陆密码 配置完成后可点击“Test Configuration”测试到目标主机的连接，出现”success“则成功连接，如果有多台应用服务器，可以点击”增加“，配置多个“SSH Servers” 点击“保存”以保存配置。 第三步，Push SSH系统管理 &gt; 系统设置 选择 Publish over SSH Passphrase 不用设置Path to key 写上生成的ssh路径：/root/.ssh/id_rsa 下面的 SSH Servers 是重点 Name 随意起名代表这个服务，待会要根据它来选择 Hostname 配置应用服务器的地址 Username 配置 linux 登陆用户名 Remote Directory 不填 点击下方增加可以添加多个应用服务器的地址 jenkins+docker+nodejs项目的自动部署环境https://my.oschina.net/gaochunzhang/blog/2246923 构建环境如果没有Node选项，前往系统管理–Global Tool Configuration设置 选择构建环境： 构建环境 Build Authorization Token Root Plugin 插件使用说明此插件是用来让你的远程git发布文件通知jenkins的时候允许匿名访问，原本路径为job/NAME/build?token=SECRET。不幸的是，Jenkins按层次结构检查URI，并且访问job/NAME/ 这个的时候需要进行身份验证。 此插件提供备用URI模式，该模式不受通常的整体或作业读取权限的约束。只需发出Http GET或POST即可buildByToken/build?job=NAME&amp;token=SECRET。无论安全设置如何，匿名用户都可以访问此URI，因此您只需要正确的令牌。 Build Authorization Token Root Plugin 插件使用说明 使用例子 buildByToken/build?job=NAME&amp;token=SECRET 第四步，部署项目首页点击新建：输入项目名称 下方选择构建一个 Maven 项目，点击确定。 勾选丢弃旧的构建，选择是否备份被替换的旧包。我这里选择备份最近的10个 源码管理，选择 SVN，配置 SVN 相关信息，点击 add 可以输入 SVN 的账户和密码 SVN 地址：http://192.168.0.xx/svn/xxx@HEAD,`@HEAD`意思取最新版本 构建环境中勾选“Add timestamps to the Console Output”，代码构建的过程中会将日志打印出来 在 Build 中输入打包前的 mvn 命令，如： 1clean install -Dmaven.test.skip=true -Ptest 意思是：排除测试的包内容，使用后缀为 test 的配置文件。 Post Steps 选择 Run only if build succeeds 点击Add post-build step，选择 Send files or execute commands over SSH Name 选择上面配置的 Push SSH Source files配置:target/xxx-0.0.1-SNAPSHOT.jar 项目jar包名Remove prefix:target/Remote directory:Jenkins-in/ 代码应用服务器的目录地址，Exec command：Jenkins-in/xxx.sh 应用服务器对应的脚本。 需要在应用服务器创建文件夹：Jenkins-in，在文件夹中复制一下脚本内容：xxx.sh 12345678910111213141516171819202122232425DATE=$(date +%Y%m%d)export JAVA_HOME PATH CLASSPATHJAVA_HOME=/usr/java/jdk1.8.0_131PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATHCLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$CLASSPATHDIR=/root/xxxJARFILE=xxx-0.0.1-SNAPSHOT.jarif [ ! -d $DIR/backup ];then mkdir -p $DIR/backupficd $DIRps -ef | grep $JARFILE | grep -v grep | awk '{print $2}' | xargs kill -9mv $JARFILE backup/$JARFILE$DATEmv -f /root/Jenkins-in/$JARFILE .java -jar $JARFILE &gt; out.log &amp;if [ $? = 0 ];then sleep 30 tail -n 50 out.logficd backup/ls -lt|awk 'NR&gt;5{print $NF}'|xargs rm -rf 1234567891011121314151617181920#!/bin/bashJAR_NAME=demo-0.0.1SERVER_NAME=mmly# 项目目录JAR_PAHT=/var/jenkins_home/workspace/mmly/targetecho &quot;查询进程id--》$JAR_NAME&quot;PID=$(ps -ef | grep $JAR_NAME.jar | grep -v grep | awk '{ print $2 }')if [ -z &quot;$PID&quot; ]then echo Application is already stoppedelse echo kill $PID kill $PIDfi# echo &quot;复制jar包到执行目录：cp &quot;nohup java -jar $JAR_PAHT/$JAR_NAME.jar &gt; myout.file 2&gt;&amp;1 &amp; 这段脚本的意思，就是 kill 旧项目，删除旧项目，启动新项目，备份老项目。 全文完。 参考：https://blog.csdn.net/LLQ_200/article/details/76921487","link":"/2019/04/25/jenkins/"},{"title":"为什么处理排序的数组要比非排序的快","text":"这世上有三样东西是别人抢不走的：一是吃进胃里的食物，二是藏在心中的梦想，三是读进大脑的书 为什么处理排序的数组要比非排序的快问题 以下是**c++**的一段非常神奇的代码。由于一些奇怪原因，对数据排序后奇迹般的让这段代码快了近6倍！！ 1234567891011121314151617181920212223242526272829303132333435#include &lt;algorithm&gt;#include &lt;ctime&gt;#include &lt;iostream&gt;int main(){ // Generate data const unsigned arraySize = 32768; int data[arraySize]; for (unsigned c = 0; c &lt; arraySize; ++c) data[c] = std::rand() % 256; // !!! With this, the next loop runs faster std::sort(data, data + arraySize); // Test clock_t start = clock(); long long sum = 0; for (unsigned i = 0; i &lt; 100000; ++i) { // Primary loop for (unsigned c = 0; c &lt; arraySize; ++c) { if (data[c] &gt;= 128) sum += data[c]; } } double elapsedTime = static_cast&lt;double&gt;(clock() - start) / CLOCKS_PER_SEC; std::cout &lt;&lt; elapsedTime &lt;&lt; std::endl; std::cout &lt;&lt; &quot;sum = &quot; &lt;&lt; sum &lt;&lt; std::endl;} 没有std::sort(data, data + arraySize);,这段代码运行了11.54秒. 有这个排序的代码，则运行了1.93秒.我原以为这也许只是语言或者编译器的不一样的问题，所以我又用Java试了一下。 以下是Java代码段 123456789101112131415161718192021222324252627282930313233343536import java.util.Arrays;import java.util.Random;public class Main{ public static void main(String[] args) { // Generate data int arraySize = 32768; int data[] = new int[arraySize]; Random rnd = new Random(0); for (int c = 0; c &lt; arraySize; ++c) data[c] = rnd.nextInt() % 256; // !!! With this, the next loop runs faster Arrays.sort(data); // Test long start = System.nanoTime(); long sum = 0; for (int i = 0; i &lt; 100000; ++i) { // Primary loop for (int c = 0; c &lt; arraySize; ++c) { if (data[c] &gt;= 128) sum += data[c]; } } System.out.println((System.nanoTime() - start) / 1000000000.0); System.out.println(&quot;sum = &quot; + sum); }} 结果相似，没有很大的差别。 我首先得想法是排序把数据放到了cache中，但是我下一个想法是我之前的想法是多么傻啊，因为这个数组刚刚被构造。 到底这是为什么呢？ 为什么排序的数组会快于没有排序的数组？ 这段代码是为了求一些无关联的数据的和，排不排序应该没有关系啊。 回答什么是分支预测？看看这个铁路分岔口Image by Mecanismo, via Wikimedia Commons. Used under the CC-By-SA 3.0 license. 为了理解这个问题，想象一下，如果我们回到19世纪. 你是在分岔口的操作员。当你听到列车来了，你没办法知道这两条路哪一条是正确的。然后呢，你让列车停下来，问列车员哪条路是对的，然后你才转换铁路方向。 火车很重有很大的惯性。所以他们得花费很长的时间开车和减速。 是不是有个更好的办法呢？你猜测哪个是火车正确的行驶方向 如果你猜对了，火车继续前行 如果你猜错了，火车得停下来，返回去，然后你再换条路。 如果你每次都猜对了，那么火车永远不会停下来。如果你猜错太多次，那么火车会花费很多时间来停车，返回，然后再启动 考虑一个if条件语句：在处理器层面上，这是一个分支指令：当处理器看到这个分支时，没办法知道哪个将是下一条指令。该怎么办呢？貌似只能暂停执行，直到前面的指令完成，然后再继续执行正确的下一条指令？现代处理器很复杂，因此它需要很长的时间”热身”、”冷却” 是不是有个更好的办法呢？你猜测下一个指令在哪！ 如果你猜对了，你继续执行。 如果你猜错了，你需要flush the pipeline，返回到那个出错的分支，然后你才能继续。 如果你每次都猜对了，那么你永远不会停如果你猜错了太多次，你就要花很多时间来滚回，重启。 这就是分支预测。我承认这不是一个好的类比，因为火车可以用旗帜来作为方向的标识。但是在电脑中，处理器不能知道哪一个分支将走到最后。 所以怎样能很好的预测，尽可能地使火车必须返回的次数变小？你看看火车之前的选择过程，如果这个火车往左的概率是99%。那么你猜左，反之亦然。如果每3次会有1次走这条路，那么你也按这个三分之一的规律进行。 换句话说，你试着定下一个模式，然后按照这个模式去执行。这就差不多是分支预测是怎么工作的。 大多数的应用都有很好的分支预测。所以现代的分支预测器通常能实现大于90%的命中率。但是当面对没有模式识别、无法预测的分支，那分支预测基本就没用了。 如果你想知道更多:Branch predictor” article on Wikipedia. 有了前面的说明，问题的来源就是这个if条件判断语句12if (data[c] &gt;= 128) sum += data[c]; 注意到数据是分布在0到255之间的。当数据排好序后，基本上前一半大的的数据不会进入这个条件语句，而后一半的数据，会进入该条件语句. 连续的进入同一个执行分支很多次，这对分支预测是非常友好的。可以更准确地预测，从而带来更高的执行效率。 快速理解一下1234567T = branch takenN = branch not takendata[] = 0, 1, 2, 3, 4, ... 126, 127, 128, 129, 130, ... 250, 251, 252, ...branch = N N N N N ... N N T T T ... T T T ... = NNNNNNNNNNNN ... NNNNNNNTTTTTTTTT ... TTTTTTTTTT (easy to predict) 但是当数据是完全随机的，分支预测就没什么用了。因为他无法预测随机的数据。因此就会有大概50%的概率预测出错。 1234data[] = 226, 185, 125, 158, 198, 144, 217, 79, 202, 118, 14, 150, 177, 182, 133, ...branch = T, T, N, T, T, T, T, N, T, N, N, T, T, T, N ... = TTNTTTTNTNNTTTN ... (completely random - hard to predict) 我们能做些什么呢如果编译器无法优化带条件的分支，如果你愿意牺牲代码的可读性换来更好的性能的话，你可以用下面的一些技巧。 把 12if (data[c] &gt;= 128) sum += data[c]; 替换成 12int t = (data[c] - 128) &gt;&gt; 31;sum += ~t &amp; data[c]; 这消灭了分支，把它替换成按位操作. （说明：这个技巧不是非常严格的等同于原来的if条件语句。但是在data[]当前这些值下是OK的） 使用的设备参数是：Core i7 920 @ 3.5 GHzC++ - Visual Studio 2010 - x64 Release 1234567891011// Branch - Randomseconds = 11.777// Branch - Sortedseconds = 2.352// Branchless - Randomseconds = 2.564// Branchless - Sortedseconds = 2.587 Java - Netbeans 7.1.1 JDK 7 - x64 1234567891011// Branch - Randomseconds = 10.93293813// Branch - Sortedseconds = 5.643797077// Branchless - Randomseconds = 3.113581453// Branchless - Sortedseconds = 3.186068823 结论： 用了分支(if)：没有排序和排序的数据，效率有很大的区别 用了上面提到的按位操作替换：排序与否，效率没有很大的区别 在使用C++的情况下，按位操作还是要比排好序的分支操作要慢。 一般的建议是尽量避免在关键循环上出现对数据很依赖的分支。（就像这个例子） 更新： GCC 4.6.1 用了 -O3 or -ftree-vectorize，在64位机器上，数据有没有排序，都是一样快。 ………等各种例子 说明了现代编译器越发成熟强大，可以在这方面充分优化代码的执行效率 相关内容CPU的流水线指令执行 想象现在有一堆指令等待CPU去执行，那么CPU是如何执行的呢？具体的细节可以找一本计算机组成原理来看。CPU执行一堆指令时，并不是单纯地一条一条取出来执行，而是按照一种流水线的方式，在CPU真正指令前，这条指令就像工厂里流水线生产的产品一样，已经被经过一些处理。简单来说，一条指令可能经过过程：取指(Fetch)、解码(Decode)、执行(Execute)、放回(Write-back)。 假设现在有指令序列ABCDEFG。当CPU正在执行(execute)指令A时，CPU的其他处理单元（CPU是由若干部件构成的）其实已经预先处理到了指令A后面的指令，例如B可能已经被解码，C已经被取指。这就是流水线执行，这可以保证CPU高效地执行指令。 分支预测 如上所说，CPU在执行一堆顺序执行的指令时，因为对于执行指令的部件来说，其基本不需要等待，因为诸如取指、解码这些过程早就被做了。但是，当CPU面临非顺序执行的指令序列时，例如之前提到的跳转指令，情况会怎样呢？ 取指、解码这些CPU单元并不知道程序流程会跳转，只有当CPU执行到跳转指令本身时，才知道该不该跳转。所以，取指解码这些单元就会继续取跳转指令之后的指令。当CPU执行到跳转指令时，如果真的发生了跳转，那么之前的预处理（取指、解码）就白做了。这个时候，CPU得从跳转目标处临时取指、解码，然后才开始执行，这意味着：CPU停了若干个时钟周期！ 这其实是个问题，如果CPU的设计放任这个问题，那么其速度就很难提升起来。为此，人们发明了一种技术，称为branch prediction，也就是分支预测。分支预测的作用，就是预测某个跳转指令是否会跳转。而CPU就根据自己的预测到目标地址取指令。这样，即可从一定程度提高运行速度。当然，分支预测在实现上有很多方法。 stackoverflow链接： 这个问题的所有回答中，最高的回答，获取了上万个vote，还有很多个回答，非常疯狂，大家觉得不过瘾可以移步到这里查看 http://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-an-unsorted-array 引用于 https://github.com/giantray/stackoverflow-java-top-qa 欢迎关注 http://yunlongn.github.io","link":"/2019/05/22/%E4%B8%BA%E4%BB%80%E4%B9%88%E5%A4%84%E7%90%86%E6%8E%92%E5%BA%8F%E7%9A%84%E6%95%B0%E7%BB%84%E8%A6%81%E6%AF%94%E9%9D%9E%E6%8E%92%E5%BA%8F%E7%9A%84%E5%BF%AB/"},{"title":"为什么说Java中只有值传递","text":"为什么说Java中只有值传递对于初学者来说，要想把这个问题回答正确，是比较难的。在第二天整理答案的时候，我发现我竟然无法通过简单的语言把这个事情描述的很容易理解，遗憾的是，我也没有在网上找到哪篇文章可以把这个事情讲解的通俗易懂。所以，就有了我写这篇文章的初衷。这篇文章中，我从什么是方法的实际参数和形式参数开始，给你讲解为什么说Java中只有值传递。 辟谣时间关于这个问题，在StackOverflow上也引发过广泛的讨论，看来很多程序员对于这个问题的理解都不尽相同，甚至很多人理解的是错误的。还有的人可能知道Java中的参数传递是值传递，但是说不出来为什么。 在开始深入讲解之前，有必要纠正一下大家以前的那些错误看法了。如果你有以下想法，那么你有必要好好阅读本文。 错误理解一：值传递和引用传递，区分的条件是传递的内容，如果是个值，就是值传递。如果是个引用，就是引用传递。 错误理解二：Java是引用传递。 错误理解三：传递的参数如果是普通类型，那就是值传递，如果是对象，那就是引用传递。 实参与形参我们都知道，在Java中定义方法的时候是可以定义参数的。比如Java中的main方法，public static void main(String[] args)，这里面的args就是参数。参数在程序语言中分为形式参数和实际参数。 形式参数：是在定义函数名和函数体的时候使用的参数,目的是用来接收调用该函数时传入的参数。 实际参数：在调用有参函数时，主调函数和被调函数之间有数据传递关系。在主调函数中调用一个函数时，函数名后面括号中的参数称为“实际参数”。 简单举个例子： 12345678public static void main(String[] args) { ParamTest pt = new ParamTest(); pt.sout(&quot;Hollis&quot;);//实际参数为 Hollis}public void sout(String name) { //形式参数为 name System.out.println(name);} 实际参数是调用有参方法的时候真正传递的内容，而形式参数是用于接收实参内容的参数。 值传递与引用传递上面提到了，当我们调用一个有参函数的时候，会把实际参数传递给形式参数。但是，在程序语言中，这个传递过程中传递的两种情况，即值传递和引用传递。我们来看下程序语言中是如何定义和区分值传递和引用传递的。 值传递（pass by value）是指在调用函数时将实际参数复制一份传递到函数中，这样在函数中如果对参数进行修改，将不会影响到实际参数。 引用传递（pass by reference）是指在调用函数时将实际参数的地址直接传递到函数中，那么在函数中对参数所进行的修改，将影响到实际参数。 有了上面的概念，然后大家就可以写代码实践了，来看看Java中到底是值传递还是引用传递 ，于是，最简单的一段代码出来了： 123456789101112public static void main(String[] args) { ParamTest pt = new ParamTest(); int i = 10; pt.pass(10); System.out.println(&quot;print in main , i is &quot; + i);}public void pass(int j) { j = 20; System.out.println(&quot;print in pass , j is &quot; + j);} 上面的代码中，我们在pass方法中修改了参数j的值，然后分别在pass方法和main方法中打印参数的值。输出结果如下： 12print in pass , j is 20print in main , i is 10 可见，pass方法内部对name的值的修改并没有改变实际参数i的值。那么，按照上面的定义，有人得到结论：Java的方法传递是值传递。 但是，很快就有人提出质疑了（哈哈，所以，不要轻易下结论咯。）。然后，他们会搬出以下代码： 1234567891011121314public static void main(String[] args) { ParamTest pt = new ParamTest(); User hollis = new User(); hollis.setName(&quot;Hollis&quot;); hollis.setGender(&quot;Male&quot;); pt.pass(hollis); System.out.println(&quot;print in main , user is &quot; + hollis);}public void pass(User user) { user.setName(&quot;hollischuang&quot;); System.out.println(&quot;print in pass , user is &quot; + user);} 同样是一个pass方法，同样是在pass方法内修改参数的值。输出结果如下： 12print in pass , user is User{name='hollischuang', gender='Male'}print in main , user is User{name='hollischuang', gender='Male'} 经过pass方法执行后，实参的值竟然被改变了，那按照上面的引用传递的定义，实际参数的值被改变了，这不就是引用传递了么。于是，根据上面的两段代码，有人得出一个新的结论：Java的方法中，在传递普通类型的时候是值传递，在传递对象类型的时候是引用传递。 但是，这种表述仍然是错误的。不信你看下面这个参数类型为对象的参数传递： 123456789101112public static void main(String[] args) { ParamTest pt = new ParamTest(); String name = &quot;Hollis&quot;; pt.pass(name); System.out.println(&quot;print in main , name is &quot; + name);}public void pass(String name) { name = &quot;hollischuang&quot;; System.out.println(&quot;print in pass , name is &quot; + name);} 上面的代码输出结果为 12print in pass , name is hollischuangprint in main , name is Hollis 这又作何解释呢？同样传递了一个对象，但是原始参数的值并没有被修改，难道传递对象又变成值传递了？ Java中的值传递上面，我们举了三个例子，表现的结果却不一样，这也是导致很多初学者，甚至很多高级程序员对于Java的传递类型有困惑的原因。 其实，我想告诉大家的是，上面的概念没有错，只是代码的例子有问题。来，我再来给大家画一下概念中的重点，然后再举几个真正恰当的例子。 值传递（pass by value）是指在调用函数时将实际参数复制一份传递到函数中，这样在函数中如果对参数进行修改，将不会影响到实际参数。 引用传递（pass by reference）是指在调用函数时将实际参数的地址直接传递到函数中，那么在函数中对参数所进行的修改，将影响到实际参数。 那么，我来给大家总结一下，值传递和引用传递之前的区别的重点是什么。 我们上面看过的几个pass的例子中，都只关注了实际参数内容是否有改变。如传递的是User对象，我们试着改变他的name属性的值，然后检查是否有改变。其实，在实验方法上就错了，当然得到的结论也就有问题了。 为什么说实验方法错了呢？这里我们来举一个形象的例子。再来深入理解一下值传递和引用传递，然后你就知道为啥错了。 你有一把钥匙，当你的朋友想要去你家的时候，如果你直接把你的钥匙给他了，这就是引用传递。这种情况下，如果他对这把钥匙做了什么事情，比如他在钥匙上刻下了自己名字，那么这把钥匙还给你的时候，你自己的钥匙上也会多出他刻的名字。 你有一把钥匙，当你的朋友想要去你家的时候，你复刻了一把新钥匙给他，自己的还在自己手里，这就是值传递。这种情况下，他对这把钥匙做什么都不会影响你手里的这把钥匙。 但是，不管上面那种情况，你的朋友拿着你给他的钥匙，进到你的家里，把你家的电视砸了。那你说你会不会受到影响？而我们在pass方法中，改变user对象的name属性的值的时候，不就是在“砸电视”么。 还拿上面的一个例子来举例，我们真正的改变参数，看看会发生什么？ 12345678910111213141516public static void main(String[] args) { ParamTest pt = new ParamTest(); User hollis = new User(); hollis.setName(&quot;Hollis&quot;); hollis.setGender(&quot;Male&quot;); pt.pass(hollis); System.out.println(&quot;print in main , user is &quot; + hollis);}public void pass(User user) { user = new User(); user.setName(&quot;hollischuang&quot;); user.setGender(&quot;Male&quot;); System.out.println(&quot;print in pass , user is &quot; + user);} 上面的代码中，我们在pass方法中，改变了user对象，输出结果如下： 12print in pass , user is User{name='hollischuang', gender='Male'}print in main , user is User{name='Hollis', gender='Male'} 我们来画一张图，看一下整个过程中发生了什么，然后我再告诉你，为啥Java中只有值传递。 稍微解释下这张图，当我们在main中创建一个User对象的时候，在堆中开辟一块内存，其中保存了name和gender等数据。然后hollis持有该内存的地址0x123456（图1）。当尝试调用pass方法，并且hollis作为实际参数传递给形式参数user的时候，会把这个地址0x123456交给user，这时，user也指向了这个地址（图2）。然后在pass方法内对参数进行修改的时候，即user = new User();，会重新开辟一块0X456789的内存，赋值给user。后面对user的任何修改都不会改变内存0X123456的内容（图3）。 上面这种传递是什么传递？肯定不是引用传递，如果是引用传递的话，在user=new User()的时候，实际参数的引用也应该改为指向0X456789，但是实际上并没有。 通过概念我们也能知道，这里是把实际参数的引用的地址复制了一份，传递给了形式参数。所以，上面的参数其实是值传递，把实参对象引用的地址当做值传递给了形式参数。 我们再来回顾下之前的那个“砸电视”的例子，看那个例子中的传递过程发生了什么。 同样的，在参数传递的过程中，实际参数的地址0X1213456被拷贝给了形参，只是，在这个方法中，并没有对形参本身进行修改，而是修改的形参持有的地址中存储的内容。 所以，值传递和引用传递的区别并不是传递的内容。而是实参到底有没有被复制一份给形参。在判断实参内容有没有受影响的时候，要看传的的是什么，如果你传递的是个地址，那么就看这个地址的变化会不会有影响，而不是看地址指向的对象的变化。就像钥匙和房子的关系。 那么，既然这样，为啥上面同样是传递对象，传递的String对象和User对象的表现结果不一样呢？我们在pass方法中使用name = &quot;hollischuang&quot;;试着去更改name的值，阴差阳错的直接改变了name的引用的地址。因为这段代码，会new一个String，在把引用交给name，即等价于name = new String(&quot;hollischuang&quot;);。而原来的那个”Hollis”字符串还是由实参持有着的，所以，并没有修改到实际参数的值。 所以说，Java中其实还是值传递的，只不过对于对象参数，值的内容是对象的引用。 总结无论是值传递还是引用传递，其实都是一种求值策略(Evaluation strategy)。在求值策略中，还有一种叫做按共享传递(call by sharing)。其实Java中的参数传递严格意义上说应该是按共享传递。 按共享传递，是指在调用函数时，传递给函数的是实参的地址的拷贝（如果实参在栈中，则直接拷贝该值）。在函数内部对参数进行操作时，需要先拷贝的地址寻找到具体的值，再进行操作。如果该值在栈中，那么因为是直接拷贝的值，所以函数内部对参数进行操作不会对外部变量产生影响。如果原来拷贝的是原值在堆中的地址，那么需要先根据该地址找到堆中对应的位置，再进行操作。因为传递的是地址的拷贝所以函数内对值的操作对外部变量是可见的。 简单点说，Java中的传递，是值传递，而这个值，实际上是对象的引用。 而按共享传递其实只是按值传递的一个特例罢了。所以我们可以说Java的传递是按共享传递，或者说Java中的传递是值传递。 参考资料Evaluation strategy 关于值传递和引用传递 按值传递、按引用传递、按共享传递 Is Java “pass-by-reference” or “pass-by-value”?","link":"/2019/06/13/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4Java%E4%B8%AD%E5%8F%AA%E6%9C%89%E5%80%BC%E4%BC%A0%E9%80%92/"},{"title":"什么是Servlet(原理，从访问到方法)","text":"Servlet简介Servlet是SUN公司提供的一门用于开发动态WEB资源的技术。SUN公司在其API中提供了一个Servlet接口，用户若想开发一个动态WEB资源(即开发一个Java程序向浏览器输出数据)，需要完成以下2个步骤： 编写一个Java类，实现Servlet接口； 把开发好的Java类部署到WEB服务器中。 那么我们不仅要问，写好的Servlet会在WEB应用中的什么位置上呢？位置如下如所示。 提示：按照一种约定俗成的称呼习惯，通常我们也把实现了Servlet接口的Java程序，称之为Servlet。 Servlet快速入门——使用Servlet向浏览器输出“Hello World”阅读Servlet API文档，文档地址是https://tomcat.apache.org/tomcat-8.5-doc/servletapi/index.html。文档里面有对Servlet接口的详细描述，如下。 借助有道翻译为： 定义了所有Servlet必须实现的方法。Servlet是运行在一个Web服务器里的一个小型Java程序。Servlets通常通过HTTP(超文本传输协议)接收并响&gt; 应来自Web客户端的请求。要实现这个接口，您可以编写一个继承了javax.servlet.GenericServlet的一般的Servlet，或者继承了javax.servlet.http.HttpServlet的HTTP Servlet。这个接口定义了方法来初始化一个Servlet，服务请求，并从服务器删除Servlet。这些被称为生命周期方法&gt; 并且按以下顺序依次调用： Servlet被构造，然后用init方法初始化； 任何来自客户机的请求在service方法中处理； Servlet从服务中移除，调用destroy方法销毁，然后垃圾收集和完成。 除了生命周期方法，该接口提供了getServletConfig方法(Servlet可以使用它来得到任何启动信息)和getServletInfo方法(它允许Servlet返回自身的基本信息，比如作者、版本和版权)。 这里面有一个专业术语——life-cycle methods，解释过来就是与生命周期相关的方法，即生命周期中的某个特定时刻必定会执行的方法。那么什么是对象的生命周期？什么又是与生命周期相关的方法呢？对象从创建到销毁经历的过程，称之为对象的生命周期。在对象生命周期过程中，在特定时刻肯定会执行一些特定的方法，这些方法称之为与生命周期相关的方法。例如，人从出生到死亡经历的过程，为人的一个生命周期，在人生命周期过程中，必定有一些与生命周期息息相关的方法，例如吃饭、上学、结婚等，这些方法在人生命周期过程中某个特定时刻必定会执行，所以这些方法是人生命周期相关的方法。但不是说对象中的所有方法都与生命周期相关，例如人自杀，这个方法不是在生命周期中必定会执行的。阅读完Servlet API，我们需要解决两个问题： 输出Hello Servlet的Java代码应该写在Servlet的哪个方法内？ 如何向浏览器输出数据？ 答案很明显： 输出Hello Servlet的Java代码应该写在Servlet的service方法中； 通过ServletResponse接口的实例中的getOutputStream方法获得输出流，向http响应对象中写入数据，服务器将http响应对象回送给浏览器，浏览器解析数据并显示。 下面我们正式编写一个入门级的Servlet。首先在Tomcat服务器webapps目录下新建一个Web应用，比如myWeb(Web应用所在目录)，在myWeb目录中新建一个WEB-INF目录，接着在WEB-INF目录下新建一个classes目录，在classes目录中新建一个Java应用程序——FirstServlet.java，代码如下： 12345678910111213package cn.liayun;import java.io.*;import javax.servlet.*;public class FirstServlet extends GenericServlet { public void service(ServletRequest req, ServletResponse res) throws ServletException, java.io.IOException { OutputStream out = res.getOutputStream(); out.write(&quot;Hello Servlet!!!&quot;.getBytes()); } } 着编译Java应用程序，如图： 所以，我们需要将Servlet所用Jar包加载到classpath路径下，如下图所示。 再在WEB-INF目录中新建一个web.xml文件，配置Servlet的访问对外路径。 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app xmlns=&quot;http://xmlns.jcp.org/xml/ns/javaee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd&quot; version=&quot;3.1&quot;&gt; &lt;servlet&gt; &lt;servlet-name&gt;FirstServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.liayun.FirstServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;FirstServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/FirstServlet&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; 最后启动Tomcat，通过Chrome浏览器进行访问。 Servlet的运行过程Servlet程序是由Web服务器调用的，Web服务器收到客户端的Servlet访问请求后： ①Web服务器首先检查是否已经装载并创建了该Servlet的实例对象。如果是，则直接执行第④步，否则，执行第②步； ②装载并创建该Servlet的一个实例对象； ③调用Servlet实例对象的init()方法； ④创建一个用于封装HTTP请求消息的HttpServletRequest对象和一个代表HTTP响应消息的HttpServletResponse对象，然后调用Servlet的service()方法并将请求和响应对象作为参数传递进去； ⑤Web应用程序被停止或重新启动之前，Servlet引擎将卸载Servlet，并在卸载之前调用Servlet的destroy()方法。 用动图来描述以上调用过程： 如果是用UML时序图来描述以上调用过程，则如下： 注意：上图并没画出destory()方法。destory()方法会在Web容器移除Servlet时执行，客户机第一次访问服务器时，服务器会创建Servlet实例对象，它就永远驻留在内存里面了，等待客户机第二次访问，这时有一个用户访问完Servlet之后，此Servlet对象并不会被摧毁，destory()方法也就不会被执行。 一道面试题：请说出Servlet的生命周期Servlet对象是用户第一次访问时创建，对象创建之后就驻留在内存里面了，响应后续的请求。Servlet对象一旦被创建，init()方法就会被执行，客户端的每次请求导致service()方法被执行，Servlet对象被摧毁时(Web服务器停止后或者Web应用从服务器里删除时)，destory()方法就会被执行。 在Eclipse中开发Servlet在Eclipse中新建一个Dynamic Web Project工程，Eclipse会自动创建下图所示目录结构： Servlet接口实现类对于Servlet接口，SUN公司定义了两个默认实现类，分别为GenericServlet和HttpServlet。HttpServlet指能够处理HTTP请求的Servlet，它在原有Servlet接口上添加了一些与HTTP协议相关的处理方法，它比Servlet接口的功能更为强大。因此开发人员在编写Servlet时，通常应继承这个类，而避免直接去实现Servlet接口。HttpServlet在实现Servlet接口时，覆写了service方法，该方法体内的代码会自动判断用户的请求方式，如为GET请求，则调用HttpServlet的doGet方法，如为Post请求，则调用doPost方法。因此，开发人员在编写Servlet时，通常只需要覆写doGet或doPost方法，而不要去覆写service方法(温馨提示：可阅读HttpServlet API文档)。 借助有道翻译为： 提供了一个抽象类派生子类来创建一个适合于一个网站的HTTP Servlet。HttpServlet的子类必须覆盖至少一个方法，通常是其中一个： doGet，如果Servlet支持HTTP GET请求 doPost，HTTP POST请求 doPut，HTTP PUT请求 doDelete，HTTP DELETE请求 初始化和销毁，管理Sevlet生命中被掌握的资源 getServletInfo，Servlet用来提供关于其自身信息 几乎没有理由覆盖service()方法。service()方法会处理标准HTTP请求，通过派遣他们每个HTTP请求类型的处理程序方法(上述doMethod方法)。同样，几乎没有理由覆盖doOptions和doTrace方法。 通过Eclipse创建和编写Servlet选中cn.liayun包，右键→New→Servlet，在Eclipse中创建和编写Servlet可参考下面一系列步骤： 这样，我们就通过Eclipse帮我们创建好一个名字为ServletSample的Servlet，创建好的ServletSample里面会有如下代码： 123456789101112131415161718192021222324252627282930313233package cn.liayun;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;/** * Servlet implementation class ServletSample */@WebServlet(&quot;/ServletSample&quot;)public class ServletSample extends HttpServlet { private static final long serialVersionUID = 1L; /** * @see HttpServlet#doGet(HttpServletRequest request, HttpServletResponse response) */ protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // TODO Auto-generated method stub response.getWriter().append(&quot;Served at: &quot;).append(request.getContextPath()); } /** * @see HttpServlet#doPost(HttpServletRequest request, HttpServletResponse response) */ protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { // TODO Auto-generated method stub doGet(request, response); }} 这些代码都是Eclipse自动生成的，而web.xml文件中也多了&lt;servlet&gt;&lt;/servlet&gt;和&lt;servlet-mapping&gt;&lt;/servlet-mapping&gt;两对标签，这两对标签是配置ServletSample的，应如下所示： 12345678&lt;servlet&gt; &lt;servlet-name&gt;ServletSample&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.liayun.ServletSample&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletSample&lt;/servlet-name&gt; &lt;url-pattern&gt;/ServletSample&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 注意：照理说，web.xml文件中会多&lt;servlet&gt;&lt;/servlet&gt;和&lt;servlet-mapping&gt;&lt;/servlet-mapping&gt;这两对标签，但是我的就没有，而且使用的是注解@WebServlet(“/ServletSample”)，好像因为我使用的是Servlet3.1规范的缘故。最后我们就可以通过浏览器访问ServletSample这个Servlet了，访问的URL地址是http://localhost:8080/day05/ServletSample。 Servlet开发注意细节如果你的Eclipse中有一个动态web项目TomcatTest，当你使用Eclipse导入一个外部项目，恰好这个项目名就是TomcatTest，这时你为了避免重名，需要修改导入的项目名，比如修改为t_ TomcatTest，然后你将其部署到Tomcat服务器中的webapps目录中，该项目映射的虚拟目录名称仍然是TomcatTest，所以你需要修改其虚拟目录。步骤如下： Servlet访问URL映射配置由于客户端是通过URL地址访问Web服务器中的资源，所以Servlet程序若想被外界访问，必须把Servlet程序映射到一个URL地址上，这个工作在web.xml文件中使用&lt;servlet&gt;元素和&lt;servlet-mapping&gt;元素完成。&lt;servlet&gt;元素用于注册Servlet，它包含有两个主要的子元素：&lt;servlet-name&gt;和&lt;servlet-class&gt;，分别用于设置Servlet的注册名称和Servlet的完整类名。一个&lt;servlet-mapping&gt;元素用于映射一个已注册的Servlet的一个对外访问路径，它包含有两个子元素：&lt;servlet-name&gt;和&lt;url-pattern&gt;，分别用于指定Servlet的注册名称和Servlet的对外访问路径。例如： 12345678&lt;servlet&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.itcast.ServletDemo1&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/ServletDemo1&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 同一个Servlet可以被映射到多个URL上，即多个&lt;servlet-mapping&gt;元素的&lt;servlet-name&gt;子元素的设置值可以是同一个Servlet的注册名。例如： 12345678910111213141516&lt;servlet&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.itcast.ServletDemo1&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/ServletDemo1&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/aa&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo1&lt;/servlet-name&gt; &lt;url-pattern&gt;/1.html&lt;/url-pattern&gt; &lt;!-- 伪静态，明显是一个动态Web资源，但将其映射成静态Web资源的名称 --&gt;&lt;/servlet-mapping&gt; 温馨提示：一个Web应用的web.xml文件内容一经修改，不需要重新发布，服务器会自动监测web.xml的改动，只要web.xml文件的内容修改，服务器就会自动加载。原因是在Tomcat服务器的conf/context.xml文件中，有如下关键代码： 根据Tomcat服务器文档可知，在conf/context.xml文件中，Context元素信息被所有的Web应用加载。即Context元素的配置信息会被所有Web应用程序所共享。所以所有的Web应用会监测web.xml的改动，只要web.xml文件的内容一旦修改，服务器就会自动重新加载。通过上面的配置，当我们想访问名称是ServletDemo1的Servlet时，可以使用如下的几个地址去访问： http://localhost:8080/day05/ServletDemo1； http://localhost:8080/day05/aa； http://localhost:8080/day05/1.html。 ServletDemo1被映射到了多个URL上。 Servlet访问URL使用*通配符映射在Servlet映射到的URL中也可以使用*通配符，但是只能有两种固定的格式：一种格式是“*.扩展名”，另一种格式是以正斜杠（/）开头并以“*”结尾。例如： 12345678910&lt;servlet-mapping&gt; &lt;servlet-name&gt;AnyName&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;AnyName&lt;/servlet-name&gt; &lt;url-pattern&gt;/action/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 对于如下的一些映射关系： Servlet1映射到/abc/*； Servlet2映射到/*； Servlet3映射到/abc； Servlet4映射到*.do。 有如下问题： 当请求URL为“/abc/a.html”，“/abc/*”和“/*”都匹配，哪个Servlet响应？——Servlet引擎将调用Servlet1； 当请求URL为“/abc”时，“/abc/*”、“/*”和“/abc”都匹配，哪个Servlet响应？——Servlet引擎将调用Servlet3； 当请求URL为“/abc/a.do”时，“/abc/*”、“/*”和“*.do”都匹配，哪个Servlet响应？——Servlet引擎将调用Servlet1； 当请求URL为“/a.do”时，“/*”和“*.do”都匹配，哪个Servlet响应？——Servlet引擎将调用Servlet2； 当请求URL为“/xxx/yyy/a.do”时，“/*”和“*.do”都匹配，哪个Servlet响应？——Servlet引擎将调用Servlet2。 *结论：匹配的原则就是”谁长得更像就找谁”，“*.do”——这种在前面的时候优先级最低。** Servlet与普通Java类的区别Servlet是一个供其他Java程序（Servlet引擎）调用的Java类，它不能独立运行，它的运行完全由Servlet引擎来控制和调度。针对客户端的多次Servlet请求，通常情况下，服务器只会创建一个Servlet实例对象，也就是说Servlet实例对象一旦创建，它就会驻留在内存中，为后续的其它请求服务，直至Web容器退出，Servlet实例对象才会销毁。验证如下： 新建一个Servlet——ServletDemo3，并覆盖init()和destroy()方法； 123456789101112131415161718192021222324252627282930313233package cn.liayun;import java.io.IOException;import javax.servlet.ServletConfig;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public class ServletDemo3 extends HttpServlet { private static final long serialVersionUID = 1L; @Override public void init(ServletConfig config) throws ServletException { super.init(config); System.out.println(&quot;init&quot;); } protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { response.getOutputStream().write(&quot;haha&quot;.getBytes()); } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); } @Override public void destroy() { System.out.println(&quot;destroy&quot;); }} 将项目部署到服务器中，启动服务器，发现没有输出init，说明启动服务器时，Servlet实例对象并没有被创建。此时，通过浏览器进行访问，会发现控制台输出init，如下： 此时再打开一个浏览器进行访问，仍然只会输出一个init，说明针对客户端的多次Servlet请求，通常情况下，服务器只会创建一个Servlet实例对象。 当Web服务器停止后或者Web应用从服务器里删除时，destroy()方法就会被执行； 在Web服务器停止前，Servlet实例对象就会被摧毁。 在Servlet的整个生命周期内，Servlet的init方法只被调用一次。而对一个Servlet的每次访问请求都导致Servlet引擎调用一次Servlet的service方法。对于每次访问请求，Servlet引擎都会创建一个新的HttpServletRequest请求对象和一个新的HttpServletResponse响应对象，然后将这两个对象作为参数传递给它调用的Servlet的service()方法，service方法再根据请求方式分别调用doXXX方法。如果在元素中配置了一个元素，那Web应用程序在启动时，就会装载并创建Servlet的实例对象、以及调用Servlet实例对象的init()方法。例如： 12345&lt;servlet&gt; &lt;servlet-name&gt;ServletDemo3&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.itcast.ServletDemo3&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 此时在启动服务器的过程中，会在控制台看到： 温馨提示：&lt;load-on-startup&gt;元素配置的数必须为正整数，数字越小，Servlet越优先创建。它的用途：可为Web应用写一个InitServlet，这个Servlet配置为启动时装载，为整个Web应用创建必要的数据库表和数据。 缺省Servlet如果某个Servlet的映射路径仅仅为一个正斜杠（/），那么这个Servlet就成为当前Web应用程序的缺省Servlet。凡是在web.xml文件中找不到匹配的&lt;servlet-mapping&gt;元素的URL，它们的访问请求都将交给缺省Servlet处理，也就是说，缺省Servlet用于处理所有其他Servlet都不处理的访问请求。例如： 12345678910&lt;servlet&gt; &lt;servlet-name&gt;ServletDemo3&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.itcast.ServletDemo3&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- 将ServletDemo3配置成缺省Servlet --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;ServletDemo3&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 当访问不存在的Servlet时，就使用配置的默认Servlet进行处理，如下图所示： 在&lt;Tomcat的安装目录&gt;\\conf\\web.xml文件中，注册了一个名称为org.apache.catalina.servlets.DefaultServlet的Servlet，并将这个Servlet设置为了缺省Servlet。 1234567891011121314151617181920&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- The mapping for the default servlet --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 当访问Tomcat服务器中的某个静态HTML文件和图片时，实际上是在访问这个缺省Servlet(服务器中的html文件数据的读取由缺省Servlet完成)。 Servlet的线程安全问题当多个客户端并发访问同一个Servlet时，Web服务器会为每一个客户端的访问请求创建一个线程，并在这个线程上调用Servlet的service方法，因此service方法内如果访问了同一个资源的话，就有可能引发线程安全问题。下面我会举例来说明。 当Servlet不存在线程安全问题时下面是不存在线程安全问题的代码。 1234567891011121314151617181920212223242526package cn.liayun;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@WebServlet(&quot;/ServletSample&quot;)public class ServletSample extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { int i = 0; i++; response.getOutputStream().write((i + &quot;&quot;).getBytes()); } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); }} 当多线程并发访问这个方法里面的代码时，会存在线程安全问题吗？显然不会，i变量被多个线程并发访问，但是没有线程安全问题，因为i是doGet方法里面的局部变量，当有多个线程并发访问doGet方法时，每一个线程里面都有自己的i变量，各个线程操作的都是自己的i变量，所以不存在线程安全问题。多线程并发访问某一个方法的时候，如果在方法内部定义了一些资源(变量，集合等)，那么每一个线程都有这些东西，所以就不存在线程安全问题。 当Servlet存在线程安全问题时下面是存在线程安全问题的代码。 12345678910111213141516171819202122232425262728293031323334package cn.liayun;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@WebServlet(&quot;/ServletSample&quot;)public class ServletSample extends HttpServlet { private int i = 0; protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { i++; try { Thread.sleep(1000 * 10); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } response.getOutputStream().write((i + &quot;&quot;).getBytes()); } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); }} 把i定义成全局变量，当多个线程并发访问变量i时，就会存在线程安全问题了。线程安全问题只存在多个线程并发操作同一个资源的情况下，所以在编写Servlet的时候，如果并发访问某一个资源(变量，集合等)，就会存在线程安全问题，那么该如何解决这个问题呢？可使用同步代码块。 12345678910111213141516171819202122232425262728293031323334353637package cn.liayun;import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@WebServlet(&quot;/ServletSample&quot;)public class ServletSample extends HttpServlet { private int i = 0;//共享资源 protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { i++; synchronized (this) { try { Thread.sleep(1000 * 10); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } } response.getOutputStream().write((i + &quot;&quot;).getBytes()); } protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); }} 加了synchronized后，并发访问i时就不存在线程安全问题了，为什么加了synchronized后就没有线程安全问题了呢？原因：假如现在有一个线程访问Servlet对象，那么它就先拿到了Servlet对象的那把锁，等到它执行完之后才会把锁还给Servlet对象，由于是它先拿到了Servlet对象的那把锁，所以当有别的线程来访问这个Servlet对象时，由于锁已经被之前的线程拿走了，后面的线程只能排队等候了。以上这种做法是给Servlet对象加了一把锁，保证任何时候都只有一个线程在访问该Servlet对象里面的资源，这样就不存在线程安全问题了。这种做法虽然解决了线程安全问题，但是编写Servlet却万万不能用这种方式处理线程安全问题，假如有9999个人同时访问这个Servlet，那么这9999个人必须按先后顺序排队轮流访问。针对Servlet的线程安全问题，SUN公司是提供有解决方案的：让Servlet去实现一个SingleThreadModel接口，如果某个Servlet实现了SingleThreadModel接口，那么Servlet引擎将以单线程模式来调用其service方法。查看Sevlet的API可以看到，SingleThreadModel接口中没有定义任何方法和常量，在Java中，把没有定义任何方法和常量的接口称之为标记接口，经常看到的一个最典型的标记接口就是”Serializable”，这个接口也是没有定义任何方法和常量的，标记接口在Java中有什么用呢？主要作用就是给某个对象打上一个标志，告诉JVM，这个对象可以做什么，比如实现了”Serializable”接口的类的对象就可以被序列化，还有一个”Cloneable”接口，这个也是一个标记接口，在默认情况下，Java中的对象是不允许被克隆的，就像现实生活中的人一样，不允许克隆，但是只要实现了”Cloneable”接口，那么对象就可以被克隆了。SingleThreadModel接口中没有定义任何方法，只要在Servlet类的定义中增加实现SingleThreadModel接口的声明即可。对于实现了SingleThreadModel接口的Servlet，Servlet引擎仍然支持对该Servlet的多线程并发访问，其采用的方式是产生多个Servlet实例对象，并发的每个线程分别调用一个独立的Servlet实例对象。实现SingleThreadModel接口并不能真正解决Servlet的线程安全问题，因为Servlet引擎会创建多个Servlet实例对象，而真正意义上解决多线程安全问题是指一个Servlet实例对象被多个线程同时调用的问题。事实上，在Servlet API 2.4中，已经将SingleThreadModel标记为Deprecated（过时的）。 以上代码还要注意异常的处理，代码Thread.sleep(1000*4);只能try不能抛，因为子类在覆盖父类的方法时，不能抛出比父类更多的异常；并且catch之后，后台记录异常的同时并给用户一个友好提示，因为用户访问的是一个网页。","link":"/2019/04/17/%E4%BB%80%E4%B9%88%E6%98%AFServlet/"},{"title":"实现一个免费的图片上传Web Server","text":"一. 框架 选用express框架 12npm initnpm install express --save 二. 简单测试请求 在当前目录新建index.js文件 12345678910const express = require(&quot;express&quot;);const app = express();app.get(&quot;/&quot;, (req, res) =&gt; { res.send(&quot;Hello Node.js&quot;);});const port = 3000;app.listen(port); 复制代码在终端输入： node index.js 在浏览器中打开 127.0.0.1:3000 三.使用form上传图片12345678910111213141516&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width&quot;&gt; &lt;title&gt;upload&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=&quot;http://127.0.0.1:3000/upload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;div&gt; &lt;input type=&quot;file&quot; name=&quot;avatar&quot; accept=&quot;image/*&quot;&gt; &lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 将index.js中的接口更新成 123app.post(&quot;/upload&quot;, (req, res) =&gt; { res.send('上传成功')}); 注意：index.js中的文件只要改了，就要重新启动服务 试着上传一下： 四. 将前端发送的图片储存在服务器中 这里需要用到一个叫multer的库 1npm install multer --save 根据他的文档，改一下index.js: 1234567891011121314151617const express = require(&quot;express&quot;);const multer = require(&quot;multer&quot;);// 这里定义图片储存的路径，是以当前文件为基本目录const upload = multer({ dest: &quot;uploads/&quot; });const app = express();/* upload.single('avatar') 接受以avatar命名的文件，也就是input中的name属性的值 avatar这个文件的信息可以冲req.file中获取*/app.post(&quot;/upload&quot;, upload.single(&quot;avatar&quot;), (req, res) =&gt; { console.log(req.file); res.send(&quot;上传成功&quot;);});const port = 3000;app.listen(port); 改完之后重新启动服务，再重新上传： 可以看到req.file中就是上传的文件信息。 同时，你会发现当前目录下，会多一个文件夹叫uoloads 那个很长名字的文件，就是刚刚前端传的图片。只要改一下后缀名就可以预览了： 五. 将储存的图片名返回给前端一般上传完头像会有一个预览功能，那么只需要后端将上传后的图片名发送给前端，前端重新请求一下图片就好了，前面都是用form默认的提交，这个提交存在一个问题就是，提交完成后页面会发生跳转。所以现在一般都是用ajax进行上传。 123456789101112131415161718// js代码upload.addEventListener('submit', (e) =&gt; { // 阻止form 的默认行为 e.preventDefault(); // 创建FormData对象 var formData = new FormData(); var fileInput = document.querySelector('input[name=&quot;avatar&quot;]'); formData.append(fileInput.name, fileInput.files[0]); // 创建XMLHttpRequest var xhr = new XMLHttpRequest(); xhr.open('POST', upload.action); xhr.onload = function() { console.log(xhr.response) } xhr.send(formData);}) 小提示：如果HTML的元素有id属性，那么可以不用document.querySelector去选中它，可以直接使用，就像全局变量一样。 1234// index.jsapp.post(&quot;/upload&quot;, upload.single(&quot;avatar&quot;), (req, res) =&gt; { res.json({name: req.file.filename }); // 使用json格式返回数据。 }); 这时候重新发送，会出现一个问题： 由于代码是写在 JS Bin上的，使用AJAX请求不同域名的接口，会出现跨域情况，解决这个问题需要，在index.js中加上一个头部，就是报错信息中的Access-Control-Allow-Origin： 1234app.post(&quot;/upload&quot;, upload.single(&quot;avatar&quot;), (req, res) =&gt; { res.set('Access-Control-Allow-Origin', '*'); res.json({name: req.file.filename });}); * 表示所有其他域名都可访问，也可以将*改为其他允许的域名。 重新发送： 这样成功的上传了图片，并且拿到了上传后的图片名。 这里可以使用一个库cors，来完成添加响应头的操作： npm install cors –save 修改index.js 123456789101112131415161718const express = require(&quot;express&quot;);const multer = require(&quot;multer&quot;);const cors = require('cors'); // 新增// 这里定义图片储存的路径，是以当前文件为基本目录const upload = multer({ dest: &quot;uploads/&quot; });const app = express();app.use(cors()); // 新增/* upload.single('avatar') 接受以avatar命名的文件，也就是input中的name属性的值 avatar这个文件的信息可以冲req.file中获取*/app.post(&quot;/upload&quot;, upload.single(&quot;avatar&quot;), (req, res) =&gt; { res.json({name: req.file.filename });});const port = 3000;app.listen(port); 六. 展示上传后的图片新定义一个接口： 123456789101112app.get(&quot;/preview/:name&quot;, (req, res) =&gt; { res.sendFile(`uploads/${req.params.name}`, { root: __dirname, headers:{ 'Content-Type': 'image/jpeg', }, }, (error)=&gt;{ if(error){ res.status(404).send('Not found') } });}); /preview:name 这种方式定义接口的路径，请求过来的时候，就可以从 req.params.name 中拿到 /preview/xxxx 中的xxxx了。修改下HTML: 1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width&quot;&gt; &lt;title&gt;JS Bin&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form id=&quot;upload&quot; action=&quot;http://127.0.0.1:3000/upload&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;div&gt; &lt;input type=&quot;file&quot; name=&quot;avatar&quot; accept=&quot;image/*&quot;&gt; &lt;/div&gt; &lt;input type=&quot;submit&quot; value=&quot;上传&quot;&gt; &lt;/form&gt; &lt;img src=&quot;&quot; id=&quot;avatarImg&quot;&gt; &lt;!-- 新增 --&gt;&lt;/body&gt;&lt;/html&gt; 改下js 123456789101112131415161718upload.addEventListener('submit', (e) =&gt; { // 阻止form 的默认行为 e.preventDefault(); // 创建FormData对象 var formData = new FormData(); var fileInput = document.querySelector('input[name=&quot;avatar&quot;]'); formData.append(fileInput.name, fileInput.files[0]); // 创建XMLHttpRequest var xhr = new XMLHttpRequest(); xhr.open('POST', upload.action); xhr.onload = function() { var imgName = JSON.parse(xhr.response).name; // 新增 avatarImg.setAttribute('src', 'http://127.0.0.1:3000/preview/' + imgName); // 新增 } xhr.send(formData);}) 结果： 六. 将代码部署到Heroku Heroku是一个支持多种编程语言的云平台即服务。最重要的它是免费的。这是他的官方网站Heroku，注意不XX上网的话，可会超级慢或者进不去。而且XX上网要全局模式.. 在部署的时候，有三个选择，我选择选择GitHub 由于选择GitHub，那么还需要创建一个仓库，把代码放上去。 放上去之间还要改一下代码： 因为部署是交给heroku的，所以端口号不能写死： const port = process.env.PORT || 3000; 在package.json添加一个npm start命令 1&quot;start&quot;: &quot;node index.js&quot; 7.在GitHub上创建仓库并上传代码，过程略,别忘了写.gitignore文件 8.这是我的仓库地址 9.在heroku中选择仓库并且选择分支master，部署 10.预览 这个就是部署好的域名了。 用这个域名试一试 大功告成。可惜的就是heroku得上网才行。","link":"/2019/04/11/%E4%BD%BF%E7%94%A8heroku%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%9A%84%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0Web-Server/"},{"title":"使用nio进行大文件复制","text":"NIO概述什么是NIO? Java NIO(New IO)是一个可以替代标准Java IO API的IO API（从Java 1.4开始)，Java NIO提供了与标准IO不同的IO工作方式。 Java NIO: Channels and Buffers（通道和缓冲区） 标准的IO基于字节流和字符流进行操作的，而NIO是基于通道（Channel）和缓冲区（Buffer）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。 Java NIO: Non-blocking IO（非阻塞IO） Java NIO可以让你非阻塞的使用IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Java NIO: Selectors（选择器） Java NIO引入了选择器的概念，选择器用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个的线程可以监听多个数据通道。 注意:传统IT是单向。 NIO类似 Buffer的概述1）容量（capacity）：表示Buffer最大数据容量，缓冲区容量不能为负，并且建立后不能修改。2）限制（limit）：第一个不应该读取或者写入的数据的索引，即位于limit后的数据不可以读写。缓冲区的限制不能为负，并且不能大于其容量（capacity）。3）位置（position）：下一个要读取或写入的数据的索引。缓冲区的位置不能为负，并且不能大于其限制（limit）。4）标记（mark）与重置（reset）：标记是一个索引，通过Buffer中的mark()方法指定Buffer中一个特定的position，之后可以通过调用reset()方法恢复到这个position。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * (缓冲区)buffer 用于NIO存储数据 支持多种不同的数据类型 &lt;br&gt; * 1.byteBuffer &lt;br&gt; * 2.charBuffer &lt;br&gt; * 3.shortBuffer&lt;br&gt; * 4.IntBuffer&lt;br&gt; * 5.LongBuffer&lt;br&gt; * 6.FloatBuffer &lt;br&gt; * 7.DubooBuffer &lt;br&gt; * 上述缓冲区管理的方式 几乎&lt;br&gt; * 通过allocate（） 获取缓冲区 &lt;br&gt; * 二、缓冲区核心的方法 put 存入数据到缓冲区 get &lt;br&gt; 获取缓冲区数据 flip 开启读模式 * 三、缓冲区四个核心属性&lt;br&gt; * capacity:缓冲区最大容量，一旦声明不能改变。 limit:界面(缓冲区可以操作的数据大小) limit后面的数据不能读写。 * position:缓冲区正在操作的位置 */public class Test004 { public static void main(String[] args) { // 1.指定缓冲区大小1024 ByteBuffer buf = ByteBuffer.allocate(1024); System.out.println(&quot;--------------------&quot;); System.out.println(buf.position()); System.out.println(buf.limit()); System.out.println(buf.capacity()); // 2.向缓冲区存放5个数据 buf.put(&quot;abcd1&quot;.getBytes()); System.out.println(&quot;--------------------&quot;); System.out.println(buf.position()); System.out.println(buf.limit()); System.out.println(buf.capacity()); // 3.开启读模式 buf.flip(); System.out.println(&quot;----------开启读模式...----------&quot;); System.out.println(buf.position()); System.out.println(buf.limit()); System.out.println(buf.capacity()); byte[] bytes = new byte[buf.limit()]; buf.get(bytes); System.out.println(new String(bytes, 0, bytes.length)); System.out.println(&quot;----------重复读模式...----------&quot;); // 4.开启重复读模式 buf.rewind(); System.out.println(buf.position()); System.out.println(buf.limit()); System.out.println(buf.capacity()); byte[] bytes2 = new byte[buf.limit()]; buf.get(bytes2); System.out.println(new String(bytes2, 0, bytes2.length)); // 5.clean 清空缓冲区 数据依然存在,只不过数据被遗忘 System.out.println(&quot;----------清空缓冲区...----------&quot;); buf.clear(); System.out.println(buf.position()); System.out.println(buf.limit()); System.out.println(buf.capacity()); System.out.println((char)buf.get()); }} 直接缓冲区与非直接缓冲耗时计算123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package com.itmayiedu;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.MappedByteBuffer;import java.nio.channels.FileChannel;import java.nio.channels.FileChannel.MapMode;import java.nio.file.Paths;import java.nio.file.StandardOpenOption;import org.junit.Test;public class Test003 { //直接缓冲区 @Test public void test002() throws IOException { long statTime=System.currentTimeMillis(); //创建管道 FileChannel inChannel= FileChannel.open(Paths.get(&quot;f://1.mp4&quot;), StandardOpenOption.READ); FileChannel outChannel= FileChannel.open(Paths.get(&quot;f://2.mp4&quot;), StandardOpenOption.READ,StandardOpenOption.WRITE, StandardOpenOption.CREATE); //定义映射文件 MappedByteBuffer inMappedByte = inChannel.map(MapMode.READ_ONLY,0, inChannel.size()); MappedByteBuffer outMappedByte = outChannel.map(MapMode.READ_WRITE,0, inChannel.size()); //直接对缓冲区操作 byte[] dsf=new byte[inMappedByte.limit()]; inMappedByte.get(dsf); outMappedByte.put(dsf); inChannel.close(); outChannel.close(); long endTime=System.currentTimeMillis(); System.out.println(&quot;操作直接缓冲区耗时时间:&quot;+(endTime-statTime)); } //直接缓冲内置函数版 @Test public void test004() throws IOException { long statTime=System.currentTimeMillis(); //创建管道 FileChannel inChannel= FileChannel.open(Paths.get(&quot;D:\\\\demo6.rar&quot;), StandardOpenOption.READ); FileChannel outChannel= FileChannel.open(Paths.get(&quot;D:\\\\Portable.rar&quot;), StandardOpenOption.READ,StandardOpenOption.WRITE, StandardOpenOption.CREATE); //定义映射文件 outChannel.transferFrom(inChannel, 0 , inChannel.size()); inChannel.close(); outChannel.close(); long endTime=System.currentTimeMillis(); System.out.println(&quot;操作直接缓冲区耗时时间:&quot;+(endTime-statTime)); } // 非直接缓冲区 读写操作 @Test public void test001() throws IOException { long statTime=System.currentTimeMillis(); // 读入流 FileInputStream fst = new FileInputStream(&quot;f://1.mp4&quot;); // 写入流 FileOutputStream fos = new FileOutputStream(&quot;f://2.mp4&quot;); // 创建通道 FileChannel inChannel = fst.getChannel(); FileChannel outChannel = fos.getChannel(); // 分配指定大小缓冲区 ByteBuffer buf = ByteBuffer.allocate(1024); while (inChannel.read(buf) != -1) { // 开启读取模式 buf.flip(); // 将数据写入到通道中 outChannel.write(buf); buf.clear(); } // 关闭通道 、关闭连接 inChannel.close(); outChannel.close(); fos.close(); fst.close(); long endTime=System.currentTimeMillis(); System.out.println(&quot;操作非直接缓冲区耗时时间:&quot;+(endTime-statTime)); }}","link":"/2019/03/29/%E4%BD%BF%E7%94%A8nio%E8%BF%9B%E8%A1%8C%E5%A4%A7%E6%96%87%E4%BB%B6%E5%A4%8D%E5%88%B6/"},{"title":"使用阿里云的图片识别成表格OCR","text":"为了简便财务总是要对照着别人发来的表格图片制作成自己的表格 图片识别 识别成表格 表格识别 ocr 将图片表格转换成excel 使用阿里云api 购买（印刷文字识别-表格识别） https://market.aliyun.com/products/57124001/cmapi024968.html 获得阿里云图片识别表格的appcode 效果图如下 整合的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235package com.xai.wuye.controller.api;import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONException;import com.alibaba.fastjson.JSONObject;import com.xai.wuye.common.JsonResult;import com.xai.wuye.exception.ResultException;import com.xai.wuye.model.AParamimport com.xai.wuye.service.CarService;import com.xai.wuye.util.HttpUtils;import org.apache.http.HttpResponse;import org.apache.http.util.EntityUtils;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.core.io.FileSystemResource;import org.springframework.http.HttpHeaders;import org.springframework.http.MediaType;import org.springframework.http.ResponseEntity;import org.springframework.scheduling.annotation.EnableAsync;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import org.springframework.web.multipart.MultipartFile;import java.io.*;import java.util.Date;import java.util.HashMap;import java.util.Map;import static org.apache.tomcat.util.codec.binary.Base64.encodeBase64;@Controller@EnableAsync@RequestMapping(&quot;/api/ocr&quot;)public class AliOCRImages { @Autowired CarService carService; private String OcrPath = &quot;/home/runApp/car/orc/&quot;; @ResponseBody @RequestMapping(&quot;table&quot;) public JsonResult getFirstLicence(@RequestParam(value = &quot;file&quot;, required = false) MultipartFile file) { if (file == null || file.isEmpty()||file.getSize() &gt; 1204*1204*3) throw new ResultException(0,&quot;文件为null，且不能大于3M&quot;); String filename = file.getOriginalFilename(); String filepath = OcrPath+&quot;temp/&quot;+filename; File newFile = new File(filepath); try { file.transferTo(newFile); String host = &quot;https://form.market.alicloudapi.com&quot;; String path = &quot;/api/predict/ocr_table_parse&quot;; // 输入阿里的code String appcode = &quot;4926a667ee6c41329c278361*****&quot;; String imgFile = &quot;图片路径&quot;; Boolean is_old_format = false;//如果文档的输入中含有inputs字段，设置为True， 否则设置为False //请根据线上文档修改configure字段 JSONObject configObj = new JSONObject(); configObj.put(&quot;format&quot;, &quot;xlsx&quot;); configObj.put(&quot;finance&quot;, false); configObj.put(&quot;dir_assure&quot;, false); String config_str = configObj.toString(); // configObj.put(&quot;min_size&quot;, 5); //String config_str = &quot;&quot;; String method = &quot;POST&quot;; Map&lt;String, String&gt; headers = new HashMap&lt;String, String&gt;(); //最后在header中的格式(中间是英文空格)为Authorization:APPCODE 83359fd73fe94948385f570e3c139105 headers.put(&quot;Authorization&quot;, &quot;APPCODE &quot; + appcode); Map&lt;String, String&gt; querys = new HashMap&lt;String, String&gt;(); // 对图像进行base64编码 String imgBase64 = &quot;&quot;; try { byte[] content = new byte[(int) newFile.length()]; FileInputStream finputstream = new FileInputStream(newFile); finputstream.read(content); finputstream.close(); imgBase64 = new String(encodeBase64(content)); } catch (IOException e) { e.printStackTrace(); return null; } // 拼装请求body的json字符串 JSONObject requestObj = new JSONObject(); try { if(is_old_format) { JSONObject obj = new JSONObject(); obj.put(&quot;image&quot;, getParam(50, imgBase64)); if(config_str.length() &gt; 0) { obj.put(&quot;configure&quot;, getParam(50, config_str)); } JSONArray inputArray = new JSONArray(); inputArray.add(obj); requestObj.put(&quot;inputs&quot;, inputArray); }else{ requestObj.put(&quot;image&quot;, imgBase64); if(config_str.length() &gt; 0) { requestObj.put(&quot;configure&quot;, config_str); } } } catch (JSONException e) { e.printStackTrace(); } String bodys = requestObj.toString(); try { /** * 重要提示如下: * HttpUtils请从 * https://github.com/aliyun/api-gateway-demo-sign-java/blob/master/src/main/java/com/aliyun/api/gateway/demo/util/HttpUtils.java * 下载 * * 相应的依赖请参照 * https://github.com/aliyun/api-gateway-demo-sign-java/blob/master/pom.xml */ HttpResponse response = HttpUtils.doPost(host, path, method, headers, querys, bodys); int stat = response.getStatusLine().getStatusCode(); if(stat != 200){ System.out.println(&quot;Http code: &quot; + stat); System.out.println(&quot;http header error msg: &quot;+ response.getFirstHeader(&quot;X-Ca-Error-Message&quot;)); System.out.println(&quot;Http body error msg:&quot; + EntityUtils.toString(response.getEntity())); return null; } String res = EntityUtils.toString(response.getEntity()); JSONObject res_obj = JSON.parseObject(res); Long fileName = System.currentTimeMillis(); if(is_old_format) { JSONArray outputArray = res_obj.getJSONArray(&quot;outputs&quot;); String output = outputArray.getJSONObject(0).getJSONObject(&quot;outputValue&quot;).getString(&quot;dataValue&quot;); JSONObject out = JSON.parseObject(output); System.out.println(out.toJSONString()); }else{ String tmp_base64path = OcrPath + fileName; File tmp_base64file = new File(tmp_base64path); if(!tmp_base64file.exists()){ tmp_base64file.getParentFile().mkdirs(); } tmp_base64file.createNewFile(); // write FileWriter fw = new FileWriter(tmp_base64file, true); BufferedWriter bw = new BufferedWriter(fw); bw.write(res_obj.getString(&quot;tables&quot;)); bw.flush(); bw.close(); fw.close(); String exelFilePath = OcrPath + fileName + &quot;_1.xlsx&quot;; Runtime.getRuntime().exec(&quot;touch &quot;+exelFilePath).destroy(); Process exec = Runtime.getRuntime().exec(&quot;sed -i -e 's/\\\\\\\\n/\\\\n/g' &quot; + tmp_base64path); exec.waitFor(); exec.destroy(); Process exec1 = null; String[] cmd = { &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;base64 -d &quot; + tmp_base64path + &quot; &gt; &quot; + exelFilePath }; exec1 = Runtime.getRuntime().exec(cmd); exec1.waitFor(); exec1.destroy(); return JsonResult.success(fileName); } } catch (Exception e) { e.printStackTrace(); } } catch (IOException e) { e.printStackTrace(); } return null; } @ResponseBody @RequestMapping(&quot;getId&quot;) public ResponseEntity&lt;FileSystemResource&gt; getFirstLicence(String id) { String exelFilePath = OcrPath + id + &quot;_1.xlsx&quot;; return export(new File(exelFilePath)); } public ResponseEntity&lt;FileSystemResource&gt; export(File file) { if (file == null) { return null; } HttpHeaders headers = new HttpHeaders(); headers.add(&quot;Cache-Control&quot;, &quot;no-cache, no-store, must-revalidate&quot;); headers.add(&quot;Content-Disposition&quot;, &quot;attachment; filename=&quot; + System.currentTimeMillis() + &quot;.xls&quot;); headers.add(&quot;Pragma&quot;, &quot;no-cache&quot;); headers.add(&quot;Expires&quot;, &quot;0&quot;); headers.add(&quot;Last-Modified&quot;, new Date().toString()); headers.add(&quot;ETag&quot;, String.valueOf(System.currentTimeMillis())); return ResponseEntity .ok() .headers(headers) .contentLength(file.length()) .contentType(MediaType.parseMediaType(&quot;application/octet-stream&quot;)) .body(new FileSystemResource(file)); } public static JSONObject getParam(int type, String dataValue) { JSONObject obj = new JSONObject(); try { obj.put(&quot;dataType&quot;, type); obj.put(&quot;dataValue&quot;, dataValue); } catch (JSONException e) { e.printStackTrace(); } return obj; }} 大功告成 以下是静态页面代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;!-- import CSS --&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/element-ui/lib/theme-chalk/index.css&quot;&gt; &lt;title&gt;table&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=&quot;app&quot;&gt; &lt;el-upload class=&quot;upload-demo&quot; drag action=&quot;https://www.***.com/car/api/ocr/table&quot; :file-list=&quot;imagelist&quot; :on-preview=&quot;pre&quot; &gt; &lt;i class=&quot;el-icon-upload&quot;&gt;&lt;/i&gt; &lt;div class=&quot;el-upload__text&quot;&gt;将文件拖到此处，或&lt;em&gt;点击上传&lt;/em&gt;&lt;/div&gt; &lt;div class=&quot;el-upload__tip&quot; slot=&quot;tip&quot;&gt;只能上传jpg/png文件，且不超过500kb&lt;/div&gt; &lt;/el-upload&gt; &lt;div class=&quot;img-content&quot; v-for=&quot;(item,key) in imagelist&quot; :key=&quot;key&quot;&gt; &lt;img :src=&quot;item.url&quot;&gt; &lt;div class=&quot;name&quot;&gt; &lt;div&gt;{{ item.name }}&lt;/div&gt; &lt;el-button type=&quot;text&quot; @click=&quot;handleFileName(item,key)&quot;&gt;修改名字&lt;/el-button&gt; &lt;/div&gt; &lt;!-- 删除icon --&gt; &lt;div class=&quot;del&quot;&gt; &lt;i @click=&quot;handleFileRemove(item,key)&quot; class=&quot;el-icon-delete2&quot;&gt;&lt;/i&gt; &lt;/div&gt; &lt;!-- 放大icon --&gt; &lt;div class=&quot;layer&quot; @click=&quot;handleFileEnlarge(item.url)&quot;&gt; &lt;i class=&quot;el-icon-view&quot;&gt;&lt;/i&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/body&gt; &lt;!-- import Vue before Element --&gt; &lt;script src=&quot;https://unpkg.com/vue/dist/vue.js&quot;&gt;&lt;/script&gt; &lt;!-- import JavaScript --&gt; &lt;script src=&quot;https://unpkg.com/element-ui/lib/index.js&quot;&gt;&lt;/script&gt; &lt;script&gt; new Vue({ el: '#app', data: function() { return { visible: false, imagelist: [ ] } }, methods: { pre(res) { console.log(res.response.msg) window.open(&quot;https://www.***.com/api/ocr/getId?id=&quot;+res.response.data); } } }) &lt;/script&gt;&lt;/html&gt;","link":"/2019/04/08/%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E7%9A%84%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB%E6%88%90%E8%A1%A8%E6%A0%BC/"},{"title":"写博客的一些推荐","text":"写博客的一些个人推荐1、使用markdown 你如果经常上github这个神奇的网站。你会看到README.md ，没错，就是 markdown写的 大家可以问问身边的大牛们，哪个不用markdown?就连渣渣程序猿的我也在用。 用它写博客，你会感受到一个字：爽！如果你还没有用过markdown，听我的，抓紧试一试。 2、使用七牛云 使用markdown的问题来了，图片存在哪？毕竟我们写博客会粘贴各种各样的图片，这个问题大家不用担心。 我个人是使用七牛云来存储。因为有10G的免费空间， 肯定是够你愉快的玩耍了。 如果你用的不是很顺手，你也可以使用阿里云。也可以用各种偏门的方法，比如把图片放到github上然后弄下链接来。 不过你想整理下文件的话 还是最好用一个文件资源服务器把。 3、使用PicGo 每次都需要登录七牛云官网，然后点击上传，上传多张图片之后，也不知道哪张是哪张了。开源的大神早已为我们找到解决办法，使用PicGo.不仅支持七牛云，还有各种云。够你用的了。 4、学会使用思维导图 使用ProcessOn 身为程序员的我们，写文章肯定用到流程图，思维导图，UML类图等各种图，ProcessOn满足你所有的需求。 ProcessOn是一个在线作图工具的聚合平台，它可以在线画流程图、思维导图、UI原型图、UML、网络拓扑图、组织结构图等等 还可以把作品分享给团队成员或好友，无论何时何地大家都可以对作品进行编辑、阅读和评论 5、 md2all 如果大家平时还维护这自己的个人公众号，肯定知道公众号里面代码格式的痛苦，没关系。开源的世界大牛们已经为我们找到解决方案。使用md2all ,可以直接将markdown转成适应所有格式的文章。 我们公司的编辑器就没有markdown，你完全可以用这个转换下。 同时还有一个我感觉不错的转换markdown的。程序员DD写的 http://blog.didispace.com/tools/online-markdown/ 6、 感觉没什么好写的？也不知道写什么？ 不要老觉着自己技术差，写出的东西太基础，没有价值，不要这种担心，技术渣渣的我都敢去写博客。你为什么不敢呢？比如java中的for循环，你如果写一篇博客把它介绍的很详细，让读者一读就明白了for循环的执行顺序。也是一篇好的博客。 不要觉得自己东西没有人看。就没有价值，这你就大错特错了。。每次遇到一个技术难关，攻破后虽然很简单。以为能牢记很久，但是时间一场就真的忘记了。如果你你有做记录记录了下来。哪怕只是只有代码。你看一眼也能马上唤起你当时的记忆。","link":"/2019/03/29/%E5%86%99%E5%8D%9A%E5%AE%A2%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8E%A8%E8%8D%90/"},{"title":"可能是把Java内存区域讲的最清楚的一篇文章","text":"写在前面(常见面试题)基本问题 介绍下 Java 内存区域（运行时数据区） Java 对象的创建过程（五步，建议能默写出来并且要知道每一步虚拟机做了什么） 对象的访问定位的两种方式（句柄和直接指针两种方式） 拓展问题 String类和常量池 8种基本类型的包装类和常量池 一 概述对于 Java 程序员来说，在虚拟机自动内存管理机制下，不再需要像C/C++程序开发程序员这样为内一个 new 操作去写对应的 delete/free 操作，不容易出现内存泄漏和内存溢出问题。正是因为 Java 程序员把内存控制权利交给 Java 虚拟机，一旦出现内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会是一个非常艰巨的任务。 二 运行时数据区域Java 虚拟机在执行 Java 程序的过程中会把它管理的内存划分成若干个不同的数据区域。JDK. 1.8 和之前的版本略有不同，下面会介绍到。 JDK 1.8之前： JDK 1.8 ： 线程私有的： 程序计数器 虚拟机栈 本地方法栈 线程共享的： 堆 方法区 直接内存(非运行时数据区的一部分) 2.1 程序计数器程序计数器是一块较小的内存空间，可以看作是当前线程所执行的字节码的行号指示器。字节码解释器工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器来完。 另外，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 从上面的介绍中我们知道程序计数器主要有两个作用： 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 注意：程序计数器是唯一一个不会出现 OutOfMemoryError 的内存区域，它的生命周期随着线程的创建而创建，随着线程的结束而死亡。 2.2 Java 虚拟机栈与程序计数器一样，Java虚拟机栈也是线程私有的，它的生命周期和线程相同，描述的是 Java 方法执行的内存模型，每次方法调用的数据都是通过栈传递的。 Java 内存可以粗糙的区分为堆内存（Heap）和栈内存(Stack),其中栈就是现在说的虚拟机栈，或者说是虚拟机栈中局部变量表部分。 （实际上，Java虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息。） 局部变量表主要存放了编译器可知的各种数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或其他与此对象相关的位置）。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError： 若Java虚拟机栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前Java虚拟机栈的最大深度的时候，就抛出StackOverFlowError异常。 OutOfMemoryError： 若 Java 虚拟机栈的内存大小允许动态扩展，且当线程请求栈时内存用完了，无法再动态扩展了，此时抛出OutOfMemoryError异常。 Java 虚拟机栈也是线程私有的，每个线程都有各自的Java虚拟机栈，而且随着线程的创建而创建，随着线程的死亡而死亡。 扩展：那么方法/函数如何调用？ Java 栈可用类比数据结构中栈，Java 栈中保存的主要内容是栈帧，每一次函数调用都会有一个对应的栈帧被压入Java栈，每一个函数调用结束后，都会有一个栈帧被弹出。 Java方法有两种返回方式： return 语句。 抛出异常。 不管哪种返回方式都会导致栈帧被弹出。 2.3 本地方法栈和虚拟机栈所发挥的作用非常相似，区别是： 虚拟机栈为虚拟机执行 Java 方法 （也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native 方法服务。 在 HotSpot 虚拟机中和 Java 虚拟机栈合二为一。 本地方法被执行的时候，在本地方法栈也会创建一个栈帧，用于存放该本地方法的局部变量表、操作数栈、动态链接、出口信息。 方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现 StackOverFlowError 和 OutOfMemoryError 两种异常。 2.4 堆Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例以及数组都在这里分配内存。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 上图所示的 eden区、s0区、s1区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden区-&gt;Survivor 区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 2.5 方法区方法区与 Java 堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 方法区也被称为永久代。很多人都会分不清方法区和永久代的关系，为此我也查阅了文献。 方法区和永久代的关系 《Java虚拟机规范》只是规定了有方法区这么个概念和它的作用，并没有规定如何去实现它。那么，在不同的 JVM 上方法区的实现肯定是不同的了。 方法区和永久代的关系很像Java中接口和类的关系，类实现了接口，而永久代就是HotSpot虚拟机对虚拟机规范中方法区的一种实现方式。 也就是说，永久代是HotSpot的概念，方法区是Java虚拟机规范中的定义，是一种规范，而永久代是一种实现，一个是标准一个是实现，其他的虚拟机实现并没有永久带这一说法。 常用参数JDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小 12-XX:PermSize=N //方法区(永久代)初始大小-XX:MaxPermSize=N //方法区(永久代)最大大小,超过这个值将会抛出OutOfMemoryError异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入方法区后就“永久存在”了。** JDK 1.8 的时候，方法区（HotSpot的永久代）被彻底移除了（JDK1.7就已经开始了），取而代之是元空间，元空间使用的是直接内存。 下面是一些常用参数： 12-XX:MetaspaceSize=N //设置Metaspace的初始（和最小大小）-XX:MaxMetaspaceSize=N //设置Metaspace的最大大小 与永久代很大的不同就是，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 为什么要将永久代(PermGen)替换为元空间(MetaSpace)呢?整个永久代有一个 JVM 本身设置固定大小上线，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，并且永远不会得到java.lang.OutOfMemoryError。你可以使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。 当然这只是其中一个原因，还有很多底层的原因，这里就不提了。 2.6 运行时常量池运行时常量池是方法区的一部分。Class 文件中除了有类的版本、字段、方法、接口等描述信息外，还有常量池信息（用于存放编译期生成的各种字面量和符号引用） 既然运行时常量池时方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出 OutOfMemoryError 异常。 JDK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 ——图片来源：https://blog.csdn.net/wangbiao007/article/details/78545189 2.7 直接内存直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁地使用。而且也可能导致 OutOfMemoryError 异常出现。 JDK1.4 中新加入的 NIO(New Input/Output) 类，引入了一种基于通道（Channel） 与缓存区（Buffer） 的 I/O 方式，它可以直接使用 Native 函数库直接分配堆外内存，然后通过一个存储在 Java 堆中的 DirectByteBuffer 对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆之间来回复制数据。 本机直接内存的分配不会收到 Java 堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制。 三 HotSpot 虚拟机对象探秘通过上面的介绍我们大概知道了虚拟机的内存情况，下面我们来详细的了解一下 HotSpot 虚拟机在 Java 堆中对象分配、布局和访问的全过程。 3.1 对象的创建下图便是 Java 对象的创建过程，我建议最好是能默写出来，并且要掌握每一步在做什么。 ①类加载检查： 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程。 ②分配内存： 在类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需的内存大小在类加载完成后便可确定，为对象分配空间的任务等同于把一块确定大小的内存从 Java 堆中划分出来。分配方式有 “指针碰撞” 和 “空闲列表” 两种，选择那种分配方式由 Java 堆是否规整决定，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 内存分配的两种方式：（补充内容，需要掌握） 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是”标记-清除”，还是”标记-整理”（也称作”标记-压缩”），值得注意的是，复制算法内存也是规整的 内存分配并发问题（补充内容，需要掌握） 在创建对象的时候有一个很重要的问题，就是线程安全，因为在实际开发过程中，创建对象是很频繁的事情，作为虚拟机来说，必须要保证线程是安全的，通常来讲，虚拟机采用两种方式来保证线程安全： CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。 TLAB： 为每一个线程预先在Eden区分配一块儿内存，JVM在给线程中的对象分配内存时，首先在TLAB分配，当对象大于TLAB中的剩余内存或TLAB的内存已用尽时，再采用上述的CAS进行内存分配 ③初始化零值： 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。 ④设置对象头： 初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是那个类的实例、如何才能找到类的元数据信息、对象的哈希吗、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。 ⑤执行 init 方法： 在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，&lt;init&gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 &lt;init&gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。 3.2 对象的内存布局在 Hotspot 虚拟机中，对象在内存中的布局可以分为3块区域：对象头、实例数据和对齐填充。 Hotspot虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的自身运行时数据（哈希码、GC分代年龄、锁状态标志等等），另一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是那个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容。 对齐填充部分不是必然存在的，也没有什么特别的含义，仅仅起占位作用。 因为Hotspot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说就是对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或2倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 3.3 对象的访问定位建立对象就是为了使用对象，我们的Java程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式有虚拟机实现而定，目前主流的访问方式有①使用句柄和②直接指针两种： 句柄： 如果使用句柄的话，那么Java堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自的具体地址信息； 直接指针： 如果使用直接指针访问，那么 Java 堆对象的布局中就必须考虑如何放置访问类型数据的相关信息，而reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。 四 重点补充内容String 类和常量池1 String 对象的两种创建方式： 12345String str1 = &quot;abcd&quot;;//先检查字符串常量池中有没有&quot;abcd&quot;，如果字符串常量池中没有，则创建一个，然后str1指向字符串常量池中的对象，如果有，则直接将str1指向&quot;abcd&quot;&quot;；String str2 = new String(&quot;abcd&quot;);//堆中创建一个新的对象String str3 = new String(&quot;abcd&quot;);//堆中创建一个新的对象System.out.println(str1==str2);//falseSystem.out.println(str2==str3);//false 这两种不同的创建方法是有差别的。 第一种方式是在常量池中拿对象； 第二种方式是直接在堆内存空间创建一个新的对象。 记住一点：只要使用new方法，便需要创建新的对象。 再给大家一个图应该更容易理解，图片来源：https://www.journaldev.com/797/what-is-java-string-pool： 2 String 类型的常量池比较特殊。它的主要使用方法有两种： 直接使用双引号声明出来的 String 对象会直接存储在常量池中。 如果不是用双引号声明的 String 对象，可以使用 String 提供的 intern 方法。String.intern() 是一个 Native 方法，它的作用是：如果运行时常量池中已经包含一个等于此 String 对象内容的字符串，则返回常量池中该字符串的引用；如果没有，则在常量池中创建与此 String 内容相同的字符串，并返回常量池中创建的字符串的引用。 123456String s1 = new String(&quot;计算机&quot;);String s2 = s1.intern();String s3 = &quot;计算机&quot;;System.out.println(s2);//计算机System.out.println(s1 == s2);//false，因为一个是堆内存中的String对象一个是常量池中的String对象，System.out.println(s3 == s2);//true，因为两个都是常量池中的String对象 3 String 字符串拼接 123456789String str1 = &quot;str&quot;;String str2 = &quot;ing&quot;;String str3 = &quot;str&quot; + &quot;ing&quot;;//常量池中的对象String str4 = str1 + str2; //在堆上创建的新的对象 String str5 = &quot;string&quot;;//常量池中的对象System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 尽量避免多个字符串拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 String s1 = new String(“abc”);这句话创建了几个字符串对象？将创建1或2个字符串。如果池中已存在字符串文字“abc”，则池中只会创建一个字符串“s1”。如果池中没有字符串文字“abc”，那么它将首先在池中创建，然后在堆空间中创建，因此将创建总共2个字符串对象。 验证： 1234String s1 = new String(&quot;abc&quot;);// 堆内存的地址值String s2 = &quot;abc&quot;;System.out.println(s1 == s2);// 输出false,因为一个是堆内存，一个是常量池的内存，故两者是不同的。System.out.println(s1.equals(s2));// 输出true 结果： 12falsetrue 8种基本类型的包装类和常量池 Java 基本类型的包装类的大部分都实现了常量池技术，即Byte,Short,Integer,Long,Character,Boolean；这5种包装类默认创建了数值[-128，127]的相应类型的缓存数据，但是超出此范围仍然会去创建新的对象。 两种浮点数类型的包装类 Float,Double 并没有实现常量池技术。 123456789Integer i1 = 33;Integer i2 = 33;System.out.println(i1 == i2);// 输出trueInteger i11 = 333;Integer i22 = 333;System.out.println(i11 == i22);// 输出falseDouble i3 = 1.2;Double i4 = 1.2;System.out.println(i3 == i4);// 输出false Integer 缓存源代码： 123456789/***此方法将始终缓存-128到127（包括端点）范围内的值，并可以缓存此范围之外的其他值。*/ public static Integer valueOf(int i) { if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } 应用场景： Integer i1=40；Java 在编译的时候会直接将代码封装成Integer i1=Integer.valueOf(40);，从而使用常量池中的对象。 Integer i1 = new Integer(40);这种情况下会创建新的对象。 123Integer i1 = 40;Integer i2 = new Integer(40);System.out.println(i1==i2);//输出false Integer比较更丰富的一个例子: 12345678910111213Integer i1 = 40;Integer i2 = 40;Integer i3 = 0;Integer i4 = new Integer(40);Integer i5 = new Integer(40);Integer i6 = new Integer(0);System.out.println(&quot;i1=i2 &quot; + (i1 == i2));System.out.println(&quot;i1=i2+i3 &quot; + (i1 == i2 + i3));System.out.println(&quot;i1=i4 &quot; + (i1 == i4));System.out.println(&quot;i4=i5 &quot; + (i4 == i5));System.out.println(&quot;i4=i5+i6 &quot; + (i4 == i5 + i6)); System.out.println(&quot;40=i5+i6 &quot; + (40 == i5 + i6)); 结果： 123456i1=i2 truei1=i2+i3 truei1=i4 falsei4=i5 falsei4=i5+i6 true40=i5+i6 true 解释： 语句i4 == i5 + i6，因为+这个操作符不适用于Integer对象，首先i5和i6进行自动拆箱操作，进行数值相加，即i4 == 40。然后Integer对象无法与数值进行直接比较，所以i4自动拆箱转为int值40，最终这条语句转为40 == 40进行数值比较。 参考 《深入理解Java虚拟机：JVM高级特性与最佳实践（第二版》 《实战java虚拟机》 https://docs.oracle.com/javase/specs/index.html http://www.pointsoftware.ch/en/under-the-hood-runtime-data-areas-javas-memory-model/ https://dzone.com/articles/jvm-permgen-%E2%80%93-where-art-thou https://stackoverflow.com/questions/9095748/method-area-and-permgen 文章引用于 Snailclimb","link":"/2019/04/16/%E5%8F%AF%E8%83%BD%E6%98%AF%E6%8A%8AJava%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E8%AE%B2%E7%9A%84%E6%9C%80%E6%B8%85%E6%A5%9A%E7%9A%84%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/"},{"title":"启动时查看配置文件application.yml","text":"Spring Boot Application 事件和监听器 在多环境的情况下。 可能需要切换配置文件的一个对应的属性来切换环境 面临的问题就是 如何在springboot加载完配置文件的时候就可以立即校验对应的属性值 SmartApplicationListener实现监听解耦 我们只需在加载完成之后去加入一个监听器。 就可以得到application.yml的内容。 不然再这个事件之前。都是拿不到对应的内容的 一、SmartApplicationListener介绍 Spring ApplicationEvent以及对应的Listener提供了一个事件监听、发布订阅的实现，内部实现方式是观察者模式，可以解耦业务系统之间的业务，提供系统的可拓展性、复用性以及可维护性。 在application.yml文件读取完会触发一个事件ConfigFileApplicationListener 该监听器实现文件的读取。 SmartApplicationListener是高级监听器，是ApplicationListener的子类，能够实现有序监听 SmartApplicationListener提供了两个方法： 123456789/** * 指定支持哪些类型的事件 */boolean supportsEventType(Class&lt;? extends ApplicationEvent&gt; var1);/** * 指定支持发生事件所在的类型 */boolean supportsSourceType(Class&lt;?&gt; var1); 二、ConfigFileApplicationListener ConfigFileApplicationListener是用来 读取配置文件的。 可以这样来粗劣的介绍一下 详情可以请看 springboot启动时是如何加载配置文件application.yml文件 三、直奔主题 新增一个监听器 既然我们要在配置文件加载之后搞事情那么我们直接复制ConfigFileApplicationListener 的实现方式 删除一下不需要处理的操作（大概就是以下代码） 并且order在ConfigFileApplicationListener 之后 1234567891011121314151617public class AfterConfigListener implements SmartApplicationListener,Ordered { public boolean supportsEventType(Class&lt;? extends ApplicationEvent&gt; eventType) { return ApplicationEnvironmentPreparedEvent.class.isAssignableFrom(eventType) || ApplicationPreparedEvent.class.isAssignableFrom(eventType); } public void onApplicationEvent(ApplicationEvent event) { if (event instanceof ApplicationEnvironmentPreparedEvent) { } if (event instanceof ApplicationPreparedEvent) { } } @Override public int getOrder() { // 写在加载配置文件之后 return ConfigFileApplicationListener.DEFAULT_ORDER + 1; }} 这样子就完成了配置文件之后的代码监听。 SmartApplicationListener又是实现了ApplicationListener的监听的，那么我们可以在onApplicationEvent执行代码。 完善代码如下。 监听并且获取配置文件内容 12345678910111213141516171819202122232425public class AfterConfigListener implements SmartApplicationListener,Ordered { public boolean supportsEventType(Class&lt;? extends ApplicationEvent&gt; eventType) { return ApplicationEnvironmentPreparedEvent.class.isAssignableFrom(eventType) || ApplicationPreparedEvent.class.isAssignableFrom(eventType); } public void onApplicationEvent(ApplicationEvent event) { if (event instanceof ApplicationEnvironmentPreparedEvent) { String banks = ((ApplicationEnvironmentPreparedEvent) event).getEnvironment().getProperty(&quot;spring.name&quot;); if (ToolUtil.isEmpty(BankEnum.getValue(banks))) { throw new RuntimeException(&quot;请检查 com.enums.BankEnum 中是否拥有该banks环境名字！&quot;); } } if (event instanceof ApplicationPreparedEvent) { } } @Override public int getOrder() { // 写在加载配置文件之后 return ConfigFileApplicationListener.DEFAULT_ORDER + 1; }} 并且在main方法中加入该监听器 123456789public class XProApplication { public static void main(String[] args) { SpringApplication springApplication = new SpringApplication(XProApplication.class); springApplication.addListeners(new AfterConfigListener()); springApplication.run(args); }}","link":"/2020/03/02/%E5%90%AF%E5%8A%A8%E6%97%B6%E6%9F%A5%E7%9C%8B%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6application-yml/"},{"title":"对象拷贝 - 优雅的解决方案 Mapstruct","text":"MapStruct GitHub 访问地址 : https://github.com/mapstruct/mapstruct/ 使用例子 : https://github.com/mapstruct/mapstruct-examples MapStrcut与其它工具对比以及使用说明! http://www.tuicool.com/articles/uiIRjai 是否一直在使用BeanUtils.copyProperties 用于对象属性拷贝。 出现种种小问题。 会将同名属性拷贝到另外一个对象中，操作方便但是存在一个缺陷 （速度慢） 有些同名字段却无法进行特殊化处理，将会导致不想修改的字段被覆盖。也不能自定义属性映射 在 mvc层 我们经常会DTO对象返回给前端 进行字段渲染。我们不喜欢将所有字段都显示给前端，或者我们需要修改字段返回给前端，例如 数据中存储的上架下架是0，1 但是前端需要的字段是true 和 false。 我们都得进行手动判断处理然后编辑成DTO返回给前端 MapStruct是一种类型安全的bean映射类生成java注释处理器。我们要做的就是定义一个映射器接口，声明任何必需的映射方法。在编译的过程中，MapStruct会生成此接口的实现。该实现使用纯java方法调用的源和目标对象之间的映射，MapStruct节省了时间，通过生成代码完成繁琐和容易出错的代码逻辑。。 MapStruct 拥有的优点： 使用普通方法调用而不是反射来快速执行，他会在编译器生成相应的 Impl 方法调用时直接通过简单的 getter/setter调用而不是反射或类似的方式将值从源复制到目标 编译时类型安全性 : 只能映射彼此的对象和属性，不能将商品实体意外映射到用户 DTO等 在构建时清除错误报告，如 映射不完整 (并非所有目标属性都被映射) 或 映射不正确(无法找到适当的映射方法或类型转换) MapStruct 提供的重要注解 : @Mapper : 标记这个接口作为一个映射接口，并且是编译时 MapStruct 处理器的入口 @Mapping : 解决源对象和目标对象中，属性名字不同的情况 Mappers.getMapper 自动生成的接口的实现可以通过 Mapper 的 class对象获取，从而让客户端可以访问 Mapper接口的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;properties&gt; // ... &lt;org.mapstruct.version&gt;1.2.0.Final&lt;/org.mapstruct.version&gt; &lt;/properties&gt; &lt;dependencies&gt; ... &lt;!-- MapStruct START --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-jdk8&lt;/artifactId&gt; &lt;version&gt;${org.mapstruct.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;${org.mapstruct.version}&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MapStruct END --&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;annotationProcessorPaths&gt; &lt;path&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;${org.mapstruct.version}&lt;/version&gt; &lt;/path&gt; &lt;/annotationProcessorPaths&gt; &lt;compilerArgs&gt; &lt;compilerArg&gt;-Amapstruct.defaultComponentModel=spring&lt;/compilerArg&gt; &lt;compilerArg&gt;-Amapstruct.suppressGeneratorTimestamp=true&lt;/compilerArg&gt; &lt;compilerArg&gt;-Amapstruct.suppressGeneratorVersionInfoComment=true&lt;/compilerArg&gt; &lt;/compilerArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; BasicObjectMapper包含了4个基本方法，单个和集合以及反转的单个和集合。开发中如需要对象转换操作可直接新建 interface 并继承 BasicObjectMapper，并在新建的接口上加上 @Mapper(componentModel = “spring”)，如果是属性中包含其它类以及该类已经存在 Mapper 则注解中加上 users = {类名.class}。componentModel = “spring” 该配置表示生成的实现类默认加上 spring @Component 注解，使用时可直接通过 @Autowire 进行注入 123456789101112131415161718192021222324252627public interface BasicObjectMapper&lt;SOURCE, TARGET&gt; { @Mappings({}) @InheritConfiguration TARGET to(SOURCE var1); @InheritConfiguration List&lt;TARGET&gt; to(List&lt;SOURCE&gt; var1); @InheritInverseConfiguration SOURCE from(TARGET var1); @InheritInverseConfiguration List&lt;SOURCE&gt; from(List&lt;TARGET&gt; var1); } 直接使用进行对象数据转换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293@Datapublic class ProductCategory { /** 类别编码 */ private String categoryCode; /** 类别名称 */ private String categoryName;} @Datapublic class CategoryVo { private String code; private String name;} import org.mapstruct.Mapper;import org.mapstruct.Mapping;import org.mapstruct.Mappings;import org.mapstruct.factory.Mappers;@Mapperpublic interface CategoryMapper extends BasicObjectMapper&lt;CategoryVo, ProductCategory&gt; { CategoryMapper MAPPER = Mappers.getMapper(CategoryMapper.class); @Mappings({ @Mapping(source = &quot;code&quot;, target = &quot;categoryCode&quot;), @Mapping(source = &quot;name&quot;, target = &quot;categoryName&quot;) }) ProductCategory to(CategoryVo source);}public static void main(String[] args) { CategoryMapper categoryMapper = CategoryMapper.MAPPER; CategoryVo vo = new CategoryVo(); vo.setCode(&quot;0000&quot;); vo.setName(&quot;属性名称&quot;); ProductCategory pc = categoryMapper.to(vo); // 通过 to方法得到 ProductCategory System.out.println(&quot;1&quot; + pc); CategoryVo vo1 = categoryMapper.from(pc); // 通过 from方法得到 CategoryVo，既反转 to方法 System.out.println(&quot;2&quot; + vo1); List&lt;ProductCategory&gt; pcList = categoryMapper.to(Arrays.asList(vo, vo1)); // 通过to方法从集合得到转换后的集合 System.out.println(&quot;3&quot; + pcList); List&lt;CategoryVo&gt; voList = categoryMapper.from(pcList); // 反转集合 System.out.println(&quot;4&quot; + voList);} 自定义方法添加到映射器 : 在某些情况下，需要手动实现 MapStruct 无法生成的从一种类型到另一种类型的特定映射，有如下两种实现方法 : 方法1&gt; 在另一个类上实现此类方法，然后由 MapStruct 生成的映射器使用该方法 方法2&gt; 在Java 8或更高版本时，可以直接在映射器界面中实现自定义方法作为默认方法。如果参数和返回类型匹配，生成的代码将调用默认方法 12345678910111213141516171819@Mapperpublic interface CarMapper { CarMapper MAPPER = Mappers.getMapper(CarMapper.class); @Mappings({...}) CarDto carToCarDto(Car car); default PersonDto personToPersonDto(Person person) { // hand-written mapping logic }} 映射器也可以定义为抽象类的形式而不是接口，并直接在此映射器类中实现自定义方法。在这种情况下，MapStruct将生成抽象类的扩展，并实现所有抽象方法。这种方法优于声明默认方法的优点是可以在映射器类中声明附加字段 1234567891011121314151617@Mapperpublic abstract class CarMapper { @Mappings(...) public abstract CarDto carToCarDto(Car car); public PersonDto personToPersonDto(Person person) { // hand-written mapping logic }} 多源参数映射方法 : MapStruct 支持多个源参数的映射方法，将几个实体组合成一个数据传输对象 123456789101112131415@Mapperpublic interface AddressMapper { @Mappings({ @Mapping(source = &quot;person.description&quot;, target = &quot;description&quot;), @Mapping(source = &quot;address.houseNo&quot;, target = &quot;houseNumber&quot;) }) DeliveryAddressDto personAndAddressToDeliveryAddressDto(Person person, Address address);} 如果多个源对象定义了一个具有相同名称的属性，则必须使用 @Mapping 注释来指定从中检索属性的源参数，如果这种歧义未得到解决，将会引发错误。对于在给定源对象中只存在一次的属性，指定源参数的名称是可选的，因为它可以自动确定 123456789101112131415161718MapStruct 还提供直接引用源参数@Mapperpublic interface AddressMapper { @Mappings({ @Mapping(source = &quot;person.description&quot;, target = &quot;description&quot;), @Mapping(source = &quot;hn&quot;, target = &quot;houseNumber&quot;) }) DeliveryAddressDto personAndAddressToDeliveryAddressDto(Person person, Integer hn);} 直接字段访问映射 : MapStruct 支持 public 没有 getter/setter 的字段的映射，如果 MapStruct 无法为属性找到合适的 getter/setter方法，MapStruct 将使用这些字段作为 读/写访问器。如果它是 public，则字段被认为是读取存取器 public final。如果一个字段 static 不被视为读取存取器只有在字段被认为是写入访问者的情况下 public。如果一个字段 final 和/或 static 它不被认为是写入访问者 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class Customer { private Long id; private String name; // getters and setter omitted for brevity} public class CustomerDto { public Long id; public String customerName;} @Mapperpublic interface CustomerMapper { CustomerMapper MAPPER = Mappers.getMapper( CustomerMapper.class ); @Mapping(source = &quot;customerName&quot;, target = &quot;name&quot;) Customer toCustomer(CustomerDto customerDto); @InheritInverseConfiguration CustomerDto fromCustomer(Customer customer);}// 生成的映射器如下public class CustomerMapperImpl implements CustomerMapper { @Override public Customer toCustomer(CustomerDto customerDto) { // ... customer.setId( customerDto.id ); customer.setName( customerDto.customerName ); // ... } @Override public CustomerDto fromCustomer(Customer customer) { // ... customerDto.id = customer.getId(); customerDto.customerName = customer.getName(); // ... }} 检索映射器 : Mapper实例 通过 org.mapstruct.factory.Mappers 的 getMapper() 方法来检索。通常 映射器接口应该定义一个名为的成员 INSTANCE ，它包含一个映射器类型的单个实例 : 123456789101112131415161718192021222324252627282930313233@Mapperpublic interface CarMapper { CarMapper INSTANCE = Mappers.getMapper(CarMapper.class); CarDto carToCarDto(Car car);}这种模式使客户非常容易地使用映射器对象，而无需反复实例化新的实例 :Car car = ...;CarDto dto = CarMapper.INSTANCE.carToCarDto( car ); 使用依赖注入 : 通过 Spring 依赖注入可以获取映射器对象@Mapper(componentModel = &quot;spring&quot;)public interface CarMapper { CarDto carToCarDto(Car car);}@Injectprivate CarMapper mapper; 数据类型转换 : 源对象和目标对象中映射的属性类型可能不同，MapStruct 提供自动处理类型转换，提供如下自动转换 : 1&gt; Java基本数据类型及其相应的包装类型，如 int 和 Integer，boolean 和 Boolean 等生成的代码是 null 转换一个包装型成相应的原始类型时一个感知，即 null 检查将被执行 2&gt; Java基本号码类型和包装类型，例如之间 int 和 long 或 byte 和 Integer (大类类型数据转换成小类可能出现精度损失) 3&gt; 所有Java基本类型之间 (包括其包装) 和 String 之间，例如 int 和 String 或 Boolean 和 String，java.text.DecimalFormat 均可以指定格式字符串 int 到 String的转换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596int 到 String的转换@Mapperpublic interface CarMapper { @Mapping(source = &quot;price&quot;, numberFormat = &quot;$#.00&quot;) CarDto carToCarDto(Car car); @IterableMapping(numberFormat = &quot;$#.00&quot;) List&lt;String&gt; prices(List&lt;Integer&gt; prices);}BigDecimal 转换为 String@Mapperpublic interface CarMapper { @Mapping(source = &quot;power&quot;, numberFormat = &quot;#.##E0&quot;) CarDto carToCarDto(Car car);}从日期到字符串的转换@Mapperpublic interface CarMapper { @Mapping(source = &quot;manufacturingDate&quot;, dateFormat = &quot;dd.MM.yyyy&quot;) CarDto carToCarDto(Car car); @IterableMapping(dateFormat = &quot;dd.MM.yyyy&quot;) List&lt;String&gt; stringListToDateList(List&lt;Date&gt; dates);} 映射对象引用 : 对象中如果包含另一个对象的引用，此时只需为引用的对象类型定义映射方法即可@Mapperpublic interface CarMapper { CarDto carToCarDto(Car car); PersonDto personToPersonDto(Person person);} # 映射器控制嵌套的bean映射@Mapperpublic interface FishTankMapper { @Mappings({ @Mapping(target = &quot;fish.kind&quot;, source = &quot;fish.type&quot;), @Mapping(target = &quot;fish.name&quot;, ignore = true), @Mapping(target = &quot;plant&quot;, ignore = true ), @Mapping(target = &quot;ornament&quot;, ignore = true ), @Mapping(target = &quot;material&quot;, ignore = true), @Mapping(target = &quot;ornament&quot;, source = &quot;interior.ornament&quot;), @Mapping(target = &quot;material.materialType&quot;, source = &quot;material&quot;), @Mapping(target = &quot;quality.report.organisation.name&quot;, source = &quot;quality.report.organisationName&quot;) }) FishTankDto map( FishTank source );} 调用其他映射器 : MapStruct 中可以调用在其他类中定义的映射方法，无论是由MapStruct生成的映射器还是手写映射方法 1234567891011121314151617181920212223242526272829303132333435363738# 手动实现的映射public class DateMapper { public String asString(Date date) { return date != null ? new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).format(date) : null; } public Date asDate(String date) { try { return date != null ? new SimpleDateFormat(&quot;yyyy-MM-dd&quot;).parse(date) : null; } catch (ParseException e) { throw new RuntimeException(e); } }} # 引用另一个映射器类@Mapper(uses = DateMapper.class)public class CarMapper { CarDto carToCarDto(Car car);} 当为该 carToCarDto() 方法的实现生成代码时，MapStruct将查找将 Date 对象映射到String的方法，在 DateMapper 该类上找到它并生成 asString() 用于映射该 manufacturingDate 属性的调用 映射集合 : 集合类型(映射 List，Set 等等) 以相同的方式映射 bean类型，通过定义与在映射器接口所需的源和目标类型的映射方法。生成的代码将包含一个遍历源集合的循环，转换每个元素并将其放入目标集合中。如果在给定的映射器或其使用的映射器中找到了集合元素类型的映射方法，则会调用此方法以执行元素转换。或者，如果存在源和目标元素类型的隐式转换，则将调用此转换例程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990@Mapperpublic interface CarMapper { Set&lt;String&gt; integerSetToStringSet(Set&lt;Integer&gt; integers); List&lt;CarDto&gt; carsToCarDtos(List&lt;Car&gt; cars); CarDto carToCarDto(Car car);} # 生成的集合映射方法@Overridepublic Set&lt;String&gt; integerSetToStringSet(Set&lt;Integer&gt; integers) { if (integers == null) { return null; } Set&lt;String&gt; set = new HashSet&lt;&gt;(); for (Integer integer : integers) { set.add(String.valueOf(integer)); } return set;} @Overridepublic List&lt;CarDto&gt; carsToCarDtos(List&lt;Car&gt; cars) { if (cars == null) { return null; } List&lt;CarDto&gt; list = new ArrayList&lt;&gt;(); for (Car car : cars) { list.add(carToCarDto(car)); } return list;} 映射Map :public interface SourceTargetMapper { @MapMapping(valueDateFormat = &quot;dd.MM.yyyy&quot;) Map&lt;String, String&gt; longDateMapToStringStringMap(Map&lt;Long, Date&gt; source);} 映射流 :@Mapperpublic interface CarMapper { Set&lt;String&gt; integerStreamToStringSet(Stream&lt;Integer&gt; integers); List&lt;CarDto&gt; carsToCarDtos(Stream&lt;Car&gt; cars); CarDto carToCarDto(Car car);} 映射枚举 : 默认情况下，源枚举中的每个常量映射到目标枚举类型中具有相同名称的常量。如果需要，可以使用 @ValueMapping 注释帮助将source enum中的常量映射为具有其他名称的常量 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Mapperpublic interface OrderMapper { OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class); @ValueMappings({ @ValueMapping(source = &quot;EXTRA&quot;, target = &quot;SPECIAL&quot;), @ValueMapping(source = &quot;STANDARD&quot;, target = &quot;DEFAULT&quot;), @ValueMapping(source = &quot;NORMAL&quot;, target = &quot;DEFAULT&quot;) }) ExternalOrderType orderTypeToExternalOrderType(OrderType orderType);} 默认值和常量 : @Mapper(uses = StringListMapper.class)public interface SourceTargetMapper { SourceTargetMapper INSTANCE = Mappers.getMapper(SourceTargetMapper.class); @Mappings({ @Mapping(target = &quot;stringProperty&quot;, source = &quot;stringProp&quot;, defaultValue = &quot;undefined&quot;), @Mapping(target = &quot;longProperty&quot;, source = &quot;longProp&quot;, defaultValue = &quot;-1&quot;), @Mapping(target = &quot;stringConstant&quot;, constant = &quot;Constant Value&quot;), @Mapping(target = &quot;integerConstant&quot;, constant = &quot;14&quot;), @Mapping(target = &quot;longWrapperConstant&quot;, constant = &quot;3001&quot;), @Mapping(target = &quot;dateConstant&quot;, dateFormat = &quot;dd-MM-yyyy&quot;, constant = &quot;09-01-2014&quot;), @Mapping(target = &quot;stringListConstants&quot;, constant = &quot;jack-jill-tom&quot;) }) Target sourceToTarget(Source s);} 表达式 :@Mapperpublic interface SourceTargetMapper { SourceTargetMapper INSTANCE = Mappers.getMapper(SourceTargetMapper.class); @Mapping(target = &quot;timeAndFormat&quot;, expression = &quot;java( new org.sample.TimeAndFormat( s.getTime(), s.getFormat() ) )&quot;) Target sourceToTarget(Source s);} 确定结果类型 : 当结果类型具有继承关系时，选择映射方法(@Mapping) 或工厂方法(@BeanMapping) 可能变得不明确。假设一个Apple和一个香蕉，这两个都是 Fruit的专业 123456789101112131415161718192021222324252627@Mapper(uses = FruitFactory.class)public interface FruitMapper { @BeanMapping(resultType = Apple.class) Fruit map(FruitDto source);} public class FruitFactory { public Apple createApple() { return new Apple(&quot;Apple&quot;); } public Banana createBanana() { return new Banana(&quot;Banana&quot;); }} 控制 ‘空’ 参数的映射结果 : 默认情况下 null 会返回，通过指定 nullValueMappingStrategy = NullValueMappingStrategy.RETURN_DEFAULT 上 @BeanMapping，@IterableMapping，@MapMapping，或全局上 @Mapper 或 @MappingConfig，映射结果可以被改变以返回空默认值 1&gt; Bean映射 : 将返回一个 ‘空’ 目标bean，除常量和表达式外，它们将在存在时填充 2&gt; 基元 : 基元的默认值将被返回，例如 false for boolean 或 0 for int 3&gt; Iterables/Arrays : 一个空的迭代器将被返回 4&gt; 地图 : 将返回空白地图 共享配置 : 通过指向中心接口来定义共享配置的可能性 @MapperConfig，要使映射器使用共享配置，需要在 @Mapper#config 属性中定义配置界面。该 @MapperConfig 注释具有相同的属性 @Mapper 注释。任何未通过的属性 @Mapper 都将从共享配置继承。指定 @Mapper 的属性优先于通过引用的配置类指定的属性 123456789@MapperConfig(uses = CustomMapperViaMapperConfig.class, unmappedTargetPolicy = ReportingPolicy.ERROR)public interface CentralConfig {} @Mapper(config = CentralConfig.class, uses = { CustomMapperViaMapper.class } )public interface SourceTargetMapper {}","link":"/2019/05/29/%E5%AF%B9%E8%B1%A1%E6%8B%B7%E8%B4%9D-%E4%BC%98%E9%9B%85%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88-Mapstruct/"},{"title":"拦截器和过滤器的区别","text":"1.过滤器：依赖于servlet容器。在实现上基于函数回调，可以对几乎所有请求进行过滤，但是缺点是一个过滤器实例只能在容器初始化时调用一次。使用过滤器的目的是用来做一些过滤操作，获取我们想要获取的数据，比如：在过滤器中修改字符编码；在过滤器中修改HttpServletRequest的一些参数，包括：过滤低俗文字、危险字符等 2.拦截器：依赖于web框架，在SpringMVC中就是依赖于SpringMVC框架。在实现上基于Java的反射机制，属于面向切面编程（AOP）的一种运用。由于拦截器是基于web框架的调用，因此可以使用Spring的依赖注入（DI）进行一些业务操作，同时一个拦截器实例在一个controller生命周期之内可以多次调用。但是缺点是只能对controller请求进行拦截，对其他的一些比如直接访问静态资源的请求则没办法进行拦截处理3.过滤器和拦截器的区别： ①拦截器是基于java的反射机制的，而过滤器是基于函数回调。 ②拦截器不依赖与servlet容器，过滤器依赖与servlet容器。 ③拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。 ④拦截器可以访问action上下文、值栈里的对象，而过滤器不能访问。 ⑤在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次。 ⑥拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。过滤器 过滤器123456@Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)throws IOException, ServletException { System.out.println(&quot;before...&quot;); chain.doFilter(request, response); System.out.println(&quot;after...&quot;); } chain.doFilter(request, response);这个方法的调用作为分水岭。事实上调用Servlet的doService()方法是在chain.doFilter(request, response);这个方法中进行的。 拦截器拦截器是被包裹在过滤器之中的。 1234567891011121314@Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler)throws Exception { System.out.println(&quot;preHandle&quot;); returntrue; } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView)throws Exception { System.out.println(&quot;postHandle&quot;); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex)throws Exception { System.out.println(&quot;afterCompletion&quot;); } a.preHandle()这个方法是在过滤器的chain.doFilter(request, response)方法的前一步执行，也就是在 [System.out.println(“before…”)][chain.doFilter(request, response)]之间执行。 b.preHandle()方法之后，在return ModelAndView之前进行，可以操控Controller的ModelAndView内容。 c.afterCompletion()方法是在过滤器返回给前端前一步执行，也就是在[chain.doFilter(request, response)][System.out.println(“after…”)]之间执行。 SpringMVC的机制是由同一个Servlet来分发请求给不同的Controller，其实这一步是在Servlet的service()方法中执行的。所以过滤器、拦截器、service()方法，dispatc()方法的执行顺序应该是这样的，大致画了个图：其实非常好测试，自己写一个过滤器，一个拦截器，然后在这些方法中都加个断点，一路F8下去就得出了结论。","link":"/2019/05/20/%E6%8B%A6%E6%88%AA%E5%99%A8%E5%92%8C%E8%BF%87%E6%BB%A4%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"带你看懂分布式事务","text":"转自Java Geek Tech 4月初在面试一家互联网公司的过程中就被问到了分布式事务问题。我又一次在没有好好整理的问题上吃了亏，记录一下，还是长记性！！！ 背景四月初，去面试了本市的一家之前在做办公室无人货架的公司，虽然他们现在在面临着转型，但是对于我这种想从传统企业往互联网行业走的孩子来说，还是比较有吸引力的。 在面试过程中就提到了分布式事务问题。我又一次在没有好好整理的问题上吃了亏，记录一下，还是长记性 ！！！ 先看面试过程面试官先是在纸上先画了这样一张图： 让我看这张图按照上面的流程走，有没有什么问题？面试官并没有直接说出来这里面会有分布式事务的问题，而是让我来告诉他，这就是面试套路呀。 我回答了这中间可能存在分布式事务的问题，当步骤 2 在调用 B 系统时，可能存在B 系统处理完成后，在响应的时候超时，导致 A 系统误认为 B 处理失败了，从而导致A 系统回滚，跟 B 系统存在数据不一致的情况。 ok ，我回答到这里，应该回答了面试官的第一层意思，至少我有这种意识，他点了点头。 接着，他继续问：“那你有什么好的解决方式吗？” 此时我脑子里面只有两阶段提交的大概流程图的印象，然后巴卡巴拉的跟他说了一番，什么中间来个协调者呀，先预提交什么的，如果有失败，就 rollback，如果 ok，再真正的提交事务，就是网上这些大神们说的这些理论。 然后面试官就继续问：那A 在调用 B 的这条线断了，你们代码具体是怎么处理的呢 ？怎么来做到 rollback 的呢 ？说说你代码怎么写的。 此时，我懵了。 最后结果，大家肯定也能猜到，凉凉。 什么是事务这里我们说的事务一般是指 数据库事务，简称 事务，是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。维基百科中这么说的。 用转账的例子来说，A 账户要给 B 账户转 100块，这中间至少包含了两个操作： A 账户 减 100 块 B 账户 加 100 块 在支持事务的数据库管理系统来说，就是得确保上面两个操作（整个“事务”）都能完成，不能存在，A 的100块扣了，然后B 的账户又没加上去的情况。 数据库事务包含了四个特性，分别是： 原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。对于转账来说，A账户扣钱，B 账户加钱，要么同时成功，要么同时失败。 一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束 隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。其他账户在转账的时候，不能影响到上面的 A 跟 B 之前的交易。 持久性（Durability）：已被提交的事务对数据库的修改应该永久保存在数据库中。 什么是分布式事务我们知道，上面的转账 我们是在一个数据库中的事务操作。我们可以使用一些框架 比如 Spring 的事务管理器来给我们统一搞定。 但是如果我们系统中出现垮库操作，比如一个操作中，我需要操作多个库，或者说这个操作会垮应用之前的调用，那么Spring 的事务管理机制就对这种场景没有办法了。 就像上面面试题中出现的问题一样，在系统 A 的步骤 2 在远程调用 B 的时候，由于网络超时，导致B 没有正常响应，但是A 这边调用失败，进行了回滚，而 B 又提交了事务。此时就可能会导致数据不一致的情况，参生分布式事务的问题。 分布式事务的存在，就是解决数据不一致的情况。 为什么我们要保证一致性CAP 理论分布式系统中有这么一个广为流传的理论：CAP 定理 这个定理呀，起源于加州大学柏克莱分校（University of California, Berkeley）的计算机科学家埃里克·布鲁尔在 2000年的分布式计算原理研讨会（PODC）上提出的一个猜想。后来在2002年，麻省理工学院（MIT）的赛斯·吉尔伯特和南希·林奇发表了布鲁尔猜想的证明，使之成为一个定理。【摘自维基百科】 他说呀，对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistency） 可用性（Availability） 分区容错性（Partition tolerance） 而一个分布式系统最多只能满足其中的两项。 那么，上面的三点分别是什么玩意儿？为什么又只能同时满足两项呢？ 我们先看这样一个场景，现在我们系统部署了两份（两个节点，web1 和 web2 ）,同样的业务代码，但是维护的是自己这个节点生成的数据。但是用户访问进来，可能会访问到不同的节点。但是不管是访问web1 还是web2 ,在用户参数数据 过后，这个数据都必须得同步到另外的节点，让用户不管访问哪个节点，都是响应他需要的数据。如下图： 分区容错性我们先说 分区容错性：也就是说呀，就算上面这两个节点之间发生了网络故障，无法发生同步的问题，但是用户访问进来，不管到哪个节点，这个节点都得单独提供服务，这一点对于互联网公司来说，是必须要满足的。 当 web1 和 web2 之间的网络发生故障，导致数据无法进行同步。用户在web1 上写了数据，马上又访问进来读取数据，请求到了 web2，但是此时 web2 是没有数据的。那么我们是 给用户返回 null ？还是说给一些提示，说系统不可用，稍后重试呢？ 都不妥吧，兄弟。 一致性如果要保证可用性，那么有数据的节点返回数据，没数据的节点返回 null ,就会出现用户那里看到的是一会儿有数据，一会儿没有数据，此时就存在 一致性 的问题。 可用性如果保证一致性，那么在用户访问的时候，不管 web1 还是web2 ，我们可能会返回一些提示信息，说系统不可用，稍后再试等等，保证每次都是一致的。明明我们有数据在，但是我们系统却响应的是提示信息，此时就是 可用性 的问题。 由于分区容错性（P）是必须保证的，那么我们分布式系统就更多是在一致性（CP） 和可用性（AP）上做平衡了，只能同时满足两个条件。 其实，大家想想，ZK 是不是就是严格实现了 CP ，而 Eureka 则是保证了 AP。 其实分布式事务强调的就是一致性。 几种分布式事务解决方案2PC在说 2PC 之前，我们先了解一下 XA规范 是个什么东西？ XA规范 描述了全局的事务管理器与局部的资源管理器之间的接口。XA规范的目的是允许多个资源（如数据库，应用服务器，消息队列，等等）在同一事务中访问，这样可以使ACID属性跨越应用程序而保持有效。 XA 使用 两阶段提交（2PC） 来保证所有资源同时提交或回滚任何特定的事务。 大家想一个场景，在做单应用的时候，有的同学连过两个库吧？在一个事务中会同时向两个系统插入数据。但是对于普通事务来讲，是管不了的。 看下图（只是举例这种操作的套路，不局限于下面的业务）： 一个服务里面要去操作两个库，如何保证事务成功呢。 这里我们介绍一个框架 Atomikos ，他就是实现了这种 XA 的套路。看代码： 具体代码移步 Github AtomikosJTATest: https://github.com/heyxyw/learn/blob/master/distributed-transaction/src/main/java/com/zhouq/jta/AtomikosJTATest.java 看到上面的图了哇，Atomikos 自己实现了一个事务管理器。我们获取的连接都从它哪里拿。 第一步先开启事务，然后进行预提交，db1 和 db2 都先进行预先执行，注意：这里没有提交事务。 第二步才是真正的提交事务，由 Atomikos 来发起提交的，如果出现异常则发起回滚操作。 这个过程是不是就有两个角色了，一个 事务管理器，一个资源管理器（我们这里是 数据库，也可以是其他的组件，消息队列什么的）。 整个执行过程是这样：上图是正常情况，下图是一方出现故障的情况。 图片来自：XA 事务处理：https://www.infoq.cn/article/xa-transactions-handle ，具体关于XA 的详细讲解，可以好好看看。整个2PC 的流程： 第一阶段（提交请求阶段）： 协调者节点向所有参与者节点询问是否可以执行提交操作，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。 **第二阶段 (提交执行阶段)**： 成功，当协调者节点从所有参与者节点获得的相应消息都为”同意”时： 协调者节点向所有参与者节点发出”正式提交”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点收到所有参与者节点反馈的”完成”消息后，完成事务。 失败，如果任一参与者节点在第一阶段返回的响应消息为”终止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出”回滚操作”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点收到所有参与者节点反馈的”回滚完成”消息后，取消事务。 有时候，第二阶段也被称作完成阶段，因为无论结果怎样，协调者都必须在此阶段结束当前事务。 可靠消息最终一致性方案基于普通的消息队列中间件上面我们说了两阶段提交的方案，接下来我们讲讲怎么基于可靠消息最终一致性方案来解决分布式事务的问题。 这个方案，就有消息服务中间件角色参与进来了。我们先看一个大提的流程图： 我们以创建订单下单过程和 后面出库 的流程为例来讲述上面的图。 在下单逻辑里面（Producer 端），我们先生成一个订单的数据，比如订单号，数量等关键的信息，先包装成一条消息，并把消息的状态置为 init ,然后发送到 独立消息服务中，并且入库。 接下来继续处理 下单的其他本地的逻辑。 处理完成后，走到确认发送消息这一步，说明我的订单是能够下成功的。那么我们再向消息服务里面发送一条confirm 的消息，消息服务里面就可以把这个订单的消息状态修改为 send 并且，发送到消息队列里面。 接下来，消费者端去消费这条消息。处理自己这边的逻辑，处理完成以后，然后反馈消息处理结果到独立消息服务，独立消息服务把消息状态置为 end 状态 ,表示结束。但是得注意保证接口的幂等性，避免重复消费带来的问题。 这里面可能出现的问题，以及各个步骤怎么解决的： 比如在 prepare 阶段就发生异常，那么这里订单这块都不会下成功。但是我们说，我们这里是基于可靠消息，得保证我们的消息服务是正常的。 在 comfirm 出现异常，此时发送确认失败，但是我们的单已经下成功了。这种情况，我们就可以在独立消息服务中起一个定时任务，定时去查询 消息状态为 init 的数据，去反向查询 订单系统中的单号是否存在，如果存在，那么我们就把消息置为 send 状态，然后发送到 消息队列里面，如果查询到不存在的订单，那么就直接抛弃掉这条消息。所以这里我们的订单系统得提供批量查询订单的接口，还有下游的消费系统得保证幂等。保证重复消费的一致性。 消息队列丢消息或者下游系统一直处理失败，导致没有消息反馈过来，出现一直是 send 状态的消息。此时独立消息我们还需要一个定时任务，就是处理这种 send 状态的消息，我们可以进行重发，直到后面系统消费成功为止。 最后消费者这端，我们在消费的时候，如果出现消费异常，或者是系统bug 导致异常的情况。那么这里我们还可以去记录日志，如果不是系统代码问题，是网络抖动导致的，那么在上面第三种情况，消息系统会重新发送消息，我们再处理就是。如果是一直失败，你就要考虑是不是你的代码真的有问题，有bug 了吧。 最后的保底方案，记录日志，出现问题人肉处理数据。现在我们系统出现错误，以目前的技术手段是没办法做到都靠机器去解决的，都得靠我们人。据我了解，现在很多大厂都会有这样的人，专门处理这种类型的问题，手动去修改数据库的方式。我们之前待的小厂，基本上都是靠我们自己去写 sql 去修改数据的，想想，是不是？ 贴一下关键的独立消息服务核心逻辑代码框架： 定时任务： 基于 RocketMQ实现这种方案，跟上面的独立消息服务一致，这里直接去掉独立服务，只利用消息队列来实现，也就是阿里的 RocketMQ 。 流程图如下： 这里的整个流程跟上面基于消息服务是一致的。这里就不过多阐述，具体代码实现请参考 ：https://www.jianshu.com/p/453c6e7ff81c ，写得非常好。 针对这里的 可靠消息最终一致性方案 来说，我们说的 可靠 是指保证消息一定能发送到消息中间件里面去，保证这里可靠。 对于下游的系统来说，消费不成功，一般来说就是采取失败重试，重试多次不成功，那么就记录日志，后续人工介入来进行处理。所以这里得强调一下，后面的系统，一定要处理 幂等，重试，日志 这几个东西。 如果是对于资金类的业务，后续系统回滚了以后，得想办法去通知前面的系统也进行回滚，或者是发送报警由人工来手工回滚和补偿。 TCC 方案TCC 的全程分为三个阶段，分别是 Try、Confirm、Cancel： Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留 Confirm阶段：这个阶段说的是在各个服务中执行实际的操作 Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作 还是以转账的例子为例，在跨银行进行转账的时候，需要涉及到两个银行的分布式事务，从A 银行向 B 银行转 1 块，如果用TCC 方案来实现： 大概思路就是这样的： Try 阶段：先把A 银行账户先冻结 1 块，B银行账户中的资金给预加 1 块。 Confirm 阶段：执行实际的转账操作，A银行账户的资金扣减 1块，B 银行账户的资金增加 1 块。 Cancel 阶段：如果任何一个银行的操作执行失败，那么就需要回滚进行补偿，就是比如A银行账户如果已经扣减了，但是B银行账户资金增加失败了，那么就得把A银行账户资金给加回去。 这种方案就比较复杂了，一步操作要做多个接口来配合完成。 以 ByteTCC 框架的实现例子来大概描述一下上面的流程，示例地址 https://gitee.com/bytesoft/ByteTCC-sample/tree/master/dubbo-sample 最开始 A 银行账户 与 B 银行账户都分别为：amount（数量）=1000，frozen（冻结金额）= 0 从A银行账户发起转账到 B 银行账户 1 块： try 阶段：A 银行账户金额减 1，冻结金额 加 1，B 银行 账户 冻结金额加 1 。 此时： A 银行账户：amount（数量）= 1000 - 1 = 999，frozen（冻结金额）= 0 + 1 = 1 B 银行账户：amount（数量）= 1000，frozen（冻结金额）= 0 + 1 = 1 confirm 阶段 ： A银行账户冻结金额 减 1，B 银行账户金额 加 1，冻结金额 减 1 此时： A 银行账户：amount（数量）= 999，frozen（冻结金额）= 1 - 1 = 0 B 银行账户：amount（数量）= 1000 + 1 = 1001，frozen（冻结金额）= 1 - 1 = 0 cancel 阶段： A 银行账户金额 + 1，冻结金额 -1 ，B 银行 冻结金额 -1 此时： A 银行账户：amount（数量）= 999 + 1 = 1000，frozen（冻结金额）= 1 - 1 = 0 B 银行账户：amount（数量）= 1000，frozen（冻结金额）= 1 - 1 = 0 至此，整个过程就演示完毕，大家记得跑一遍代码。其实还是蛮复杂的，有许多接口一起来配合完成整个业务，试想一下，如果我们项目中大量用到 TCC 来写，你受得了？ 再提一下 BASE理论BASE 理论是 Basically Available(基本可用)，Soft State（软状态）和Eventually Consistent（最终一致性）三个短语的缩写。 基本可用（Basically Available）： 指分布式系统在出现不可预知故障的时候，允许损失部分可用性。 软状态（ Soft State）：指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。 最终一致（ Eventual Consistency）：强调的是所有的数据更新操作，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 其核心思想是： 即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency） 到这里大家再想想， 上面 TCC 方案中的账户设计了一个冻结字段 frozen ，这里是不是就是 BASE理论 中间的 软状态 呢 ？ 最后对存在非常多的微服务的公司来说，服务之间的调用异常的复杂，那么在引入分布式事务的过程中，你需要考虑加入分布式事务后，系统实现起来的复杂性和开发成本，或者说哪些地方根本就不需要搞分布式事务。 其实没必要到处都搞分布式事务，对于大多数的业务来说，其实我们并不需要做分布式事务，直接做日志，做监控就好了。然后出现问题，手工去处理，一个月也不会有那么多的问题的。如果你天天都出现这些问题，你是不是要好好去排查排查你的代码Bug了。 对于资金类的场景，那么基本上会采用分布式事务方案来保证，像其他的服务，会员，积分，商品信息呀这些，可能就不需要这么去搞了。","link":"/2019/04/22/%E5%B8%A6%E4%BD%A0%E7%9C%8B%E6%87%82%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"title":"搞定JVM垃圾回收就是这么简单","text":"写在前面本节常见面试题：问题答案在文中都有提到 如何判断对象是否死亡（两种方法）。 简单的介绍一下强引用、软引用、弱引用、虚引用（虚引用与软引用和弱引用的区别、使用软引用能带来的好处）。 如何判断一个常量是废弃常量 如何判断一个类是无用的类 垃圾收集有哪些算法，各自的特点？ HotSpot为什么要分为新生代和老年代？ 常见的垃圾回收器有那些？ 介绍一下CMS,G1收集器。 Minor Gc和Full GC 有什么不同呢？ 本文导火索 当需要排查各种 内存溢出问题、当垃圾收集成为系统达到更高并发的瓶颈时，我们就需要对这些“自动化”的技术实施必要的监控和调节。 1 揭开JVM内存分配与回收的神秘面纱Java 的自动内存管理主要是针对对象内存的回收和对象内存的分配。同时，Java 自动内存管理最核心的功能是 堆 内存中对象的分配与回收。 Java 堆是垃圾收集器管理的主要区域，因此也被称作GC堆（Garbage Collected Heap）.从垃圾回收的角度，由于现在收集器基本都采用分代垃圾收集算法，所以 Java 堆还可以细分为：新生代和老年代：再细致一点有：Eden空间、From Survivor、To Survivor空间等。进一步划分的目的是更好地回收内存，或者更快地分配内存。 堆空间的基本结构： 上图所示的 eden区、s0区、s1区都属于新生代，tentired 区属于老年代。大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 s0 或者 s1，并且对象的年龄还会加 1(Eden区-&gt;Survivor 区后对象的初始年龄变为1)，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 1.1 对象优先在eden区分配目前主流的垃圾收集器都会采用分代回收算法，因此需要将堆内存分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 大多数情况下，对象在新生代中 eden 区分配。当 eden 区没有足够空间进行分配时，虚拟机将发起一次Minor GC.下面我们来进行实际测试以下。 在测试之前我们先来看看 Minor GC和Full GC 有什么不同呢？ 新生代GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC非常频繁，回收速度一般也比较快。 老年代GC（Major GC/Full GC）:指发生在老年代的GC，出现了Major GC经常会伴随至少一次的Minor GC（并非绝对），Major GC的速度一般会比Minor GC的慢10倍以上。 测试： 12345678public class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2; allocation1 = new byte[30900*1024]; //allocation2 = new byte[900*1024]; }} 通过以下方式运行： 添加的参数：-XX:+PrintGCDetails 运行结果(红色字体描述有误，应该是对应于JDK1.7的永久代)： 从上图我们可以看出eden区内存几乎已经被分配完全（即使程序什么也不做，新生代也会使用2000多k内存）。假如我们再为allocation2分配内存会出现什么情况呢？ 1allocation2 = new byte[900*1024]; 简单解释一下为什么会出现这种情况： 因为给allocation2分配内存的时候eden区内存几乎已经被分配完了，我们刚刚讲了当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC.GC期间虚拟机又发现allocation1无法存入Survivor空间，所以只好通过 分配担保机制 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放allocation1，所以不会出现Full GC。执行Minor GC后，后面分配的对象如果能够存在eden区的话，还是会在eden区分配内存。可以执行如下代码验证： 123456789101112public class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2,allocation3,allocation4,allocation5; allocation1 = new byte[32000*1024]; allocation2 = new byte[1000*1024]; allocation3 = new byte[1000*1024]; allocation4 = new byte[1000*1024]; allocation5 = new byte[1000*1024]; }} 1.2 大对象直接进入老年代大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 为什么要这样呢？ 为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。 1.3 长期存活的对象将进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。 如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 1.4 动态对象年龄判定为了更好的适应不同程序的内存情况，虚拟机不是永远要求对象年龄必须达到了某个值才能进入老年代，如果 Survivor 空间中相同年龄所有对象大小的总和大于 Survivor 空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无需达到要求的年龄。 2 对象已经死亡？堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断那些对象已经死亡（即不能再被任何途径使用的对象）。 2.1 引用计数法给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，如下面代码所示：除了对象objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数算法无法通知 GC 回收器回收他们。 123456789101112public class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; }} 2.2 可达性分析算法这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 2.3 再谈引用无论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与“引用”有关。 JDK1.2之前，Java中引用的定义很传统：如果reference类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用。 JDK1.2以后，Java对引用的概念进行了扩充，将引用分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱） 1．强引用 以前我们使用的大部分引用实际上都是强引用，这是使用最普遍的引用。如果一个对象具有强引用，那就类似于必不可少的生活用品，垃圾回收器绝不会回收它。当内存空 间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。 2．软引用（SoftReference） 如果一个对象只具有软引用，那就类似于可有可无的生活用品。如果内存空间足够，垃圾回收器就不会回收它，如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，JAVA虚拟机就会把这个软引用加入到与之关联的引用队列中。 3．弱引用（WeakReference） 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 4．虚引用（PhantomReference） “虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。 虚引用与软引用和弱引用的一个区别在于： 虚引用必须和引用队列（ReferenceQueue）联合使用。当垃 圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。程序可以通过判断引用队列中是 否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 特别注意，在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速JVM对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生。 2.4 不可达的对象并非“非死不可”即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程；可达性分析法中不可达的对象被第一次标记并且进行一次筛选，筛选的条件是此对象是否有必要执行 finalize 方法。当对象没有覆盖 finalize 方法，或 finalize 方法已经被虚拟机调用过时，虚拟机将这两种情况视为没有必要执行。 被判定为需要执行的对象将会被放在一个队列中进行第二次标记，除非这个对象与引用链上的任何一个对象建立关联，否则就会被真的回收。 2.5 如何判断一个常量是废弃常量运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？ 假如在常量池中存在字符串 “abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。 注意：我们在 可能是把Java内存区域讲的最清楚的一篇文章 也讲了JDK1.7及之后版本的 JVM 已经将运行时常量池从方法区中移了出来，在 Java 堆（Heap）中开辟了一块区域存放运行时常量池。 2.6 如何判断一个类是无用的类方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？ 判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 3 垃圾收集算法 3.1 标记-清除算法算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，效率也很高，但是会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 3.2 复制算法为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 3.3 标记-整理算法根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 3.4 分代收集算法当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。 比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 延伸面试问题： HotSpot为什么要分为新生代和老年代？ 根据上面的对分代收集算法的介绍回答。 4 垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 虽然我们对各个收集器进行比较，但并非要挑选出一个最好的收集器。因为知道现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的HotSpot虚拟机就不会实现那么多不同的垃圾收集器了。 4.1 Serial收集器Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 新生代采用复制算法，老年代采用标记-整理算法。 虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。 但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择。 4.2 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。 新生代采用复制算法，老年代采用标记-整理算法。 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 并行和并发概念补充： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。 4.3 Parallel Scavenge收集器Parallel Scavenge 收集器类似于ParNew 收集器。 那么它有什么特别之处呢？ 12345678-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行-XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行 Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，手工优化存在的话可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。 新生代采用复制算法，老年代采用标记-整理算法。 4.4.Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 4.5 Parallel Old收集器 Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 4.6 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用。 CMS（Concurrent Mark Sweep）收集器是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤： 初始标记： 暂停所有的其他线程，并记录下直接与root相连的对象，速度很快 ； 并发标记： 同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除： 开启用户线程，同时GC线程开始对为标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对CPU资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 4.7 G1收集器G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征. 被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备一下特点： 并行与并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 空间整合：与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内。 G1收集器的运作大致分为以下几个步骤： 初始标记 并发标记 最终标记 筛选回收 **G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)**。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）。 文章引用于 Snailclimb 参考： 《深入理解Java虚拟机：JVM高级特性与最佳实践（第二版》 https://my.oschina.net/hosee/blog/644618","link":"/2019/04/16/%E6%90%9E%E5%AE%9AJVM%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%B0%B1%E6%98%AF%E8%BF%99%E4%B9%88%E7%AE%80%E5%8D%95/"},{"title":"教你你快捷编程 --- 将List&lt;User&gt; 对象的id快速抽取出来","text":"在编程过程中 我们总是会遇到 需要将某个集合中的对象的id或者某个属性快速抽取出来。 那么我们使用jdk8 的方法 快速的抽取你想要的属性集合 啥也不说了 上代码！ List&lt;Admin&gt; adminList -&gt; Set&lt;Integer&gt; adminSet (id) 12345678910111213141516171819202122232425262728@Datapublic class Admin { private Integer id; private Integer name;} public static void main(String[] args) { List&lt;Admin&gt; adminList = new ArrayList&lt;&gt;(); Admin adminRoleDO = new Admin(); adminList.add(adminRoleDO); adminList.add(adminRoleDO); adminList.add(adminRoleDO); adminList.add(adminRoleDO); // 我们需要将这个对象的id 抽取出来变成一个集合。 // 假设我们需要一个不重复的 id集合 Set&lt;Integer&gt; adminSet = adminRoleList.stream().map(Admin::getId).collect(Collectors.toSet()); // 如果转换为Map对象 id为key name为value方法如下 Map&lt;Integer,Integer&gt; adminMap = from.stream().collect(Collectors.toMap(Admin::getId, Admin::getName)); }","link":"/2019/06/01/%E6%95%99%E4%BD%A0%E4%BD%A0%E5%BF%AB%E6%8D%B7%E7%BC%96%E7%A8%8B-%E5%B0%86List-User-%E5%AF%B9%E8%B1%A1%E7%9A%84id%E5%BF%AB%E9%80%9F%E6%8A%BD%E5%8F%96%E5%87%BA%E6%9D%A5/"},{"title":"浅谈SpringBoot的Cors跨域设置","text":"这世上有三样东西是别人抢不走的：一是吃进胃里的食物，二是藏在心中的梦想，三是读进大脑的书 1、什么是跨越？ 一个网页向另一个不同域名/不同协议/不同端口的网页请求资源，这就是跨域。 跨域原因产生：在当前域名请求网站中，默认不允许通过ajax请求发送其他域名。 SpringBoot的Cors跨域设置 SpringBoot可以基于Cors解决跨域问题，Cors是一种机制，告诉我们的后台，哪边（origin ）来的请求可以访问服务器的数据。 全局配置 配置实例如下： 123456789101112@Configurationpublic class CorsConfig implements WebMvcConfigurer { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) .allowCredentials(true) .allowedMethods(&quot;GET&quot;, &quot;POST&quot;, &quot;PUT&quot;, &quot;DELETE&quot;, &quot;OPTIONS&quot;) .maxAge(3600); }} 首先实现了WebMvcConfigurer 接口，WebMvcConfigurer 这个接口十分强大，里面还有很多可用的方法，在SpringBoot2.0里面可以解决WebMvcConfigurerAdapter曾经的部分任务。其中一个方法就是addCorsMappings()，是专门为开发人员解决跨域而诞生的接口。其中构造参数为CorsRegistry。 看下CorsRegistry源码，十分简单： 12345678910111213141516171819public class CorsRegistry { private final List&lt;CorsRegistration&gt; registrations = new ArrayList&lt;&gt;(); public CorsRegistration addMapping(String pathPattern) { CorsRegistration registration = new CorsRegistration(pathPattern); this.registrations.add(registration); return registration; } protected Map&lt;String, CorsConfiguration&gt; getCorsConfigurations() { Map&lt;String, CorsConfiguration&gt; configs = new LinkedHashMap&lt;&gt;(this.registrations.size()); for (CorsRegistration registration : this.registrations) { configs.put(registration.getPathPattern(), registration.getCorsConfiguration()); } return configs; }} 可以看出CorsRegistry 有个属性registrations ，按道理可以根据不同的项目路径进行定制访问行为，但是我们示例直接将pathPattern 设置为 /**，也就是说已覆盖项目所有路径，只需要创建一个CorsRegistration就好。getCorsConfigurations(),这个方法是获取所有CorsConfiguration的Map集合，key值为传入路径pathPattern。 回到示例代码CorsConfig中，registry对象addMapping()增加完传入路径pathPattern之后，return了一个CorsRegistration对象，是进行更多的配置，看一下CorsRegistration的代码，看看我们能配些什么？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class CorsRegistration { //传入的路径 private final String pathPattern; //配置信息实体类 private final CorsConfiguration config; //构造方法 public CorsRegistration(String pathPattern) { this.pathPattern = pathPattern; //原生注释看到了一个 @CrossOrigin 这个注解，待会看看是什么 // Same implicit default values as the @CrossOrigin annotation + allows simple methods this.config = new CorsConfiguration().applyPermitDefaultValues(); } //允许哪些源网站访问，默认所有 public CorsRegistration allowedOrigins(String... origins) { this.config.setAllowedOrigins(Arrays.asList(origins)); return this; } //允许何种方式访问，默认简单方式，即：GET，HEAD，POST public CorsRegistration allowedMethods(String... methods) { this.config.setAllowedMethods(Arrays.asList(methods)); return this; } //设置访问header，默认所有 public CorsRegistration allowedHeaders(String... headers) { this.config.setAllowedHeaders(Arrays.asList(headers)); return this; } //设置response headers，默认没有（什么都不设置） public CorsRegistration exposedHeaders(String... headers) { this.config.setExposedHeaders(Arrays.asList(headers)); return this; } //是否浏览器应该发送credentials，例如cookies Access-Control-Allow-Credentials public CorsRegistration allowCredentials(boolean allowCredentials) { this.config.setAllowCredentials(allowCredentials); return this; } //设置等待时间，默认1800秒 public CorsRegistration maxAge(long maxAge) { this.config.setMaxAge(maxAge); return this; } protected String getPathPattern() { return this.pathPattern; } protected CorsConfiguration getCorsConfiguration() { return this.config; }} 局部配置 刚才遇到一个@CrossOrigin这个注解，看看它是干什么的？ 12345678910111213141516171819202122232425262728293031323334353637383940@Target({ ElementType.METHOD, ElementType.TYPE })@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CrossOrigin { /** @deprecated as of Spring 5.0, in favor of {@link CorsConfiguration#applyPermitDefaultValues} */ @Deprecated String[] DEFAULT_ORIGINS = { &quot;*&quot; }; /** @deprecated as of Spring 5.0, in favor of {@link CorsConfiguration#applyPermitDefaultValues} */ @Deprecated String[] DEFAULT_ALLOWED_HEADERS = { &quot;*&quot; }; /** @deprecated as of Spring 5.0, in favor of {@link CorsConfiguration#applyPermitDefaultValues} */ @Deprecated boolean DEFAULT_ALLOW_CREDENTIALS = false; /** @deprecated as of Spring 5.0, in favor of {@link CorsConfiguration#applyPermitDefaultValues} */ @Deprecated long DEFAULT_MAX_AGE = 1800 /** * Alias for {@link #origins}. */ @AliasFor(&quot;origins&quot;) String[] value() default {}; @AliasFor(&quot;value&quot;) String[] origins() default {}; String[] allowedHeaders() default {}; String[] exposedHeaders() default {}; RequestMethod[] methods() default {}; String allowCredentials() default &quot;&quot;; long maxAge() default -1;} 这个注解可以作用于方法或者类上，实现局部跨域，你会发现除了设置路径（因为没必要了，都定位到局部了）其他的参数与全局类似。","link":"/2019/06/12/%E6%B5%85%E8%B0%88SpringBoot%E7%9A%84Cors%E8%B7%A8%E5%9F%9F%E8%AE%BE%E7%BD%AE/"},{"title":"最全最全的Nginx解读文章","text":"Nginx 概述Nginx 是开源、高性能、高可靠的 Web 和反向代理服务器，而且支持热部署，几乎可以做到 7 * 24 小时不间断运行，即使运行几个月也不需要重新启动，还能在不间断服务的情况下对软件版本进行热更新。性能是 Nginx 最重要的考量，其占用内存少、并发能力强、能支持高达 5w 个并发连接数，最重要的是， Nginx 是免费的并可以商业化，配置使用也比较简单。 Nginx 特点 高并发、高性能； 模块化架构使得它的扩展性非常好； 异步非阻塞的事件驱动模型这点和 Node.js 相似； 相对于其它服务器来说它可以连续几个月甚至更长而不需要重启服务器使得它具有高可靠性； 热部署、平滑升级； 完全开源，生态繁荣； Nginx 作用Nginx 的最重要的几个使用场景： 静态资源服务，通过本地文件系统提供服务； 反向代理服务，延伸出包括缓存、负载均衡等； API 服务， OpenResty ； 对于前端来说 Node.js 并不陌生， Nginx 和 Node.js 的很多理念类似， HTTP 服务器、事件驱动、异步非阻塞等，且 Nginx 的大部分功能使用 Node.js 也可以实现，但 Nginx 和 Node.js 并不冲突，都有自己擅长的领域。 Nginx 擅长于底层服务器端资源的处理（静态资源处理转发、反向代理，负载均衡等）， Node.js 更擅长上层具体业务逻辑的处理，两者可以完美组合。 用一张图表示： Nginx 安装本文演示的是 Linux centOS 7.x 的操作系统上安装 Nginx ，至于在其它操作系统上进行安装可以网上自行搜索，都非常简单的。 使用 yum 安装 Nginx ： 12yum install nginx -y 安装完成后，通过 rpm -ql nginx 命令查看 Nginx 的安装信息： 123456789101112131415161718192021222324252627# Nginx配置文件/etc/nginx/nginx.conf # nginx 主配置文件/etc/nginx/nginx.conf.default# 可执行程序文件/usr/bin/nginx-upgrade/usr/sbin/nginx# nginx库文件/usr/lib/systemd/system/nginx.service # 用于配置系统守护进程/usr/lib64/nginx/modules # Nginx模块目录# 帮助文档/usr/share/doc/nginx-1.16.1/usr/share/doc/nginx-1.16.1/CHANGES/usr/share/doc/nginx-1.16.1/README/usr/share/doc/nginx-1.16.1/README.dynamic/usr/share/doc/nginx-1.16.1/UPGRADE-NOTES-1.6-to-1.10# 静态资源目录/usr/share/nginx/html/404.html/usr/share/nginx/html/50x.html/usr/share/nginx/html/index.html# 存放Nginx日志文件/var/log/nginx 主要关注的文件夹有两个： /etc/nginx/conf.d/ 是子配置项存放处， /etc/nginx/nginx.conf 主配置文件会默认把这个文件夹中所有子配置项都引入； /usr/share/nginx/html/ 静态文件都放在这个文件夹，也可以根据你自己的习惯放在其他地方； Nginx 常用命令systemctl 系统命令： 12345678910111213141516171819202122232425# 开机配置systemctl enable nginx # 开机自动启动systemctl disable nginx # 关闭开机自动启动# 启动Nginxsystemctl start nginx # 启动Nginx成功后，可以直接访问主机IP，此时会展示Nginx默认页面# 停止Nginxsystemctl stop nginx# 重启Nginxsystemctl restart nginx# 重新加载Nginxsystemctl reload nginx# 查看 Nginx 运行状态systemctl status nginx# 查看Nginx进程ps -ef | grep nginx# 杀死Nginx进程kill -9 pid # 根据上面查看到的Nginx进程号，杀死Nginx进程，-9 表示强制结束进程 Nginx 应用程序命令： 1234567nginx -s reload # 向主进程发送信号，重新加载配置文件，热重启nginx -s reopen # 重启 Nginxnginx -s stop # 快速关闭nginx -s quit # 等待工作进程处理完成后关闭nginx -T # 查看当前 Nginx 最终的配置nginx -t # 检查配置是否有问题 Nginx 核心配置配置文件结构 main 全局配置，对全局生效； events 配置影响 Nginx 服务器与用户的网络连接； http 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置； server 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块； location 用于配置匹配的 uri ； upstream 配置后端服务器具体地址，负载均衡配置不可或缺的部分； 用一张图清晰的展示它的层级结构： 配置文件 main 段核心参数user指定运行 Nginx 的 woker 子进程的属主和属组，其中组可以不指定。 1234user USERNAME [GROUP]user nginx lion; # 用户是nginx;组是lion pid指定运行 Nginx master 主进程的 pid 文件存放路径。 12pid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件 worker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。 12worker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。 worker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。 123worker_rlimit_core 50M; # 存放大小限制working_directory /opt/nginx/tmp; # 存放目录 worker_processes_number指定 Nginx 启动的 worker 子进程数量。 123worker_processes 4; # 指定具体子进程数量worker_processes auto; # 与当前cpu物理核心数一致 worker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。 1worker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程 将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。 worker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx 。 12worker_priority -10; # 120-10=110，110就是最终的优先级 Linux 默认进程的优先级值是120，值越小越优先； nice 定范围为 -20 到 +19 。 [备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。 worker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。 12worker_shutdown_timeout 5s; timer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。 12timer_resolution 100ms; 在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。 daemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。 12daemon off; # 默认是on，后台运行模式 配置文件 events 段核心参数useNginx 使用何种事件驱动模型。 1234use method; # 不推荐配置它，让nginx自己选择method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport worker_connectionsworker 子进程能够处理的最大并发连接数。 12worker_connections 1024 # 每个子进程的最大连接数为1024 accept_mutex是否打开负载均衡互斥锁。 12accept_mutex on # 默认是off关闭的，这里推荐打开 server_name 指令指定虚拟主机域名。 12345server_name name1 name2 name3# 示例：server_name www.nginx.com; 域名匹配的四种写法： 精确匹配： server_name www.nginx.com ; 左侧通配： server_name *.nginx.com ; 右侧统配： server_name www.nginx.* ; 正则匹配： server_name ~^www\\.nginx\\.*$ ; 匹配优先级：精确匹配 &gt; 左侧通配符匹配 &gt; 右侧通配符匹配 &gt; 正则表达式匹配 server_name 配置实例： 1、配置本地 DNS 解析 vim /etc/hosts （ macOS 系统） 12345678# 添加如下内容，其中 121.42.11.34 是阿里云服务器IP地址121.42.11.34 www.nginx-test.com121.42.11.34 mail.nginx-test.com121.42.11.34 www.nginx-test.org121.42.11.34 doc.nginx-test.com121.42.11.34 www.nginx-test.cn121.42.11.34 fe.nginx-test.club [注意] 这里使用的是虚拟域名进行测试，因此需要配置本地 DNS 解析，如果使用阿里云上购买的域名，则需要在阿里云上设置好域名解析。 2、配置阿里云 Nginx ，vim /etc/nginx/nginx.conf 123456789101112131415161718192021222324252627282930313233343536373839404142# 这里只列举了http端中的sever端配置# 左匹配server { listen 80; server_name *.nginx-test.com; root /usr/share/nginx/html/nginx-test/left-match/; location / { index index.html; }}# 正则匹配server { listen 80; server_name ~^.*\\.nginx-test\\..*$; root /usr/share/nginx/html/nginx-test/reg-match/; location / { index index.html; }}# 右匹配server { listen 80; server_name www.nginx-test.*; root /usr/share/nginx/html/nginx-test/right-match/; location / { index index.html; }}# 完全匹配server { listen 80; server_name www.nginx-test.com; root /usr/share/nginx/html/nginx-test/all-match/; location / { index index.html; }} 3、访问分析 当访问 www.nginx-test.com 时，都可以被匹配上，因此选择优先级最高的“完全匹配”； 当访问 mail.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.org 时，会进行“右匹配”； 当访问 doc.nginx-test.com 时，会进行“左匹配”； 当访问 www.nginx-test.cn 时，会进行“右匹配”； 当访问 fe.nginx-test.club 时，会进行“正则匹配”； root指定静态资源目录位置，它可以写在 http 、 server 、 location 等配置中。 123456789root path例如：location /image { root /opt/nginx/static;}当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png [注意] root 会将定义路径与 URI 叠加， alias 则只取定义路径。 alias它也是指定静态资源目录位置，它只能写在 location 中。 123456location /image { alias /opt/nginx/static/image/;}当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png [注意] 使用 alias 末尾一定要添加 / ，并且它只能位于 location 中。 location配置路径。 1234location [ = | ~ | ~* | ^~ ] uri { ...} 匹配规则： = 精确匹配； ~ 正则匹配，区分大小写； ~* 正则匹配，不区分大小写； ^~ 匹配到即停止搜索； 匹配优先级： = &gt; ^~ &gt; ~ &gt; ~* &gt; 不带任何字符。 实例： 12345678910111213141516171819202122server { listen 80; server_name www.nginx-test.com; # 只有当访问 www.nginx-test.com/match_all/ 时才会匹配到/usr/share/nginx/html/match_all/index.html location = /match_all/ { root /usr/share/nginx/html index index.html } # 当访问 www.nginx-test.com/1.jpg 等路径时会去 /usr/share/nginx/images/1.jpg 找对应的资源 location ~ \\.(jpeg|jpg|png|svg)$ { root /usr/share/nginx/images; } # 当访问 www.nginx-test.com/bbs/ 时会匹配上 /usr/share/nginx/html/bbs/index.html location ^~ /bbs/ { root /usr/share/nginx/html; index index.html index.htm; }} location 中的反斜线12345678location /test { ...}location /test/ { ...} 不带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ；如果没有 test 目录， nginx 则会找是否有 test 文件。 带 / 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ，如果没有它也不会去找是否存在 test 文件。 return停止处理请求，直接返回响应码或重定向到其他 URL ；执行 return 指令后， location 中后续指令将不会被执行。 123456789101112131415161718192021return code [text];return code URL;return URL;例如：location / { return 404; # 直接返回状态码}location / { return 404 &quot;pages not found&quot;; # 返回状态码 + 一段文本}location / { return 302 /bbs ; # 返回状态码 + 重定向地址}location / { return https://www.baidu.com ; # 返回重定向地址} rewrite根据指定正则表达式匹配规则，重写 URL 。 123456语法：rewrite 正则表达式 要替换的内容 [flag];上下文：server、location、if示例：rewirte /images/(.*\\.jpg)$ /pic/$1; # $1是前面括号(.*\\.jpg)的反向引用 flag 可选值的含义： last 重写后的 URL 发起新请求，再次进入 server 段，重试 location 的中的匹配； break 直接使用重写后的 URL ，不再匹配其它 location 中语句； redirect 返回302临时重定向； permanent 返回301永久重定向； 123456789101112131415161718192021server{ listen 80; server_name fe.lion.club; # 要在本地hosts文件进行配置 root html; location /search { rewrite ^/(.*) https://www.baidu.com redirect; } location /images { rewrite /images/(.*) /pics/$1; } location /pics { rewrite /pics/(.*) /photos/$1; } location /photos { }} 按照这个配置我们来分析： 当访问 fe.lion.club/search 时，会自动帮我们重定向到 https://www.baidu.com。 当访问 fe.lion.club/images/1.jpg 时，第一步重写 URL 为 fe.lion.club/pics/1.jpg ，找到 pics 的 location ，继续重写 URL 为 fe.lion.club/photos/1.jpg ，找到 /photos 的 location 后，去 html/photos 目录下寻找 1.jpg 静态资源。 if 指令123456789语法：if (condition) {...}上下文：server、location示例：if($http_user_agent ~ Chrome){ rewrite /(.*)/browser/$1 break;} condition 判断条件： $variable 仅为变量时，值为空或以0开头字符串都会被当做 false 处理； = 或 != 相等或不等； ~ 正则匹配； ! ~ 非正则匹配； ~* 正则匹配，不区分大小写； -f 或 ! -f 检测文件存在或不存在； -d 或 ! -d 检测目录存在或不存在； -e 或 ! -e 检测文件、目录、符号链接等存在或不存在； -x 或 ! -x 检测文件可以执行或不可执行； 实例： 123456789101112server { listen 8080; server_name localhost; root html; location / { if ( $uri = &quot;/images/&quot; ){ rewrite (.*) /pics/ break; } }} 当访问 localhost:8080/images/ 时，会进入 if 判断里面执行 rewrite 命令。 autoindex用户请求以 / 结尾时，列出目录结构，可以用于快速搭建静态资源下载网站。 autoindex.conf 配置信息： 1234567891011121314server { listen 80; server_name fe.lion-test.club; location /download/ { root /opt/source; autoindex on; # 打开 autoindex，，可选参数有 on | off autoindex_exact_size on; # 修改为off，以KB、MB、GB显示文件大小，默认为on，以bytes显示出⽂件的确切⼤⼩ autoindex_format html; # 以html的方式进行格式化，可选参数有 html | json | xml autoindex_localtime off; # 显示的⽂件时间为⽂件的服务器时间。默认为off，显示的⽂件时间为GMT时间 }} 当访问 fe.lion.com/download/ 时，会把服务器 /opt/source/download/ 路径下的文件展示出来，如下图所示： 变量Nginx 提供给使用者的变量非常多，但是终究是一个完整的请求过程所产生数据， Nginx 将这些数据以变量的形式提供给使用者。 下面列举些项目中常用的变量： 变量名 含义 remote_addr 客户端 IP 地址 remote_port 客户端端口 server_addr 服务端 IP 地址 server_port 服务端端口 server_protocol 服务端协议 binary_remote_addr 二进制格式的客户端 IP 地址 connection TCP 连接的序号，递增 connection_request TCP 连接当前的请求数量 uri 请求的URL，不包含参数 request_uri 请求的URL，包含参数 scheme 协议名， http 或 https request_method 请求方法 request_length 全部请求的长度，包含请求行、请求头、请求体 args 全部参数字符串 arg_参数名 获取特定参数值 is_args URL 中是否有参数，有的话返回 ? ，否则返回空 query_string 与 args 相同 host 请求信息中的 Host ，如果请求中没有 Host 行，则在请求头中找，最后使用 nginx 中设置的 server_name 。 http_user_agent 用户浏览器 http_referer 从哪些链接过来的请求 http_via 每经过一层代理服务器，都会添加相应的信息 http_cookie 获取用户 cookie request_time 处理请求已消耗的时间 https 是否开启了 https ，是则返回 on ，否则返回空 request_filename 磁盘文件系统待访问文件的完整路径 document_root 由 URI 和 root/alias 规则生成的文件夹路径 limit_rate 返回响应时的速度上限值 实例演示 var.conf ： 12345678910111213141516171819202122232425262728293031323334server{ listen 8081; server_name var.lion-test.club; root /usr/share/nginx/html; location / { return 200 &quot;remote_addr: $remote_addrremote_port: $remote_portserver_addr: $server_addrserver_port: $server_portserver_protocol: $server_protocolbinary_remote_addr: $binary_remote_addrconnection: $connectionuri: $urirequest_uri: $request_urischeme: $schemerequest_method: $request_methodrequest_length: $request_lengthargs: $argsarg_pid: $arg_pidis_args: $is_argsquery_string: $query_stringhost: $hosthttp_user_agent: $http_user_agenthttp_referer: $http_refererhttp_via: $http_viarequest_time: $request_timehttps: $httpsrequest_filename: $request_filenamedocument_root: $document_root&quot;; }} 当我们访问 http://var.lion-test.club:8081/test?pid=121414&amp;cid=sadasd 时，由于 Nginx 中写了 return 方法，因此 chrome 浏览器会默认为我们下载一个文件，下面展示的就是下载的文件内容： 12345678910111213141516171819202122232425remote_addr: 27.16.220.84remote_port: 56838server_addr: 172.17.0.2server_port: 8081server_protocol: HTTP/1.1binary_remote_addr: \u001b\u0010茉connection: 126uri: /test/request_uri: /test/?pid=121414&amp;cid=sadasdscheme: httprequest_method: GETrequest_length: 518args: pid=121414&amp;cid=sadasdarg_pid: 121414is_args: ?query_string: pid=121414&amp;cid=sadasdhost: var.lion-test.clubhttp_user_agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36http_referer: http_via: request_time: 0.000https: request_filename: /usr/share/nginx/html/test/document_root: /usr/share/nginx/html Nginx 的配置还有非常多，以上只是罗列了一些常用的配置，在实际项目中还是要学会查阅文档。 Nginx 应用核心概念代理是在服务器和客户端之间假设的一层服务器，代理将接收客户端的请求并将它转发给服务器，然后将服务端的响应转发给客户端。 不管是正向代理还是反向代理，实现的都是上面的功能。 正向代理 正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。 正向代理是为我们服务的，即为客户端服务的，客户端可以根据正向代理访问到它本身无法访问到的服务器资源。 正向代理对我们是透明的，对服务端是非透明的，即服务端并不知道自己收到的是来自代理的访问还是来自真实客户端的访问。 反向代理 反向代理*（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 反向代理是为服务端服务的，反向代理可以帮助服务器接收来自客户端的请求，帮助服务器做请求转发，负载均衡等。 反向代理对服务端是透明的，对我们是非透明的，即我们并不知道自己访问的是代理服务器，而服务器知道反向代理在为他服务。 反向代理的优势： 隐藏真实服务器； 负载均衡便于横向扩充后端动态服务； 动静分离，提升系统健壮性； 那么“动静分离”是什么？负载均衡又是什么？ 动静分离动静分离是指在 web 服务器架构中，将静态页面与动态页面或者静态内容接口和动态内容接口分开不同系统访问的架构设计方法，进而提示整个服务的访问性和可维护性。 一般来说，都需要将动态资源和静态资源分开，由于 Nginx 的高并发和静态资源缓存等特性，经常将静态资源部署在 Nginx 上。如果请求的是静态资源，直接到静态资源目录获取资源，如果是动态资源的请求，则利用反向代理的原理，把请求转发给对应后台应用去处理，从而实现动静分离。 使用前后端分离后，可以很大程度提升静态资源的访问速度，即使动态服务不可用，静态资源的访问也不会受到影响。 负载均衡一般情况下，客户端发送多个请求到服务器，服务器处理请求，其中一部分可能要操作一些资源比如数据库、静态资源等，服务器处理完毕后，再将结果返回给客户端。 这种模式对于早期的系统来说，功能要求不复杂，且并发请求相对较少的情况下还能胜任，成本也低。随着信息数量不断增长，访问量和数据量飞速增长，以及系统业务复杂度持续增加，这种做法已无法满足要求，并发量特别大时，服务器容易崩。 很明显这是由于服务器性能的瓶颈造成的问题，除了堆机器之外，最重要的做法就是负载均衡。 请求爆发式增长的情况下，单个机器性能再强劲也无法满足要求了，这个时候集群的概念产生了，单个服务器解决不了的问题，可以使用多个服务器，然后将请求分发到各个服务器上，将负载分发到不同的服务器，这就是负载均衡，核心是「分摊压力」。 Nginx 实现负载均衡，一般来说指的是将请求转发给服务器集群。 举个具体的例子，晚高峰乘坐地铁的时候，入站口经常会有地铁工作人员大喇叭“请走 B 口， B 口人少车空….”，这个工作人员的作用就是负载均衡。 Nginx 实现负载均衡的策略： 轮询策略：默认情况下采用的策略，将所有客户端请求轮询分配给服务端。这种策略是可以正常工作的，但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。 最小连接数策略：将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。 最快响应时间策略：优先分配给响应时间最短的服务器。 客户端 ip 绑定策略：来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。 Nginx 实战配置在配置反向代理和负载均衡等等功能之前，有两个核心模块是我们必须要掌握的，这两个模块应该说是 Nginx 应用配置中的核心，它们分别是： upstream 、proxy_pass 。 upstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。 1234567891011语法：upstream name { ...}上下文：http示例：upstream back_end_server{ server 192.168.100.33:8081} 在 upstream 内可使用的指令： server 定义上游服务器地址； zone 定义共享内存，用于跨 worker 子进程； keepalive 对上游服务启用长连接； keepalive_requests 一个长连接最多请求 HTTP 的个数； keepalive_timeout 空闲情形下，一个长连接的超时时长； hash 哈希负载均衡算法； ip_hash 依据 IP 进行哈希计算的负载均衡算法； least_conn 最少连接数负载均衡算法； least_time 最短响应时间负载均衡算法； random 随机负载均衡算法； server定义上游服务器地址。 1234语法：server address [parameters]上下文：upstream parameters 可选值： weight=number 权重值，默认为1； max_conns=number 上游服务器的最大并发连接数； fail_timeout=time 服务器不可用的判定时间； max_fails=numer 服务器不可用的检查次数； backup 备份服务器，仅当其他服务器都不可用时才会启用； down 标记服务器长期不可用，离线维护； keepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。 123456keepalive connections;上下文：upstream示例：keepalive 16; keepalive_requests单个长连接可以处理的最多 HTTP 请求个数。 123456语法：keepalive_requests number;默认值：keepalive_requests 100;上下文：upstream keepalive_timeout空闲长连接的最长保持时间。 123456语法：keepalive_timeout time;默认值：keepalive_timeout 60s;上下文：upstream 配置实例1234567upstream back_end{ server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2; keepalive 32; keepalive_requests 50; keepalive_timeout 30s;} proxy_pass用于配置代理服务器。 12345678语法：proxy_pass URL;上下文：location、if、limit_except示例：proxy_pass http://127.0.0.1:8081proxy_pass http://127.0.0.1:8081/proxy URL 参数原则 URL 必须以 http 或 https 开头； URL 中可以携带变量； URL 中是否带 URI ，会直接影响发往上游请求的 URL ； 接下来让我们来看看两种常见的 URL 用法： proxy_pass http://192.168.100.33:8081 proxy_pass http://192.168.100.33:8081/ 这两种用法的区别就是带 / 和不带 / ，在配置代理时它们的区别可大了： 不带 / 意味着 Nginx 不会修改用户 URL ，而是直接透传给上游的应用服务器； 带 / 意味着 Nginx 会修改用户 URL ，修改方法是将 location 后的 URL 从用户 URL 中删除； 不带 / 的用法： 1234location /bbs/{ proxy_pass http://127.0.0.1:8080;} 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /bbs/abc/test.html 带 / 的用法： 1234location /bbs/{ proxy_pass http://127.0.0.1:8080/;} 分析： 用户请求 URL ： /bbs/abc/test.html 请求到达 Nginx 的 URL ： /bbs/abc/test.html 请求到达上游应用服务器的 URL ： /abc/test.html 并没有拼接上 /bbs ，这点和 root 与 alias 之间的区别是保持一致的。 配置反向代理这里为了演示更加接近实际，作者准备了两台云服务器，它们的公网 IP 分别是： 121.42.11.34 与 121.5.180.193 。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置： 1234567891011121314# /etc/nginx/conf.d/proxy.confserver{ listen 8080; server_name localhost; location /proxy/ { root /usr/share/nginx/html/proxy; index index.html; }}# /usr/share/nginx/html/proxy/index.html&lt;h1&gt; 121.42.11.34 proxy html &lt;/h1&gt; 配置完成后重启 Nginx 服务器 nginx -s reload 。 把 121.5.180.193 服务器作为代理服务器，做如下配置： 123456789101112131415# /etc/nginx/conf.d/proxy.confupstream back_end { server 121.42.11.34:8080 weight=2 max_conns=1000 fail_timeout=10s max_fails=3; keepalive 32; keepalive_requests 80; keepalive_timeout 20s;}server { listen 80; server_name proxy.lion.club; location /proxy { proxy_pass http://back_end/proxy; }} 本地机器要访问 proxy.lion.club 域名，因此需要配置本地 hosts ，通过命令：vim /etc/hosts 进入配置文件，添加如下内容： 1121.5.180.193 proxy.lion.club 分析： 当访问 proxy.lion.club/proxy 时通过 upstream 的配置找到 121.42.11.34:8080 ； 因此访问地址变为 http://121.42.11.34:8080/proxy ； 连接到 121.42.11.34 服务器，找到 8080 端口提供的 server ； 通过 server 找到 /usr/share/nginx/html/proxy/index.html 资源，最终展示出来。 配置负载均衡配置负载均衡主要是要使用 upstream 指令。 我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： 123456789101112131415161718192021server{ listen 8020; location / { return 200 'return 8020 \\n'; }}server{ listen 8030; location / { return 200 'return 8030 \\n'; }}server{ listen 8040; location / { return 200 'return 8040 \\n'; }} 配置完成后： nginx -t 检测配置是否正确； nginx -s reload 重启 Nginx 服务器； 执行 ss -nlt 命令查看端口是否被占用，从而判断 Nginx 服务是否正确启动。 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/balance.conf ）： 123456789101112131415upstream demo_server { server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040;}server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; }} 配置完成后重启 Nginx 服务器。并且在需要访问的客户端配置好 ip 和域名的映射关系。 1234# /etc/hosts121.5.180.193 balance.lion.club 在客户端机器执行 curl http://balance.lion.club/balance/ 命令： 不难看出，负载均衡的配置已经生效了，每次给我们分发的上游服务器都不一样。就是通过简单的轮询策略进行上游服务器分发。 接下来，我们再来了解下 Nginx 的其它分发策略。 hash 算法通过制定关键字作为 hash key ，基于 hash 算法映射到特定的上游服务器中。关键字可以包含有变量、字符串。 12345678910111213141516upstream demo_server { hash $request_uri; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040;}server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; }} hash $request_uri 表示使用 request_uri 变量作为 hash 的 key 值，只要访问的 URI 保持不变，就会一直分发给同一台服务器。 ip_hash根据客户端的请求 ip 进行判断，只要 ip 地址不变就永远分配到同一台主机。它可以有效解决后台服务器 session 保持的问题。 12345678910111213141516upstream demo_server { ip_hash; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040;}server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; }} 最少连接数算法各个 worker 子进程通过读取共享内存的数据，来获取后端服务器的信息。来挑选一台当前已建立连接数最少的服务器进行分配请求。 1234语法：least_conn;上下文：upstream; 示例： 1234567891011121314151617upstream demo_server { zone test 10M; # zone可以设置共享内存空间的名字和大小 least_conn; server 121.42.11.34:8020; server 121.42.11.34:8030; server 121.42.11.34:8040;}server { listen 80; server_name balance.lion.club; location /balance/ { proxy_pass http://demo_server; }} 最后你会发现，负载均衡的配置其实一点都不复杂。 配置缓存缓存可以非常有效的提升性能，因此不论是客户端（浏览器），还是代理服务器（ Nginx ），乃至上游服务器都多少会涉及到缓存。可见缓存在每个环节都是非常重要的。下面让我们来学习 Nginx 中如何设置缓存策略。 proxy_cache存储一些之前被访问过、而且可能将要被再次访问的资源，使用户可以直接从代理服务器获得，从而减少上游服务器的压力，加快整个访问速度。 123456语法：proxy_cache zone | off ; # zone 是共享内存的名称默认值：proxy_cache off;上下文：http、server、location proxy_cache_path设置缓存文件的存放路径。 123456语法：proxy_cache_path path [level=levels] ...可选参数省略，下面会详细列举默认值：proxy_cache_path off上下文：http 参数含义： path 缓存文件的存放路径； level path 的目录层级； keys_zone 设置共享内存； inactive 在指定时间内没有被访问，缓存会被清理，默认10分钟； proxy_cache_key设置缓存文件的 key 。 123456语法：proxy_cache_key默认值：proxy_cache_key $scheme$proxy_host$request_uri;上下文：http、server、location proxy_cache_valid配置什么状态码可以被缓存，以及缓存时长。 123456语法：proxy_cache_valid [code...] time;上下文：http、server、location配置示例：proxy_cache_valid 200 304 2m;; # 说明对于状态为200和304的缓存文件的缓存时间是2分钟 proxy_no_cache定义相应保存到缓存的条件，如果字符串参数的至少一个值不为空且不等于“ 0”，则将不保存该响应到缓存。 123456语法：proxy_no_cache string;上下文：http、server、location示例：proxy_no_cache $http_pragma $http_authorization; proxy_cache_bypass定义条件，在该条件下将不会从缓存中获取响应。 123456语法：proxy_cache_bypass string;上下文：http、server、location示例：proxy_cache_bypass $http_pragma $http_authorization; upstream_cache_status 变量它存储了缓存是否命中的信息，会设置在响应头信息中，在调试中非常有用。 12345678MISS: 未命中缓存HIT： 命中缓存EXPIRED: 缓存过期STALE: 命中了陈旧缓存REVALIDDATED: Nginx验证陈旧缓存依然有效UPDATING: 内容陈旧，但正在更新BYPASS: X响应从原始服务器获取 配置实例我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： 12345678910111213141516server { listen 1010; root /usr/share/nginx/html/1010; location / { index index.html; }}server { listen 1020; root /usr/share/nginx/html/1020; location / { index index.html; }} 把 121.5.180.193 服务器作为代理服务器，做如下配置（ /etc/nginx/conf.d/cache.conf ）： 12345678910111213141516171819proxy_cache_path /etc/nginx/cache_temp levels=2:2 keys_zone=cache_zone:30m max_size=2g inactive=60m use_temp_path=off;upstream cache_server{ server 121.42.11.34:1010; server 121.42.11.34:1020;}server { listen 80; server_name cache.lion.club; location / { proxy_cache cache_zone; # 设置缓存内存，上面配置中已经定义好的 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 }} 缓存就是这样配置，我们可以在 /etc/nginx/cache_temp 路径下找到相应的缓存文件。 对于一些实时性要求非常高的页面或数据来说，就不应该去设置缓存，下面来看看如何配置不缓存的内容。 1234567891011121314151617181920...server { listen 80; server_name cache.lion.club; # URI 中后缀为 .txt 或 .text 的设置变量值为 &quot;no cache&quot; if ($request_uri ~ \\.(txt|text)$) { set $cache_name &quot;no cache&quot; } location / { proxy_no_cache $cache_name; # 判断该变量是否有值，如果有值则不进行缓存，如果没有值则进行缓存 proxy_cache cache_zone; # 设置缓存内存 proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟 proxy_cache_key $request_uri; # 缓存文件的key为请求的URI add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端 proxy_pass http://cache_server; # 代理转发 }} HTTPS在学习如何配置 HTTPS 之前，我们先来简单回顾下 HTTPS 的工作流程是怎么样的？它是如何进行加密保证安全的？ HTTPS 工作流程 客户端（浏览器）访问 https://www.baidu.com 百度网站； 百度服务器返回 HTTPS 使用的 CA 证书； 浏览器验证 CA 证书是否为合法证书； 验证通过，证书合法，生成一串随机数并使用公钥（证书中提供的）进行加密； 发送公钥加密后的随机数给百度服务器； 百度服务器拿到密文，通过私钥进行解密，获取到随机数（公钥加密，私钥解密，反之也可以）； 百度服务器把要发送给浏览器的内容，使用随机数进行加密后传输给浏览器； 此时浏览器可以使用随机数进行解密，获取到服务器的真实传输内容； 这就是 HTTPS 的基本运作原理，使用对称加密和非对称机密配合使用，保证传输内容的安全性。 关于HTTPS更多知识，可以查看作者的另外一篇文章《学习 HTTP 协议》。 配置证书下载证书的压缩文件，里面有个 Nginx 文件夹，把 xxx.crt 和 xxx.key 文件拷贝到服务器目录，再进行如下配置： 1234567891011121314server { listen 443 ssl http2 default_server; # SSL 访问端口号为 443 server_name lion.club; # 填写绑定证书的域名(我这里是随便写的) ssl_certificate /etc/nginx/https/lion.club_bundle.crt; # 证书地址 ssl_certificate_key /etc/nginx/https/lion.club.key; # 私钥地址 ssl_session_timeout 10m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 支持ssl协议版本，默认为后三个，主流版本是[TLSv1.2] location / { root /usr/share/nginx/html; index index.html index.htm; }} 如此配置后就能正常访问 HTTPS 版的网站了。 配置跨域 CORS先简单回顾下跨域究竟是怎么回事。 跨域的定义同源策略限制了从同一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。通常不允许不同源间的读操作。 同源的定义如果两个页面的协议，端口（如果有指定）和域名都相同，则两个页面具有相同的源。 下面给出了与 URL http://store.company.com/dir/page.html 的源进行对比的示例: 12345http://store.company.com/dir2/other.html 同源https://store.company.com/secure.html 不同源，协议不同http://store.company.com:81/dir/etc.html 不同源，端口不同http://news.company.com/dir/other.html 不同源，主机不同 不同源会有如下限制： Web 数据层面，同源策略限制了不同源的站点读取当前站点的 Cookie 、 IndexDB 、 LocalStorage 等数据。 DOM 层面，同源策略限制了来自不同源的 JavaScript 脚本对当前 DOM 对象读和写的操作。 网络层面，同源策略限制了通过 XMLHttpRequest 等方式将站点的数据发送给不同源的站点。 Nginx 解决跨域的原理例如： 前端 server 的域名为： fe.server.com 后端服务的域名为： dev.server.com 现在我在 fe.server.com 对 dev.server.com 发起请求一定会出现跨域。 现在我们只需要启动一个 Nginx 服务器，将 server_name 设置为 fe.server.com 然后设置相应的 location 以拦截前端需要跨域的请求，最后将请求代理回 dev.server.com 。如下面的配置： 12345678server { listen 80; server_name fe.server.com; location / { proxy_pass dev.server.com; }} 这样可以完美绕过浏览器的同源策略： fe.server.com 访问 Nginx 的 fe.server.com 属于同源访问，而 Nginx 对服务端转发的请求不会触发浏览器的同源策略。 配置开启 gzip 压缩GZIP 是规定的三种标准 HTTP 压缩格式之一。目前绝大多数的网站都在使用 GZIP 传输 HTML 、CSS 、 JavaScript 等资源文件。 对于文本文件， GZiP 的效果非常明显，开启后传输所需流量大约会降至 1/4~1/3 。 并不是每个浏览器都支持 gzip 的，如何知道客户端是否支持 gzip 呢，请求头中的 Accept-Encoding 来标识对压缩的支持。启用 gzip 同时需要客户端和服务端的支持，如果客户端支持 gzip 的解析，那么只要服务端能够返回 gzip 的文件就可以启用 gzip 了,我们可以通过 Nginx 的配置来让服务端支持 gzip 。下面的 respone 中 content-encoding:gzip ，指服务端开启了 gzip 的压缩方式。在 /etc/nginx/conf.d/ 文件夹中新建配置文件 gzip.conf ： 12345678910111213141516171819202122232425262728# # 默认off，是否开启gzipgzip on; # 要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用；gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;# ---- 以上两个参数开启就可以支持Gzip压缩了 ---- ## 默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容；gzip_static on;# 默认 off，nginx做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩；gzip_proxied any;# 用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩；gzip_vary on;# gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6；gzip_comp_level 6;# 获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得；gzip_buffers 16 8k;# 允许压缩的页面最小字节数，页面字节数从header头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大；# gzip_min_length 1k;# 默认 1.1，启用 gzip 所需的 HTTP 最低版本；gzip_http_version 1.1; 其实也可以通过前端构建工具例如 webpack 、rollup 等在打生产包时就做好 Gzip 压缩，然后放到 Nginx 服务器中，这样可以减少服务器的开销，加快访问速度。 关于 Nginx 的实际应用就学习到这里，相信通过掌握了 Nginx 核心配置以及实战配置，之后再遇到什么需求，我们也能轻松应对。接下来，让我们再深入一点学习下 Nginx 的架构。 Nginx 架构进程结构多进程结构 Nginx 的进程模型图： 多进程中的 Nginx 进程架构如下图所示，会有一个父进程（ Master Process ），它会有很多子进程（ Child Processes ）。 Master Process 用来管理子进程的，其本身并不真正处理用户请求。 某个子进程 down 掉的话，它会向 Master 进程发送一条消息，表明自己不可用了，此时 Master 进程会去新起一个子进程。 某个配置文件被修改了 Master 进程会去通知 work 进程获取新的配置信息，这也就是我们所说的热部署。 子进程间是通过共享内存的方式进行通信的。 配置文件重载原理reload 重载配置文件的流程： 向 master 进程发送 HUP 信号（ reload 命令）； master 进程检查配置语法是否正确； master 进程打开监听端口； master 进程使用新的配置文件启动新的 worker 子进程； master 进程向老的 worker 子进程发送 QUIT 信号； 老的 worker 进程关闭监听句柄，处理完当前连接后关闭进程； 整个过程 Nginx 始终处于平稳运行中，实现了平滑升级，用户无感知； Nginx 模块化管理机制Nginx 的内部结构是由核心部分和一系列的功能模块所组成。这样划分是为了使得每个模块的功能相对简单，便于开发，同时也便于对系统进行功能扩展。Nginx 的模块是互相独立的,低耦合高内聚。 总结相信通过本文的学习，你应该会对 Nginx 有一个更加全面的认识。","link":"/2021/03/24/%E6%9C%80%E5%85%A8%E6%9C%80%E5%85%A8%E7%9A%84Nginx%E8%A7%A3%E8%AF%BB%E6%96%87%E7%AB%A0/"},{"title":"消息队列面试相关","text":"使用消息队列的原因其实就是问问你消息队列都有哪些使用场景，然后你项目里具体是什么场景，说说你在这个场景里用消息队列是什么面试官问你这个问题，期望的一个回答是说，你们公司有个什么业务场景，这个业务场景有个什么技术挑战，如果不用MQ可能会很麻烦，但是你现在用了MQ之后带给了你很多的好处先说一下消息队列的常见使用场景吧，其实场景有很多，但是比较核心的有3个：解耦、异步、削峰、解耦 现场画个图来说明一下： A系统发送个数据到BCD三个系统，接口调用发送，那如果E系统也要这个数据呢？那如果C系统现在不需要了呢？现在A系统又要发送第二种数据了呢？A系统负责人濒临崩溃中。。。再来点更加崩溃的事儿，A系统要时时刻刻考虑BCDE四个系统如果挂了咋办？我要不要重发？我要不要把消息存起来？头发都白了啊。。。 面试技巧：你需要去考虑一下你负责的系统中是否有类似的场景，就是一个系统或者一个模块，调用了多个系统或者模块，互相之间的调用很复杂，维护起来很麻烦。但是其实这个调用是不需要直接同步调用接口的，如果用MQ给他异步化解耦，也是可以的，你就需要去考虑在你的项目里，是不是可以运用这个MQ去进行系统的解耦。在简历中体现出来这块东西，用MQ作解耦。异步：现场画个图来说明一下， A系统接收一个请求，需要在自己本地写库，还需要在BCD三个系统写库，自己本地写库要3ms，BCD三个系统分别写库要300ms、450ms、200ms。最终请求总延时是3 + 300 + 450 + 200 = 953ms，接近1s，用户感觉搞个什么东西，慢死了慢死了。_削峰_：每天0点到11点，A系统风平浪静，每秒并发请求数量就100个。结果每次一到11点~1点，每秒并发请求数量突然会暴增到1万条。但是系统最大的处理能力就只能是每秒钟处理1000个请求啊。。。尴尬了，系统会死。。。 消息队列有什么优点和缺点优点上面已经说了，就是在特殊场景下有其对应的好处，解耦、异步、削峰缺点呢？显而易见的 系统可用性降低：系统引入的外部依赖越多，越容易挂掉，本来你就是A系统调用BCD三个系统的接口就好了，人ABCD四个系统好好的，没啥问题，你偏加个MQ进来，万一MQ挂了咋整？MQ挂了，整套系统崩溃了，你不就完了么。系统复杂性提高：硬生生加个MQ进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已一致性问题：A系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是BCD三个系统那里，BD两个系统写库成功了，结果C系统写库失败了，咋整？你这数据就不一致了。所以消息队列实际是一种非常复杂的架构，你引入它有很多好处，但是也得针对它带来的坏处做各种额外的技术方案和架构来规避掉，最好之后，你会发现，妈呀，系统复杂度提升了一个数量级，也许是复杂了10倍。但是关键时刻，用，还是得用的。。。 kafka、activemq、rabbitmq、rocketmq都有什么优点和缺点啊？常见的MQ其实就这几种，别的还有很多其他MQ，但是比较冷门的，那么就别多说了作为一个码农，你起码得知道各种mq的优点和缺点吧，咱们来画个表格看看 | 特性 |ActiveMQ|RabbitMQ|RocketMQ|Kafka|| —— | —— | —— | —— |—— || 单机吞吐量|万级，吞吐量比RocketMQ和Kafka要低了一个数量级 | 万级，吞吐量比RocketMQ和Kafka要低了一个数量级|10万级，RocketMQ也是可以支撑高吞吐的一种MQ | 10万级别，这是kafka最大的优点，就是吞吐量高。一般配合大数据类的系统来进行实时数据计算、日志采集等场景|| topic数量对吞吐量的影响|||topic可以达到几百，几千个的级别，吞吐量会有较小幅度的下降这是RocketMQ的一大优势，在同等机器下，可以支撑大量的topic|topic从几十个到几百个的时候，吞吐量会大幅度下降所以在同等机器下，kafka尽量保证topic数量不要过多。如果要支撑大规模topic，需要增加更多的机器资源||时效性|ms级|微秒级，这是rabbitmq的一大特点，延迟是最低的|ms级|延迟在ms级以内||可用性|高，基于主从架构实现高可用性|高，基于主从架构实现高可用性|非常高，分布式架构|非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用||消息可靠性|有较低的概率丢失数据||经过参数优化配置，可以做到0丢失|经过参数优化配置，消息可以做到0丢失||功能支持|MQ领域的功能极其完备|基于erlang开发，所以并发能力很强，性能极其好，延时很低|MQ功能较为完善，还是分布式的，扩展性好|功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准||优劣势总结|非常成熟，功能强大，在业内大量的公司以及项目中都有应用偶尔会有较低概率丢失消息而且现在社区以及国内应用都越来越少，官方社区现在对ActiveMQ 5.x维护越来越少，几个月才发布一个版本而且确实主要是基于解耦和异步来用的，较少在大规模吞吐的场景中使用|erlang语言开发，性能极其好，延时很低；吞吐量到万级，MQ功能比较完备而且开源提供的管理界面非常棒，用起来很好用社区相对比较活跃，几乎每个月都发布几个版本分在国内一些互联网公司近几年用rabbitmq也比较多一些但是问题也是显而易见的，RabbitMQ确实吞吐量会低一些，这是因为他做的实现机制比较重。而且erlang开发，国内有几个公司有实力做erlang源码级别的研究和定制？如果说你没这个实力的话，确实偶尔会有一些问题，你很难去看懂源码，你公司对这个东西的掌控很弱，基本职能依赖于开源社区的快速维护和修复bug。而且rabbitmq集群动态扩展会很麻烦，不过这个我觉得还好。其实主要是erlang语言本身带来的问题。很难读源码，很难定制和掌控。|接口简单易用，而且毕竟在阿里大规模应用过，有阿里品牌保障日处理消息上百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是ok的，还可以支撑大规模的topic数量，支持复杂MQ业务场景而且一个很大的优势在于，阿里出品都是java系的，我们可以自己阅读源码，定制自己公司的MQ，可以掌控社区活跃度相对较为一般，不过也还可以，文档相对来说简单一些，然后接口这块不是按照标准JMS规范走的有些系统要迁移需要修改大量代码还有就是阿里出台的技术，你得做好这个技术万一被抛弃，社区黄掉的风险，那如果你们公司有技术实力我觉得用RocketMQ挺好的|kafka的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展同时kafka最好是支撑较少的topic数量即可，保证其超高吞吐量而且kafka唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集|","link":"/2019/06/07/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%9D%A2%E8%AF%95%E7%9B%B8%E5%85%B3/"},{"title":"小程序的三级联动","text":"项目中经常遇到要选择城市。用到三级联动的方式 微信小程序的 picker 组件 mode=date 是三级联动的，但是无法自定义，这让我们心痛不已，值得我们欣慰的 picker-view 组件是可以自定义添加多个选项，但还是无法联动。既然这样那就自己写一个联动。 做到如下图所示： 分为动态获取地址 引用静态文件获取地址 addressAdd.wxml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;view class=&quot;add-address&quot;&gt; &lt;view class=&quot;add-form&quot;&gt; &lt;view class=&quot;form-item&quot;&gt; &lt;input class=&quot;input&quot; bindinput=&quot;bindinputName&quot; placeholder=&quot;姓名&quot; value=&quot;{{address.name}}&quot; /&gt; &lt;/view&gt; &lt;view class=&quot;form-item&quot;&gt; &lt;input class=&quot;input&quot; bindinput=&quot;bindinputMobile&quot; value=&quot;{{address.mobile}}&quot; placeholder=&quot;手机号码&quot; /&gt; &lt;/view&gt; &lt;view class=&quot;form-item&quot;&gt; &lt;input class=&quot;input&quot; bindinput=&quot;bindinputAddress&quot; value=&quot;{{address.address}}&quot; placeholder=&quot;详细地址&quot; /&gt; &lt;/view&gt; &lt;view class=&quot;form-item&quot; bindtap='select'&gt; &lt;view class=&quot;weui-cell__bd&quot;&gt; {{areaInfo}} &lt;/view&gt; &lt;/view&gt; &lt;view class=&quot;form-default&quot;&gt; &lt;text bindtap=&quot;bindIsDefault&quot; class=&quot;default-input {{address.isDefault == 1 ? 'selected' : ''}}&quot;&gt;设为默认地址&lt;/text&gt; &lt;/view&gt; &lt;/view&gt; &lt;view class=&quot;btns&quot;&gt; &lt;button class=&quot;cannel&quot; bindtap=&quot;cancelAddress&quot;&gt;取消&lt;/button&gt; &lt;button class=&quot;save&quot; bindtap=&quot;saveAddress&quot;&gt;保存&lt;/button&gt; &lt;/view&gt;&lt;/view&gt;&lt;view class=&quot;bg-mask&quot; bindtap=&quot;cancelSelectRegion&quot; wx:if=&quot;{{openSelectRegion}}&quot;&gt;&lt;/view&gt;&lt;view class=&quot;picker-view&quot; animation=&quot;{{animationAddressMenu}}&quot; style=&quot;visibility:{{addressMenuIsShow ? 'visible':'hidden'}}&quot;&gt; &lt;!-- 确认取消按钮 --&gt; &lt;view class='btn'&gt; &lt;text catchtap=&quot;cityCancel&quot;&gt;取消&lt;/text&gt; &lt;text style=&quot;float: right&quot; catchtap=&quot;citySure&quot;&gt;确定&lt;/text&gt; &lt;/view&gt; &lt;!-- 选择地址 --&gt; &lt;picker-view class='cont' bindchange=&quot;cityChange&quot; value=&quot;{{value}}&quot; wx:key=&quot;&quot;&gt; &lt;!-- 省 --&gt; &lt;picker-view-column&gt; &lt;view wx:for=&quot;{{provinces}}&quot; class=&quot;picker-item&quot; wx:key=&quot;{{index}}&quot;&gt;{{item.area}}&lt;/view&gt; &lt;/picker-view-column&gt; &lt;!-- 市 --&gt; &lt;picker-view-column&gt; &lt;view wx:for=&quot;{{citys}}&quot; class=&quot;picker-item&quot; wx:key=&quot;index&quot;&gt;{{item.area}}&lt;/view&gt; &lt;/picker-view-column&gt; &lt;!-- 区 --&gt; &lt;picker-view-column&gt; &lt;view wx:for=&quot;{{areas}}&quot; class=&quot;picker-item&quot; wx:key=&quot;index&quot;&gt;{{item.area}}&lt;/view&gt; &lt;/picker-view-column&gt; &lt;/picker-view&gt;&lt;/view&gt; addressAdd.wxss 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221page{ height: 100%; background: #f4f4f4;}.add-address .add-form{ background: #fff; width: 100%; height: auto; overflow: hidden;}.add-address .form-item{ height: 116rpx; padding-left: 31.25rpx; border-bottom: 1px solid #d9d9d9; display: flex; align-items: center; padding-right: 31.25rpx;}.add-address .input{ flex: 1; height: 44rpx; line-height: 44rpx; overflow: hidden;}.add-address .form-default{ border-bottom: 1px solid #d9d9d9; height: 96rpx; background: #fafafa; padding-top: 28rpx; font-size: 28rpx;}.default-input{ margin: 0 auto; display: block; width: 240rpx; height: 40rpx; padding-left: 50rpx; line-height: 40rpx; background: url(http://yanxuan.nosdn.127.net/hxm/yanxuan-wap/p/20161201/style/img/sprites/checkbox-sed825af9d3-a6b8540d42.png) 1rpx -448rpx no-repeat; background-size: 38rpx 486rpx; font-size: 28rpx;}.default-input.selected{ background: url(http://yanxuan.nosdn.127.net/hxm/yanxuan-wap/p/20161201/style/img/sprites/checkbox-sed825af9d3-a6b8540d42.png) 0 -192rpx no-repeat; background-size: 38rpx 486rpx;}.add-address .btns{ position: fixed; bottom: 0; left: 0; overflow: hidden; display: flex; height: 100rpx; width: 100%;}.add-address .cannel,.add-address .save{ flex: 1; height: 100rpx; text-align: center; line-height: 100rpx; font-size: 28rpx; color: #fff; border:none; border-radius: 0;}.add-address .cannel{ background: #3F3F3F;}.add-address .save{ background: #a78845;}.region-select{ width: 100%; height: 600rpx; background: #fff; position: fixed; z-index: 10; left:0; bottom: 0;}.region-select .hd{ height: 108rpx; width: 100%; border-bottom: 1px solid #f4f4f4; padding: 46rpx 30rpx 0 30rpx;}.region-select .region-selected{ float: left; height: 60rpx; display: flex;}.region-select .region-selected .item{ max-width: 140rpx; margin-right: 30rpx; text-align: left; line-height: 60rpx; height: 100%; color: #333; font-size: 28rpx; overflow: hidden; text-overflow: ellipsis; white-space: nowrap;}.region-select .region-selected .item.disabled{ color: #999;}.region-select .region-selected .item.selected{ color: #a78845;}.region-select .done{ float: right; height: 60rpx; width: 60rpx; border: none; background: #fff; line-height: 60rpx; text-align: center; color: #333; font-size: 28rpx;}.region-select .done.disabled{ color: #999;}.region-select .bd{ height: 492rpx; width: 100%; padding: 0 30rpx;}.region-select .region-list{ height: 492rpx;}.region-select .region-list .item{ width: 100%; height: 104rpx; line-height: 104rpx; text-align: left; color: #333; font-size: 28rpx;}.region-select .region-list .item.selected{ color: #b4282d;}.bg-mask{ height: 100%; width: 100%; background: rgba(0, 0, 0, 0.4); position: fixed; top:0; left:0; z-index: 8;}.picker-view { width: 100%; display: flex; z-index:12; background-color: #fff; /* background: rgba(0, 0, 0, .2); */ flex-direction: column; justify-content: center; align-items: center; position: fixed; bottom: 0; left: 0rpx; height: 40vh;}.btn { width: 100%; height: 90rpx; padding: 0 24rpx; box-sizing: border-box; line-height: 90rpx; text-align: center; display: flex; background: rgba(255,255,255,.8); justify-content: space-between;}.cont { width: 100%; height: 389rpx;}.picker-item { line-height: 70rpx; margin-left: 5rpx; margin-right: 5rpx; text-align: center;}.address { width: 100%; height: 90rpx; line-height: 90rpx; text-align: center; border-bottom: 1rpx solid #f1f1f1;} addressAdd.js (分两个版本一个是动态获取的 就是选择的时候动态向后台获取内容 下方是动态获取的例子：) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363var util = require('../../../utils/util.js');var api = require('../../../config/api.js');var app = getApp();Page({ data: { addressId: 0, openSelectRegion: false, regionType: 1, selectRegionDone: false, szxqList: [], szxq: { id: &quot;&quot;, name: &quot;请选择小区&quot; }, szdsList: [], szds: { id: &quot;&quot;, name: &quot;&quot; }, fanghao: &quot;&quot;, animationAddressMenu: {}, addressMenuIsShow: false, value: [0, 0, 0], provinces: [], citys: [], areas: [], areaInfo: '', areaJson: {} }, bindinputMobile(event) { let address = this.data.address; address.mobile = event.detail.value; this.setData({ address: address }); }, bindinputName(event) { let address = this.data.address; address.name = event.detail.value; this.setData({ address: address }); }, bindinputAddress(event) { let address = this.data.address; address.address = event.detail.value; this.setData({ address: address }); }, bindIsDefault() { let address = this.data.address; address.isDefault = !address.isDefault; this.setData({ address: address }); }, getAddressDetail() { let that = this; // util.request(api.AddressDetail, { // id: that.data.addressId // }).then(function(res) { // if (res.errno === 0) { // if (res.data) { // that.setData({ // address: res.data // }); // } // } // }); }, wxChooseAddress() { let that = this; let address = this.data.address; // 用户已经同意小程序使用地址功能 wx.chooseAddress({ success: function(res) { address.provinceId = 99999; address.cityId = 88888; address.areaId = 77777; address.name = res.userName; address.mobile = res.telNumber; address.provinceName = res.provinceName; address.cityName = res.cityName; address.areaName = res.countyName; address.address = res.provinceName + res.cityName + res.countyName + res.detailInfo; that.setData({ address: address, }); } }); }, wxAddress() { let that = this; // 可以通过 wx.getSetting 先查询一下用户是否授权了 &quot;scope.address&quot; 这个 scope wx.getSetting({ success(res) { if (!res.authSetting['scope.address']) { wx.authorize({ scope: 'scope.address', success() { that.wxChooseAddress(); } }) } else { that.wxChooseAddress(); } } }) }, onLoad: function(options) { let that = this; // 页面初始化 options为页面跳转所带来的参数 console.log(options); if (options.id &amp;&amp; options.id != 0) { this.setData({ addressId: options.id }); this.getAddressDetail(); } else { that.wxAddress(); } }, onReady: function() { }, cancelAddress() { wx.navigateBack(); }, saveAddress() { console.log(this.data.address); let address = this.data.address; if (address.name == '') { util.showErrorToast('请输入姓名'); return false; } if (address.mobile == '') { util.showErrorToast('请输入手机号码'); return false; } if (address.areaId == 0) { util.showErrorToast('请输入省市区'); return false; } if (address.address == '') { util.showErrorToast('请输入详细地址'); return false; } let that = this; }, onShow: function() { // 获取所在栋数 var animation = wx.createAnimation({ duration: 500, timingFunction: 'linear', }) this.animation = animation const that = this // 获取所在地区 console.log() util.getAreaReq().then(provinces =&gt; { util.getAreaReq(provinces[0].code).then(citys =&gt; { util.getAreaReq(citys[0].code).then(areas =&gt; { that.setData({ provinces: provinces, citys: citys, areas: areas, areaJson: { provinces: { id: 40, name: &quot;广东省&quot; }, citys: { id: 4006, name: &quot;河源市&quot; }, areas: { id: 400602, name: &quot;源城区&quot; } } }) var areas = that.data.areaJson.areas.name == null ? &quot;&quot; : that.data.areaJson.areas.name var areaInfo = that.data.areaJson.provinces.name + '·' + that.data.areaJson.citys.name + '·' + areas that.setData({ areaInfo: areaInfo, }) }) }) }) }, // 点击所在地区弹出选择框 select: function(e) { // 如果已经显示，不在执行显示动画 if (this.data.addressMenuIsShow) { return false } else { // 执行显示动画 this.startAddressAnimation(true) } }, // 处理省市县联动逻辑 cityChange: function(e) { // console.log(this.data.provinces) var value = e.detail.value var provinces = this.data.provinces var citys = this.data.citys var areas = this.data.areas var provinceNum = value[0] var cityNum = value[1] var countyNum = value[2] var that = this; // console.log(provinces) // 如果省份选择项和之前不一样，表示滑动了省份，此时市默认是省的第一组数据， if (this.data.value[0] != provinceNum) { var id = provinces[provinceNum].id // console.log(citys[cityNum]) util.getAreaReq(provinces[provinceNum].code).then(citys =&gt; { util.getAreaReq(citys[0].code).then(areas =&gt; { this.setData({ value: [provinceNum, 0, 0], citys: citys, areas: areas, areaJson: { provinces: { id: provinces[provinceNum].code, name: provinces[provinceNum].area }, citys: { id: citys[0].code, name: citys[0].area }, areas: { id: areas.length &gt; 0 ? areas[0].code : null, name: areas.length &gt; 0 ? areas[0].area : null, } } }) }) }) } else if (this.data.value[1] != cityNum) { // 滑动选择了第二项数据，即市，此时区显示省市对应的第一组数据 var id = citys[cityNum].id util.getAreaReq(citys[cityNum].code).then(areas =&gt; { this.setData({ value: [provinceNum, cityNum, 0], areas: areas, areaJson: { provinces: { id: provinces[provinceNum].code, name: provinces[provinceNum].area }, citys: { id: citys[cityNum].code, name: citys[cityNum].area }, areas: { id: areas.length &gt; 0 ? areas[0].code : null, name: areas.length &gt; 0 ? areas[0].area : null, } } }) }) } else { // 滑动选择了区 this.setData({ value: [provinceNum, cityNum, countyNum], areaJson: { provinces: { id: provinces[provinceNum].code, name: provinces[provinceNum].area }, citys: { id: citys[cityNum].code, name: citys[cityNum].area }, areas: { id: areas[countyNum].code, name: areas[countyNum].area } } }) // console.log(that.data.areaJson) } }, // 执行动画 startAddressAnimation: function(isShow) { if (isShow) { // vh是用来表示尺寸的单位，高度全屏是100vh this.animation.translateY(0 + 'vh').step() } else { this.animation.translateY(40 + 'vh').step() } this.setData({ animationAddressMenu: this.animation.export(), addressMenuIsShow: isShow, }) }, // 点击地区选择取消按钮 cityCancel: function(e) { this.startAddressAnimation(false) }, // 点击地区选择确定按钮 citySure: function(e) { var that = this var city = that.data.city var value = that.data.value this.startAddressAnimation(false) // console.log(that.data.areaJson) var areas = that.data.areaJson.areas.name == null ? &quot;&quot; : that.data.areaJson.areas.name // 将选择的城市信息显示到输入框 var areaInfo = that.data.areaJson.provinces.name + '·' + that.data.areaJson.citys.name + '·' + areas that.setData({ areaInfo: areaInfo, }) }, onHide: function() { // 页面隐藏 }, onUnload: function() { // 页面关闭 }}); 需要使用外部js（utils） 自己封装的一个工具 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384var api = require('../config/api.js');var app = getApp();var user = require('./user.js');/** * 封装微信的的request */function request(url, data = {}, method = &quot;GET&quot;) { return new Promise(function(resolve, reject) { user.checkLogin().then(res =&gt; { }).catch(() =&gt; { wx.switchTab({ url: '/pages/ucenter/index/index?show=true' }); }); wx.request({ url: url, data: data, method: method, header: { 'Content-Type': 'application/x-www-form-urlencoded', 'Cookie': &quot;token=&quot; + wx.getStorageSync('token') + &quot;;&quot; + wx.getStorageSync('sessionid'), 'X-Requested-With': &quot;XMLHttpRequest&quot; }, success: function(res) { if (res.statusCode == 400) { user.loginByWeixin().then(res =&gt; { app.globalData.hasLogin = true; }); wx.redirectTo({ url: '/pages/index/index' }); wx.showToast({ title: '已经重新登录', }) } if (res.header[&quot;Set-Cookie&quot;]) { wx.setStorageSync(&quot;sessionid&quot;, res.header[&quot;Set-Cookie&quot;]) } if (res.statusCode == 200) { if (res.data.errno == 501) { } else { resolve(res); } } else { reject(res); } }, fail: function(err) { reject(err) } }) });}function getAreaReq(id) { const that = this; return new Promise(function(resolve, reject) { that.request(&quot;****&quot;, JSON.stringify({ }), &quot;post&quot;).then(response =&gt; { console.log(response.data.rs_data) resolve(response.data.rs_data); }) })}module.exports = { request, getAreaReq }; 使用静态获取的时候。js如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384var util = require('../../../utils/util.js');var api = require('../../../config/api.js');var area = require('../../../config/area.js');var app = getApp();Page({ data: { address: { id: 0, provinceId: 0, cityId: 0, areaId: 0, address: '', name: '', mobile: '', isDefault: 0, provinceName: '', cityName: '', areaName: '' }, addressId: 0, openSelectRegion: false, regionType: 1, selectRegionDone: false, szxqList: [], szxq: { id: &quot;&quot;, name: &quot;请选择小区&quot; }, szdsList: [], szds: { id: &quot;&quot;, name: &quot;&quot; }, fanghao: &quot;&quot;, animationAddressMenu: {}, addressMenuIsShow: false, value: [0, 0, 0], provinces: [], citys: [], areas: [], areaInfo: '', areaJson: {} }, bindinputMobile(event) { let address = this.data.address; address.mobile = event.detail.value; this.setData({ address: address }); }, bindinputName(event) { let address = this.data.address; address.name = event.detail.value; this.setData({ address: address }); }, bindinputAddress(event) { let address = this.data.address; address.address = event.detail.value; this.setData({ address: address }); }, bindIsDefault() { let address = this.data.address; address.isDefault = !address.isDefault; this.setData({ address: address }); }, getAddressDetail() { let that = this; util.request(api.AddressDetail, { id: that.data.addressId }).then(function(res) { if (res.errno === 0) { if (res.data) { that.setData({ address: res.data }); } } }); }, wxChooseAddress() { let that = this; let address = this.data.address; // 用户已经同意小程序使用地址功能 wx.chooseAddress({ success: function(res) { address.provinceId = 99999; address.cityId = 88888; address.areaId = 77777; address.name = res.userName; address.mobile = res.telNumber; address.provinceName = res.provinceName; address.cityName = res.cityName; address.areaName = res.countyName; address.address = res.provinceName + res.cityName + res.countyName + res.detailInfo; that.setData({ address: address, }); } }); }, wxAddress() { let that = this; // 可以通过 wx.getSetting 先查询一下用户是否授权了 &quot;scope.address&quot; 这个 scope wx.getSetting({ success(res) { if (!res.authSetting['scope.address']) { wx.authorize({ scope: 'scope.address', success() { that.wxChooseAddress(); } }) } else { that.wxChooseAddress(); } } }) }, onLoad: function(options) { let that = this; // 页面初始化 options为页面跳转所带来的参数 console.log(options); if (options.id &amp;&amp; options.id != 0) { this.setData({ addressId: options.id }); this.getAddressDetail(); } else { that.wxAddress(); } }, onReady: function() { }, cancelAddress() { wx.navigateBack(); }, saveAddress() { console.log(this.data.address); let address = this.data.address; if (address.name == '') { util.showErrorToast('请输入姓名'); return false; } if (address.mobile == '') { util.showErrorToast('请输入手机号码'); return false; } if (address.areaId == 0) { util.showErrorToast('请输入省市区'); return false; } if (address.address == '') { util.showErrorToast('请输入详细地址'); return false; } if (!check.isValidPhone(address.mobile)) { util.showErrorToast('手机号不正确'); return false; } }, onShow: function() { // 获取所在栋数 var animation = wx.createAnimation({ duration: 500, timingFunction: 'linear', }) this.animation = animation util.request(&quot;https://www.xaibox.com/czbb/interface/dataInfo.php&quot;, JSON.stringify({ &quot;param_key&quot;: { &quot;info_mode&quot;: &quot;getcity_jd&quot; }, &quot;secret_key&quot;: &quot;047709aaa7df22205d818bf4c1707458&quot; }), &quot;post&quot;).then(response =&gt; { console.log(response) that.setData({ szxqList: response.data.rs_data }) that.setData({ szxq: response.data.data[0] }) that.setData({ szds: response.data.data[0]['buildingList']['0'] }) that.setData({ szdsList: response.data.data[0]['buildingList'] }) }) // 获取所在地区 that.setData({ provinces: areajs, citys: areajs[0].children, areas: areajs[0].children ? areajs[0].children[0].children : [], areaJson: { provinces: { id: 40, name: &quot;广东省&quot; }, citys: { id: 4006, name: &quot;河源市&quot; }, areas: { id: 400602, name: &quot;源城区&quot; } } }) var areas = that.data.areaJson.areas.name == null ? &quot;&quot; : that.data.areaJson.areas.name var areaInfo = that.data.areaJson.provinces.name + '·' + that.data.areaJson.citys.name + '·' + areas that.setData({ areaInfo: areaInfo, }) }, // 点击所在地区弹出选择框 select: function(e) { // 如果已经显示，不在执行显示动画 if (this.data.addressMenuIsShow) { return false } else { // 执行显示动画 this.startAddressAnimation(true) } }, // 处理省市县联动逻辑 cityChange: function(e) { // console.log(this.data.provinces) var value = e.detail.value var provinces = this.data.provinces var citys = this.data.citys var areas = this.data.areas var provinceNum = value[0] var cityNum = value[1] var countyNum = value[2] var that = this; // console.log(provinces) // 如果省份选择项和之前不一样，表示滑动了省份，此时市默认是省的第一组数据， if (this.data.value[0] != provinceNum) { var id = provinces[provinceNum].id // console.log(citys[cityNum]) this.setData({ value: [provinceNum, 0, 0], citys: provinces[provinceNum].children, areas: provinces[provinceNum].children ? provinces[provinceNum].children[0].children : [], areaJson: { provinces: { id: provinces[provinceNum].code, name: provinces[provinceNum].area }, citys: { id: provinces[provinceNum].children[0].code, name: provinces[provinceNum].children[0].area }, areas: { id: citys[cityNum].children.length &gt; 0 ? citys[cityNum].children[0].code : null, name: citys[cityNum].children.length &gt; 0 ? citys[cityNum].children[0].area : null } } }) } else if (this.data.value[1] != cityNum) { // 滑动选择了第二项数据，即市，此时区显示省市对应的第一组数据 var id = citys[cityNum].id this.setData({ value: [provinceNum, cityNum, 0], areas: citys[cityNum].children, areaJson: { provinces: { id: provinces[provinceNum].code, name: provinces[provinceNum].area }, citys: { id: citys[cityNum].code, name: citys[cityNum].area }, areas: { id: citys[cityNum].children.length &gt; 0 ? citys[cityNum].children[0].code : null, name: citys[cityNum].children.length &gt; 0 ? citys[cityNum].children[0].area : null } } }) } else { // 滑动选择了区 this.setData({ value: [provinceNum, cityNum, countyNum], areaJson: { provinces: { id: provinces[provinceNum].code, name: provinces[provinceNum].area }, citys: { id: citys[cityNum].code, name: citys[cityNum].area }, areas: { id: areas[countyNum].code, name: areas[countyNum].area } } }) // console.log(that.data.areaJson) } }, // 执行动画 startAddressAnimation: function(isShow) { if (isShow) { // vh是用来表示尺寸的单位，高度全屏是100vh this.animation.translateY(0 + 'vh').step() } else { this.animation.translateY(40 + 'vh').step() } this.setData({ animationAddressMenu: this.animation.export(), addressMenuIsShow: isShow, }) }, // 点击地区选择取消按钮 cityCancel: function(e) { this.startAddressAnimation(false) }, // 点击地区选择确定按钮 citySure: function(e) { var that = this var city = that.data.city var value = that.data.value this.startAddressAnimation(false) // console.log(that.data.areaJson) var areas = that.data.areaJson.areas.name == null ? &quot;&quot; : that.data.areaJson.areas.name // 将选择的城市信息显示到输入框 var areaInfo = that.data.areaJson.provinces.name + '·' + that.data.areaJson.citys.name + '·' + areas that.setData({ areaInfo: areaInfo, }) }, onHide: function() { // 页面隐藏 }, onUnload: function() { // 页面关闭 }}); 静态获取三级联动 的话则需要文件area.js点击下载","link":"/2019/04/23/%E7%A8%8B%E5%BA%8F%E7%9A%84%E4%B8%89%E7%BA%A7%E8%81%94%E5%8A%A8/"},{"title":"线上服务器进行分库分表（达到亿级数据）","text":"如何设计可以动态扩容缩容的分库分表方案？ （1）选择一个数据库中间件，调研、学习、测试 （2）设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，3个库每个库4个表 （3）基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写 （4）完成单库单表到分库分表的迁移，双写方案 （5）线上系统开始基于分库分表对外提供服务 （6）扩容了，扩容成6个库，每个库需要12个表，你怎么来增加更多库和表呢？ 1）停机扩容晚上的时候挂公告 说晚上扩容整改。然后后台停机 进行数据迁移 这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。 从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万 写个工具，多弄几台机器并行跑，1小时数据就导完了 3个库+12个表，跑了一段时间了，数据量都1亿~2亿了。光是导2亿数据，都要导个几个小时，6点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10点才可以搞完 2）优化后的方案使用数据双写 然后新的分库分表的数据库和原数据库 一起用 有新的数据就两边都写入。并且开一个服务 进行将旧的数据 逐渐迁移到新的库之中 一开始上来就是32个库，每个库32个表，1024张表 我可以告诉各位同学说，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题 每个库正常承载的写入并发量是1000，那么32个库就可以承载32 * 1000 = 32000的写并发，如果每个库承载1500的写并发，32 * 1500 = 48000的写并发，接近5万/s的写入并发，前面再加一个MQ，削峰，每秒写入MQ 8万条数据，每秒消费5万条数据。 有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128个库，256个库，512个库 1024张表，假设每个表放500万数据，在MySQL里可以放50亿条数据 每秒的5万写并发，总共50亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了 谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32个库，1024张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了 一个实践是利用32 * 32来分库分表，即分为32个库，每个库里一个表分为32张表。一共就是1024张表。根据某个id先根据32取模路由到库，再根据32取模路由到库里的表。 刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个mysql服务器可能建了n个库，比如16个库。后面如果要拆分，就是不断在库和mysql服务器之间做迁移就可以了。然后系统配合改一下配置即可。 比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到1024个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表么。 这么搞，是不用自己写代码做数据迁移的，都交给dba来搞好了，但是dba确实是需要做一些库表迁移的工作，但是总比你自己写代码，抽数据导数据来的效率高得多了。 哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。 对2 ^ n取模 orderId 模 32 = 库orderId / 32 模 32 = 表 259 3 81189 5 5352 0 114593 17 15 1、设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是32库 * 32表，对于大部分公司来说，可能几年都够了 2、路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表 3、扩容的时候，申请增加更多的数据库服务器，装好mysql，倍数扩容，4台服务器，扩到8台服务器，16台服务器 4、由dba负责将原先数据库服务器的库，迁移到新的数据库服务器上去，很多工具，库迁移，比较便捷 5、我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址 6、重新发布系统，上线，原先的路由规则变都不用变，直接可以基于2倍的数据库服务器的资源，继续进行线上系统的提供服务","link":"/2019/01/21/%E7%BA%BF%E4%B8%8A%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%EF%BC%88%E8%BE%BE%E5%88%B0%E4%BA%BF%E7%BA%A7%E6%95%B0%E6%8D%AE%EF%BC%89/"},{"title":"编译安装Keepalived2.0.0","text":"简介 Keepalived是基于vrrp协议的一款高可用软件。Keepailived有一台主服务器和多台备份服务器，在主服务器和备份服务器上面部署相同的服务配置，使用一个虚拟IP地址对外提供服务，当主服务器出现故障时，虚拟IP地址会自动漂移到备份服务器。 VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议），VRRP是为了解决静态路由的高可用。VRRP的基本架构虚拟路由器由多个路由器组成，每个路由器都有各自的IP和共同的VRID(0-255)，其中一个VRRP路由器通过竞选成为MASTER，占有VIP，对外提供路由服务，其他成为BACKUP，MASTER以IP组播（组播地址：224.0.0.18）形式发送VRRP协议包，与BACKUP保持心跳连接，若MASTER不可用（或BACKUP接收不到VRRP协议包），则BACKUP通过竞选产生新的MASTER并继续对外提供路由服务，从而实现高可用。 vrrp协议的相关术语 虚拟路由器：Virtual Router虚拟路由器标识：VRID(0-255)物理路由器： master ：主设备 backup ：备用设备 priority：优先级 VIP：Virtual IPVMAC：Virutal MAC (00-00-5e-00-01-VRID)GraciousARP 安全认证 简单字符认证、HMAC机制，只对信息做认证 MD5（leepalived不支持） 工作模式 主/备：单虚拟路径器 主/主：主/备（虚拟路径器），备/主（虚拟路径器） 工作类型 抢占式：当出现比现有主服务器优先级高的服务器时，会发送通告抢占角色成为主服务器 非抢占式： 核心组件 vrrp stack：vrrp协议的实现 ipvs wrapper：为集群内的所有节点生成IPVS规则 checkers：对IPVS集群的各RS做健康状态检测 控制组件：配置文件分析器，用来实现配置文件的分析和加载 IO复用器 内存管理组件，用来管理keepalived高可用是的内存管理 注意 各节点时间必须同步 确保各节点的用于集群服务的接口支持MULTICAST通信（组播） 安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@masga ~]# yum install openssl-devel popt-devel libnl libnl-devel libnfnetlink-devel gcc -y[root@masga ~]# cd /usr/local/src/[root@masga src]# wget http://www.keepalived.org/software/keepalived-2.0.0.tar.gz[root@masga src]# tar zxvf keepalived-2.0.0.tar.gz[root@masga src]# mkdir -p ../keepalived[root@masga src]# cd keepalived-2.0.0[root@masga keepalived-2.0.0]# ./configure --prefix=/usr/local/keepalivedKeepalived configuration------------------------Keepalived version : 2.0.0Compiler : gccPreprocessor flags : Compiler flags : -Wall -Wunused -Wstrict-prototypes -Wextra -Winit-self -g -O2 -D_GNU_SOURCE -fPIE -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches Linker flags : -pieExtra Lib : -lcrypto -lssl -lnlUse IPVS Framework : YesIPVS use libnl : YesIPVS syncd attributes : NoIPVS 64 bit stats : Nofwmark socket support : YesUse VRRP Framework : YesUse VRRP VMAC : YesUse VRRP authentication : YesWith ip rules/routes : YesUse BFD Framework : NoSNMP vrrp support : NoSNMP checker support : NoSNMP RFCv2 support : NoSNMP RFCv3 support : NoDBUS support : NoSHA1 support : NoUse Json output : Nolibnl version : 1Use IPv4 devconf : NoUse libiptc : NoUse libipset : Noinit type : systemdBuild genhash : YesBuild documentation : No[root@masga keepalived-2.0.0]# make &amp;&amp; make install[root@masga keepalived-2.0.0]# cp /usr/local/src/keepalived-2.0.0/keepalived/etc/init.d/keepalived /etc/init.d/[root@masga keepalived-2.0.0]# cp /usr/local/keepalived/etc/sysconfig/keepalived /etc/sysconfig/[root@masga keepalived-2.0.0]# mkdir /etc/keepalived[root@masga keepalived-2.0.0]# cp /usr/local/keepalived/etc/keepalived/keepalived.conf /etc/keepalived/[root@masga keepalived-2.0.0]# cp /usr/local/keepalived/sbin/keepalived /usr/sbin/[root@masga keepalived-2.0.0]# echo &quot;/etc/init.d/keepalived start&quot; &gt;&gt; /etc/rc.local[root@masga keepalived-2.0.0]# chmod +x /etc/rc.d/init.d/keepalived[root@masga keepalived-2.0.0]# chkconfig keepalived on","link":"/2019/04/25/%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Keepalived2-0-0/"},{"title":"网站跨域的五种解决方式","text":"1、什么是跨越？ 一个网页向另一个不同域名/不同协议/不同端口的网页请求资源，这就是跨域。 跨域原因产生：在当前域名请求网站中，默认不允许通过ajax请求发送其他域名。 网络请求示意图 2、为什么会产生跨域请求？ 因为浏览器使用了同源策略 3、什么是同源策略？ 同源策略是Netscape提出的一个著名的安全策略，现在所有支持JavaScript的浏览器都会使用这个策略。同源策略是浏览器最核心也最基本的安全功能，如果缺少同源策略，浏览器的正常功能可能受到影响。可以说web是构建在同源策略的基础之上的，浏览器只是针对同源策略的一种实现。 4、为什么浏览器要使用同源策略？ 是为了保证用户的信息安全，防止恶意网站窃取数据，如果网页之间不满足同源要求，将不能: 1、共享Cookie、LocalStorage、IndexDB 2、获取DOM 3、AJAX请求不能发送 同源策略的非绝对性： 123456&lt;script&gt;&lt;/script&gt;&lt;img/&gt;&lt;iframe/&gt;&lt;link/&gt;&lt;video/&gt;&lt;audio/&gt; 等带有src属性的标签可以从不同的域加载和执行资源。其他插件的同源策略：flash、java applet、silverlight、googlegears等浏览器加载的第三方插件也有各自的同源策略，只是这些同源策略不属于浏览器原生的同源策略，如果有漏洞则可能被黑客利用，从而留下XSS攻击的后患 所谓的同源指：域名、网络协议、端口号相同，三条有一条不同就会产生跨域。 例如：你用浏览器打开http://baidu.com，浏览器执行JavaScript脚本时发现脚本向http://cloud.baidu.com域名发请求，这时浏览器就会报错，这就是跨域报错。 解决方案有五：1、前端使用jsonp （不推荐使用） 当我们正常地请求一个JSON数据的时候，服务端返回的是一串 JSON类型的数据，而我们使用 JSONP模式来请求数据的时候服务端返回的是一段可执行的 JavaScript代码。因为jsonp 跨域的原理就是用的动态加载 script的src ，所以我们只能把参数通过 url的方式传递,所以jsonp的 type类型只能是get示例： 123456789$.ajax({ url: 'http://192.168.1.114/yii/demos/test.php', //不同的域 type: 'GET', // jsonp模式只有GET 是合法的 data: { 'action': 'aaron' }, dataType: 'jsonp', // 数据类型 jsonp: 'backfunc', // 指定回调函数名，与服务器端接收的一致，并回传回来}) 使用JSONP 模式来请求数据的整个流程：客户端发送一个请求，规定一个可执行的函数名（这里就是 jQuery做了封装的处理，自动帮你生成回调函数并把数据取出来供success属性方法来调用,而不是传递的一个回调句柄），服务器端接受了这个 backfunc函数名，然后把数据通过实参的形式发送出去 （在jquery 源码中， jsonp的实现方式是动态添加&lt;script&gt;标签来调用服务器提供的 js脚本。jquery 会在window对象中加载一个全局的函数，当 &lt;script&gt;代码插入时函数执行，执行完毕后就 &lt;script&gt;会被移除。同时jquery还对非跨域的请求进行了优化，如果这个请求是在同一个域名下那么他就会像正常的 Ajax请求一样工作。） 2、后台Http请求转发 使用HttpClinet转发进行转发(简单的例子 不推荐使用这种方式) 1234567891011try { HttpClient client = HttpClients.createDefault(); //client对象 HttpGet get = new HttpGet(&quot;http://localhost:8080/test&quot;); //创建get请求 CloseableHttpResponse response = httpClient.execute(get); //执行get请求 String mes = EntityUtils.toString(response.getEntity()); //将返回体的信息转换为字符串 System.out.println(mes);} catch (ClientProtocolException e) { e.printStackTrace();} catch (IOException e) { e.printStackTrace();} 3、后台配置同源Cors （推荐） 在SpringBoot2.0 上的跨域 用以下代码配置 即可完美解决你的前后端跨域请求问题 在SpringBoot2.0 上的跨域 用以下代码配置 即可完美解决你的前后端跨域请求问题 123456789101112131415161718192021222324252627282930313233import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.web.cors.CorsConfiguration;import org.springframework.web.cors.UrlBasedCorsConfigurationSource;import org.springframework.web.filter.CorsFilter;/** * 实现基本的跨域请求 * @author linhongcun * */@Configurationpublic class CorsConfig { @Bean public CorsFilter corsFilter() { final UrlBasedCorsConfigurationSource urlBasedCorsConfigurationSource = new UrlBasedCorsConfigurationSource(); final CorsConfiguration corsConfiguration = new CorsConfiguration(); /*是否允许请求带有验证信息*/ corsConfiguration.setAllowCredentials(true); /*允许访问的客户端域名*/ corsConfiguration.addAllowedOrigin(&quot;*&quot;); /*允许服务端访问的客户端请求头*/ corsConfiguration.addAllowedHeader(&quot;*&quot;); /*允许访问的方法名,GET POST等*/ corsConfiguration.addAllowedMethod(&quot;*&quot;); urlBasedCorsConfigurationSource.registerCorsConfiguration(&quot;/**&quot;, corsConfiguration); return new CorsFilter(urlBasedCorsConfigurationSource); }} 4、使用SpringCloud网关 服务网关(zuul)又称路由中心，用来统一访问所有api接口，维护服务。 Spring Cloud Zuul通过与Spring Cloud Eureka的整合，实现了对服务实例的自动化维护，所以在使用服务路由配置的时候，我们不需要向传统路由配置方式那样去指定具体的服务实例地址，只需要通过Ant模式配置文件参数即可 5、使用nginx做转发 现在有两个网站想互相访问接口 在http://a.a.com:81/A中想访问 http://b.b.com:81/B 那么进行如下配置即可 然后通过访问 www.my.com/A 里面即可访问 www.my.com/B123456789101112server { listen 80; server_name www.my.com; location /A { proxy_pass http://a.a.com:81/A; index index.html index.htm; } location /B { proxy_pass http://b.b.com:81/B; index index.html index.htm; } } 如果是两个端口想互相访问接口 在http://b.b.com:80/Api中想访问 http://b.b.com:81/Api 那么进行如下配置即可 使用nginx转发机制就可以完成跨域问题12345678server { listen 80; server_name b.b.com; location /Api { proxy_pass http://b.b.com:81/Api; index index.html index.htm; } }","link":"/2019/04/25/%E7%BD%91%E7%AB%99%E8%B7%A8%E5%9F%9F%E7%9A%84%E5%9B%9B%E7%A7%8D%E8%A7%A3%E5%86%B3%E6%96%B9%E5%BC%8F/"},{"title":"网站防止恶意登陆或防盗链的使用","text":"使用场景：明明引用了一个正确的图片地址，但显示出来的却是一个红叉或写有“此图片仅限于网站用户交流沟通使用”之类的“假图片”。用嗅探软件找到了多媒体资源的真实地址用下载软件仍然不能下载。下载一些资源时总是出错，如果确认地址没错的话，大多数情况都是遇上防盗链系统了。常见的防盗链系统，一般使用在图片、音视频、软件等相关的资源上。 实现原理：把当前请求的主机与服务器的主机进行比对，如果不一样则就是恶意链接，反之则是正常链接。 不说了，直接上代码： 1234567891011121314String address=request.getHeader(&quot;referer&quot;); //获取页面的请求地址 String pathAdd=&quot;&quot;; //定义空字符串 if(address!=null){ //判断当前的页面的请求地址为空时 URL urlOne=new URL(address);//实例化URL方法 pathAdd=urlOne.getHost(); //获取请求页面的服务器主机 } String address1=request.getRequestURL().toString(); //获取当前页面的地址 String pathAdd1=&quot;&quot;; if(address1!=null){ URL urlTwo=new URL(address1); pathAdd1=urlTwo.getHost(); //获取当前服务器的主机 } if(!pathAdd.equals(pathAdd1)){ //判断当前页面的主机与服务器的主机是否相同 } 根据这个原理 可以设置企业白名单 使用Request对象设置页面的防盗链 所谓的防盗链就是当你以一个非正常渠道去访问某一个Web资源的时候，服务器会将你的请求忽略并且将你的当前请求变为按正常渠道访问时的请求并返回到相应的页面，用户只有通过该页面中的相关操作去访问想要请求的最终资源。 例如，你有一个访问某资源的网址，但是你事先不知道这个网址是有防盗链的，那么当你输入该网址时你可能会发现，并没有马上跳转到你想要的资源页面而是一些无关的信息页面，但是就是在这些信息页面中你发现有一个超链接或是其他操作可以跳转到你所访问的最终资源页面。 这就是防盗链技术了，好了来看一个具体应用： 123456789101112131415161718192021222324252627282930313233Request.java package net.csdn.request;import java.io.IOException; import java.io.PrintWriter;import java.util.Enumeration import javax.servlet.RequestDispatcher; import javax.servlet.ServletException; import javax.servlet.http.HttpServlet; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpServletResponse; public class Request extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {getDoorChain(request, response);} private void getDoorChain(HttpServletRequest request, HttpServletResponse response) throws IOException { String referer = request.getHeader(&quot;referer&quot;); if(referer==null || !referer.endsWith(&quot;http://localhost:8080/Request/index.jsp&quot;)){ response.sendRedirect(&quot;http://localhost:8080/Request/index.jsp&quot;); return; } response.setCharacterEncoding(&quot;utf-8&quot;); response.setContentType(&quot;text/html;charset =utf-8&quot;); PrintWriter pw = response.getWriter(); pw.write(&quot;喜剧片《东成西就》&quot;); } public void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { doGet(request, response); } } 123456789101112131415161718192021222324252627282930313233index.jsp &lt;%@ page language=&quot;java&quot; import=&quot;java.util.*&quot; pageEncoding=&quot;utf-8&quot;%&gt; &lt;% String path = request.getContextPath(); String basePath = request.getScheme()+&quot;://&quot;+request.getServerName()+&quot;:&quot; +request.getServerPort()+path+&quot;/&quot;; %&gt; &lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&gt; &lt;html&gt; &lt;head&gt; &lt;base href=&quot;&lt;%=basePath%&gt;&quot;&gt; &lt;title&gt;My JSP 'index.jsp' starting page&lt;/title&gt; &lt;meta http-equiv=&quot;pragma&quot; content=&quot;no-cache&quot;&gt; &lt;meta http-equiv=&quot;cache-control&quot; content=&quot;no-cache&quot;&gt; &lt;meta http-equiv=&quot;expires&quot; content=&quot;0&quot;&gt; &lt;meta http-equiv=&quot;keywords&quot; content=&quot;keyword1,keyword2,keyword3&quot;&gt; &lt;meta http-equiv=&quot;description&quot; content=&quot;This is my page&quot;&gt; &lt;!-- &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;styles.css&quot;&gt; --&gt; &lt;/head&gt; &lt;body&gt; 这里是防盗链技术的应用检测！ &lt;br&gt; &lt;a href =&quot;/Request/Request&quot; &gt;喜剧片 &lt;/a&gt; &lt;/body&gt; &lt;/html&gt; 例如我最终想要通过http://lcoalhost:8080/Request/Request这个网址获取到我想要的《东成西就》 的资源可是当我真正的输入这个网址时，却转到了： http://localhost:8080/Request/index.jsp这个页面 只有当你点击“喜剧片”这个超链接时才会真正的得到你想要的资源页面 什么是Referer？这里的 Referer 指的是HTTP头部的一个字段，也称为HTTP来源地址（HTTP Referer），用来表示从哪儿链接到目前的网页，采用的格式是URL。换句话说，借着 HTTP Referer 头部网页可以检查访客从哪里而来，这也常被用来对付伪造的跨网站请求。 什么是空Referer，什么时候会出现空Referer？ 首先，我们对空Referer的定义为，Referer 头部的内容为空，或者，一个HTTP请求中根本不包含Referer头部。 那么什么时候HTTP请求会不包含Referer字段呢？根据Referer的定义，它的作用是指示一个请求是从哪里链接过来，那么当一个请求并不是由链接触发产生的，那么自然也就不需要指定这个请求的链接来源。 比如，直接在浏览器的地址栏中输入一个资源的URL地址，那么这种请求是不会包含Referer字段的，因为这是一个“凭空产生”的HTTP请求，并不是从一个地方链接过去的。 在防盗链设置中，允许空Referer和不允许空Referer有什么区别？ 在防盗链中，如果允许包含空的Referer，那么通过浏览器地址栏直接访问该资源URL是可以访问到的； 但如果不允许包含空的Referer，那么通过浏览器直接访问也是被禁止的。","link":"/2019/05/10/%E7%BD%91%E7%AB%99%E9%98%B2%E6%AD%A2%E6%81%B6%E6%84%8F%E7%99%BB%E9%99%86%E6%88%96%E9%98%B2%E7%9B%97%E9%93%BE%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"自己搭建一个 SpringCloud 商城 --- 第一天","text":"最近看到了一个 微服务制作的商城 onemall https://gitee.com/zhijiantianya/onemall 这是一个未完善的商城。 突然 自己也想搭建一个SpringCloud 商城 搭建项目环境 创建项目 12345678910111213141516171819├─nnmall----------------------------父项目，公共依赖│ ││ ├─nnmall-eureka-----------------------微服务配置中心│ ││ ├─nnmall-system│ │ ││ │ ├─nnmall-system-api-------------------管理员api│ │ ││ │ ├─nnmall-system-impl------------------管理员实现类│ │ ││ │ ├─nnmall-system-sdk-------------------管理员提供的权限拦截注解│ │ ││ │ └─nnmall-system-web-------------------管理员web│ ││ ├─nnmall-common--------------------------公共包│ │ ││ │ ├─mall-spring-core--------------------核心包 全局异常拦截与拦截器│ │ ││ │ └─nnmall-common-core------------------一些常用对象和类 创建gitlab仓库 权限服务 先完成商城后台管理 加入管理员权限拦截 增加、删除、更改、查询 记录遇到的种种坑 一定要注意扫包的路径 在springboot 扫包路径最好是填写一下 不然他不扫你的jar包里面的注解 1234567891011121314151617import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.openfeign.EnableFeignClients;import org.springframework.scheduling.annotation.EnableAsync;@SpringBootApplication(scanBasePackages = {&quot;cn.yunlongn.mall.admin&quot;, &quot;cn.yunlongn.mall.spring.core&quot;})@EnableAsync(proxyTargetClass = true)@EnableEurekaClient@EnableFeignClients(basePackages = {&quot;cn.yunlongn.mall.admin&quot;})public class SystemApplication { public static void main(String[] args) { SpringApplication.run(SystemApplication.class, args); }} 在feign的调用中遇到的问题有 如果你的服务返回了错误信息。那么你的返回结果可能就不一定会有错误提示。 最好是转发一下你服务的错误提示。 转发方法如下 继承feign 的异常处理类 使用feign的时候 最好是能做一个fallback处理使用hystrix 的熔断机制。当服务不能即时响应的时候 可以自己设置一个返回信息 （请参考https://blog.csdn.net/asdfsadfasdfsa/article/details/79286960） 1234567891011121314151617181920212223242526272829303132333435363738import cn.yunlongn.mall.common.core.exception.ServiceException;import com.alibaba.fastjson.JSONException;import com.alibaba.fastjson.JSONObject;import feign.Response;import feign.Util;import feign.codec.ErrorDecoder;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.annotation.Configuration;import java.io.IOException;/** * 定义feign的异常处理类 在feign出异常的时候进入此类 * 例如feign的密码错误 */@Configurationpublic class FeignErrorDecoder implements ErrorDecoder { @Override public Exception decode(String methodKey, Response response) { try { // 这里直接拿到我们抛出的异常信息 String message = Util.toString(response.body().asReader()); try { JSONObject jsonObject = JSONObject.parseObject(message); return new ServiceException(jsonObject.getInteger(&quot;code&quot;),jsonObject.getString(&quot;message&quot;)); } catch (JSONException e) { e.printStackTrace(); } } catch (IOException ignored) { } return decode(methodKey, response); }}","link":"/2019/05/31/%E8%87%AA%E5%B7%B1%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA-SpringCloud-%E5%95%86%E5%9F%8E-%E7%AC%AC%E4%B8%80%E5%A4%A9/"},{"title":"解决git push代码到github上一直提示输入用户名及密码的问题","text":"我们将github上的工程clone到本地后，修改完代码后想要push到github，但一直会有提示输入用户名及密码.原因分析 出现这种情况的原因是我们使用了http的方式clone代码到本地，相应的，也是使用http的方式将代码push到服务器。 如图所示，在github系统上克隆代码的地址默认采用的是http的方式，我们一般这样clone代码： git clone https://github.com/yychuyu/linux-system-programming.git 这就容易导致这个问题的出现。 而如果采用ssh方式的话，是这样clone代码的： git clone git@github.com:yychuyu/linux-system-programming.git 解决办法 解决办法很简单，将http方式改为ssh方式即可。 先查看当前方式： git remote -v 把http方式改为ssh方式。先移除旧的http的origin： git remote rm origin 再添加新的ssh方式的origin： git remote add origin git@github.com:yunlongn/myHexo.git 改动完之后直接执行git push是无法推送代码的，需要设置一下上游要跟踪的分支，与此同时会自动执行一次git push命令，此时已经不用要求输入用户名及密码啦！ git push --set-upstream origin master","link":"/2019/04/29/%E8%A7%A3%E5%86%B3git-push%E4%BB%A3%E7%A0%81%E5%88%B0github%E4%B8%8A%E4%B8%80%E7%9B%B4%E6%8F%90%E7%A4%BA%E8%BE%93%E5%85%A5%E7%94%A8%E6%88%B7%E5%90%8D%E5%8F%8A%E5%AF%86%E7%A0%81%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"记一次事务的坑 Transaction rolled back because it has been marked as rollback-only","text":"最近在项目中发现了一则报错：“org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only”。根据报错信息来看是spring框架中的事务管理报错：事务回滚了，因为它被标记为回滚状态。 报错原因多层嵌套事务中，如果使用了默认的事务传播方式，当内层事务抛出异常，外层事务捕捉并正常执行完毕时，就会报出rollback-only异常。 spring框架是使用AOP的方式来管理事务，如果一个被事务管理的方法正常执行完毕，方法结束时spring会将方法中的sql进行提交。如果方法执行过程中出现异常，则回滚。spring框架的默认事务传播方式是PROPAGATION_REQUIRED：如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。在项目中，一般我们都会使用默认的传播方式，这样无论外层事务和内层事务任何一个出现异常，那么所有的sql都不会执行。在嵌套事务场景中，内层事务的sql和外层事务的sql会在外层事务结束时进行提交或回滚。如果内层事务抛出异常e，在内层事务结束时，spring会把事务标记为“rollback-only”。这时如果外层事务捕捉了异常e，那么外层事务方法还会继续执行代码，直到外层事务也结束时，spring发现事务已经被标记为“rollback-only”，但方法却正常执行完毕了，这时spring就会抛出“org.springframework.transaction.UnexpectedRollbackException: Transaction rolled back because it has been marked as rollback-only”。代码示例如下： 12345678910111213141516171819Class ServiceA { @Resource(name = &quot;serviceB&quot;) private ServiceB b; @Transactional public void a() { try { b.b() } catch (Exception ignore) { } }}Class ServiceB { @Transactional public void b() { throw new RuntimeException(); }} 当调用a()时，就会报出“rollback-only”异常。 解决方案 如果希望内层事务抛出异常时中断程序执行，直接在外层事务的catch代码块中抛出e. 如果希望程序正常执行完毕，并且希望外层事务结束时全部提交，需要在内层事务中做异常捕获处理。 如果希望内层事务回滚，但不影响外层事务提交，需要将内层事务的传播方式指定为PROPAGATION_NESTED。注：PROPAGATION_NESTED基于数据库savepoint实现的嵌套事务，外层事务的提交和回滚能够控制嵌内层事务，而内层事务报错时，可以返回原始savepoint，外层事务可以继续提交。 在我的项目中之所以会报“rollback-only”异常的根本原因是代码风格不一致的原因。外层事务对错误的处理方式是返回true或false来告诉上游执行结果，而内层事务是通过抛出异常来告诉上游（这里指外层事务）执行结果，这种差异就导致了“rollback-only”异常。虽然最后事务依然是回滚了，不影响程序对sql的处理，但外层事务的上游本期望返回true和false，却收到了UnexpectedRollbackException异常，(╯￣Д￣)╯︵ ┻━┻。 附：事务传播方式 @see org.springframework.transaction.annotation.Propagation | 事务传播方式 | 说明 | | ——– | —– || PROPAGATION_REQUIRED | 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是默认的传播方式 || PROPAGATION_SUPPORTS | 支持当前事务，如果当前没有事务，就以非事务方式执行 || PROPAGATION_MANDATORY | 使用当前的事务，如果当前没有事务，就抛出异常 || PROPAGATION_REQUIRES_NEW | 新建事务，如果当前存在事务，把当前事务挂起 || PROPAGATION_NOT_SUPPORTED | 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起 || PROPAGATION_NEVER | 以非事务方式执行，如果当前存在事务，则抛出异常 || PROPAGATION_SUPPORTS | 支持当前事务，如果当前没有事务，就以非事务方式执行。 || PROPAGATION_NESTED | 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。 |","link":"/2019/05/06/%E8%AE%B0%E4%B8%80%E6%AC%A1%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%9D%91Transaction-rolled-back-because-it-has-been-marked-as-rollback-only/"},{"title":"说说MySQL读写分离的原理？主从同步延时咋解决？","text":"1、面试题你们有没有做MySQL读写分离？如何实现mysql的读写分离？MySQL主从复制原理的是啥？如何解决mysql主从同步的延时问题？ 2、面试官心里分析这个，高并发这个阶段，那肯定是需要做读写分离的，啥意思？因为实际上大部分的互联网公司，一些网站，或者是app，其实都是读多写少。所以针对这个情况，就是写一个主库，但是主库挂多个从库，然后从多个从库来读，那不就可以支撑更高的读并发压力了吗？ 3、面试题剖析（1）如何实现mysql的读写分离？其实很简单，就是基于主从复制架构，简单来说，就搞一个主库，挂多个从库，然后我们就单单只是写主库，然后主库会自动把数据给同步到从库上去。 （2）MySQL主从复制原理的是啥？主库将变更写binlog日志，然后从库连接到主库之后，从库有一个IO线程，将主库的binlog日志拷贝到自己本地，写入一个中继日志中。接着从库中有一个SQL线程会从中继日志读取binlog，然后执行binlog日志中的内容，也就是在自己本地再次执行一遍SQL，这样就可以保证自己跟主库的数据是一样的。 这里有一个非常重要的一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行的操作，在从库上会串行执行。所以这就是一个非常重要的点了，由于从库从主库拷贝日志以及串行执行SQL的特点，在高并发场景下，从库的数据一定会比主库慢一些，是有延时的。所以经常出现，刚写入主库的数据可能是读不到的，要过几十毫秒，甚至几百毫秒才能读取到。 而且这里还有另外一个问题，就是如果主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。 所以mysql实际上在这一块有两个机制，一个是半同步复制，用来解决主库数据丢失问题；一个是并行复制，用来解决主从同步延时问题。 这个所谓半同步复制，semi-sync复制，指的就是主库写入binlog日志之后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了。 所谓并行复制，指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 1）主从复制的原理 2）主从延迟问题产生的原因 3）主从复制的数据丢失问题，以及半同步复制的原理 4）并行复制的原理，多库并发重放relay日志，缓解主从延迟问题 （3）mysql主从同步延时问题（精华）线上确实处理过因为主从同步延时问题，导致的线上的bug，小型的生产事故 show status，Seconds_Behind_Master，你可以看到从库复制主库的数据落后了几ms 其实这块东西我们经常会碰到，就比如说用了mysql主从架构之后，可能会发现，刚写入库的数据结果没查到，结果就完蛋了。。。。 所以实际上你要考虑好应该在什么场景下来用这个mysql主从同步，建议是一般在读远远多于写，而且读的时候一般对数据时效性要求没那么高的时候，用mysql主从同步 所以这个时候，我们可以考虑的一个事情就是，你可以用mysql的并行复制，但是问题是那是库级别的并行，所以有时候作用不是很大 所以这个时候。。通常来说，我们会对于那种写了之后立马就要保证可以查到的场景，采用强制读主库的方式，这样就可以保证你肯定的可以读到数据了吧。其实用一些数据库中间件是没问题的。 一般来说，如果主从延迟较为严重 1、分库，将一个主库拆分为4个主库，每个主库的写并发就500/s，此时主从延迟可以忽略不计 2、打开mysql支持的并行复制，多个库并行复制，如果说某个库的写入并发就是特别高，单库写并发达到了2000/s，并行复制还是没意义。28法则，很多时候比如说，就是少数的几个订单表，写入了2000/s，其他几十个表10/s。 3、重写代码，写代码的同学，要慎重，当时我们其实短期是让那个同学重写了一下代码，插入数据之后，直接就更新，不要查询 4、如果确实是存在必须先插入，立马要求就查询到，然后立马就要反过来执行一些操作，对这个查询设置直连主库。不推荐这种方法，你这么搞导致读写分离的意义就丧失了","link":"/2019/01/21/%E8%AF%B4%E8%AF%B4MySQL%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E7%9A%84%E5%8E%9F%E7%90%86%EF%BC%9F%E4%B8%BB%E4%BB%8E%E5%90%8C%E6%AD%A5%E5%BB%B6%E6%97%B6%E5%92%8B%E8%A7%A3%E5%86%B3%EF%BC%9F/"},{"title":"面试题 - 使用线程交替打印奇数偶数","text":"这世上有三样东西是别人抢不走的：一是吃进胃里的食物，二是藏在心中的梦想，三是读进大脑的书 分析题目。需要使用两个线程交替打印奇偶数。 使用同步锁解决这个问题 使用信号量来实现交替打印 定义两个信号量，一个奇数信号量，一个偶数信号量，都初始化为1 先用掉偶数的信号量，因为要让奇数先启动，等奇数打印完再释放 信号量实现 具体实现思路： 定义两个信号量，一个奇数信号量，一个偶数信号量，都初始化为1 先用掉偶数的信号量，因为要让奇数先启动，等奇数打印完再释放 具体流程就是 第一次的时候先减掉偶数的信号量 奇数线程打印完成以后用掉奇数的信号量。然后释放偶数的信号量如此循环 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import java.util.concurrent.Semaphore;/** * @ClassName AlternatePrinting * @Author yunlogn * @Date 2019/5/21 * @Description 交替打印奇偶数 */public class AlternatePrinting { static int i = 0; public static void main(String[] args) throws InterruptedException { Semaphore semaphoreOdd = new Semaphore(1); Semaphore semaphoreEven = new Semaphore(1); semaphoreOdd.acquire(); //让奇数先等待启动，所以先减掉偶数的信号量 等奇数线程来释放 SemaphorePrintEven semaphorePrintEven = new SemaphorePrintEven(semaphoreOdd, semaphoreEven); Thread t1 = new Thread(semaphorePrintEven); t1.start(); SemaphorePrintOdd semaphorePrintOdd = new SemaphorePrintOdd(semaphoreOdd, semaphoreEven); Thread t2 = new Thread(semaphorePrintOdd); t2.start(); } /** * 使用信号量实现 */ static class SemaphorePrintOdd implements Runnable { private Semaphore semaphoreOdd; private Semaphore semaphoreEven; public SemaphorePrintOdd(Semaphore semaphoreOdd, Semaphore semaphoreEven) { this.semaphoreOdd = semaphoreOdd; this.semaphoreEven = semaphoreEven; } @Override public void run() { try { semaphoreOdd.acquire();//获取信号量 semaphoreOdd在初始化的时候被获取了信号量所以这里被阻塞了，所以会先执行下面的奇数线程 while (true) { i++; if (i % 2 == 0) { System.out.println(&quot;偶数线程：&quot; + i); semaphoreEven.release();//释放偶数信号量 让奇数线程那边的阻塞解除 //再次申请获取偶数信号量，因为之前已经获取过，如果没有奇数线程去释放，那么就会一直阻塞在这，等待奇数线程释放 semaphoreOdd.acquire(); } } } catch (InterruptedException e) { e.printStackTrace(); } } } static class SemaphorePrintEven implements Runnable { private Semaphore semaphoreOdd; private Semaphore semaphoreEven; public SemaphorePrintEven(Semaphore semaphoreOdd, Semaphore semaphoreEven) { this.semaphoreOdd = semaphoreOdd; this.semaphoreEven = semaphoreEven; } @Override public void run() { try { semaphoreEven.acquire(); while (true) { i++; if (i % 2 == 1) { System.out.println(&quot;奇数线程：&quot; + i); semaphoreOdd.release(); //释放奇数信号量 让偶数线程那边的阻塞解除 //这里阻塞，等待偶数线程释放信号量 //再次申请获取奇数信号量，需要等偶数线程执行完然后释放该信号量，不然阻塞 semaphoreEven.acquire(); } } } catch (Exception ex) {} } }} 需要注意的是，如果某个线程来不及释放就异常中断了，会导致另一个线程一直在等，造成死锁。 虽然这个异常不在这个问题的考虑范围内 但是可以使用finally 来包裹释放锁资源 同步锁打印 让两个线程使用同一把锁。交替执行 。 判断是不是奇数 如果是奇数进入奇数线程执行打印并加一。然后线程释放锁资源。然后让该线程等待 判断是不是偶数，如果是偶数进入偶数线程执行打印并加一。然后线程释放锁资源。然后让该线程等待 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import java.util.concurrent.atomic.AtomicInteger;/** * @ClassName AlternatePrinting * @Author yunlogn * @Date 2019/5/21 * @Description 交替打印奇偶数 */public class AlternatePrinting { public static AtomicInteger atomicInteger = new AtomicInteger(1); public static void main(String[] args) throws InterruptedException { Thread a=new Thread(new AThread()); Thread b=new Thread(new BThread()); a.start(); b.start(); } public static class AThread implements Runnable { @Override public void run() { while (true) { synchronized (atomicInteger) { if (atomicInteger.intValue() % 2 != 0) { System.out.println(&quot;奇数线程:&quot; + atomicInteger.intValue()); atomicInteger.getAndIncrement(); // 奇数线程释放锁资源 atomicInteger.notify(); try { atomicInteger.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } else { try { // 奇数线程等待 atomicInteger.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } } } } public static class BThread implements Runnable { @Override public void run() { while (true){ synchronized (atomicInteger){ if(atomicInteger.intValue() %2== 0 ){ System.out.println(&quot;偶数线程:&quot;+ atomicInteger.intValue()); atomicInteger.getAndIncrement(); // 偶数线程释放锁资源 atomicInteger.notify(); try { atomicInteger.wait(); } catch (InterruptedException e) { e.printStackTrace(); } }else{ try { // 偶数线程等待 atomicInteger.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } } } }} 一种更简单的写法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class TheadTest { public static void main(String[] args) { PrintDigitThread print1 = new PrintDigitThread((i) -&gt; i % 2 == 1, &quot;thread1&quot;); PrintDigitThread print2 = new PrintDigitThread((i) -&gt; i % 2 == 0, &quot;thread2&quot;); print1.start(); print2.start(); }} class ShareData { public static final AtomicInteger atomicInt = new AtomicInteger(0);} class PrintDigitThread extends Thread { private Predicate&lt;Integer&gt; predicate; public PrintDigitThread(Predicate&lt;Integer&gt; predicate, String name) { this.predicate = predicate; this.setName(name); } @Override public void run() { int v = ShareData.atomicInt.get(); while (v &lt; 100) { synchronized (ShareData.atomicInt) { v = ShareData.atomicInt.get(); if (predicate.test(v)) { System.out.println(Thread.currentThread().getName() + &quot;:&quot; + v); ShareData.atomicInt.incrementAndGet(); try { ShareData.atomicInt.notify(); } catch (Exception ex) { } } else { try { ShareData.atomicInt.wait(); } catch (Exception ex) { } } } } }} 欢迎关注 http://yunlongn.github.io","link":"/2019/05/21/%E9%9D%A2%E8%AF%95%E9%A2%98-%E4%BD%BF%E7%94%A8%E7%BA%BF%E7%A8%8B%E4%BA%A4%E6%9B%BF%E6%89%93%E5%8D%B0%E5%A5%87%E6%95%B0%E5%81%B6%E6%95%B0/"},{"title":"给你的SpringBoot做埋点监控--JVM应用度量框架Micrometer","text":"JVM应用度量框架Micrometer实战前提spring-actuator做度量统计收集，使用Prometheus（普罗米修斯）进行数据收集，Grafana（增强ui）进行数据展示，用于监控生成环境机器的性能指标和业务数据指标。一般，我们叫这样的操作为”埋点”。SpringBoot中的依赖spring-actuator中集成的度量统计API使用的框架是Micrometer，官网是Micrometer.io。在实践中发现了业务开发者滥用了Micrometer的度量类型Counter，导致无论什么情况下都只使用计数统计的功能。这篇文章就是基于Micrometer分析其他的度量类型API的作用和适用场景。 Micrometer提供的度量类库Meter是指一组用于收集应用中的度量数据的接口，Meter单词可以翻译为”米”或者”千分尺”，但是显然听起来都不是很合理，因此下文直接叫Meter，理解它为度量接口即可。Meter是由MeterRegistry创建和保存的，可以理解MeterRegistry是Meter的工厂和缓存中心，一般而言每个JVM应用在使用Micrometer的时候必须创建一个MeterRegistry的具体实现。Micrometer中，Meter的具体类型包括：Timer，Counter，Gauge，DistributionSummary，LongTaskTimer，FunctionCounter，FunctionTimer和TimeGauge。下面分节详细介绍这些类型的使用方法和实战使用场景。而一个Meter具体类型需要通过名字和Tag(这里指的是Micrometer提供的Tag接口)作为它的唯一标识，这样做的好处是可以使用名字进行标记，通过不同的Tag去区分多种维度进行数据统计。 MeterRegistryMeterRegistry在Micrometer是一个抽象类，主要实现包括： 1、SimpleMeterRegistry：每个Meter的最新数据可以收集到SimpleMeterRegistry实例中，但是这些数据不会发布到其他系统，也就是数据是位于应用的内存中的。 2、CompositeMeterRegistry：多个MeterRegistry聚合，内部维护了一个MeterRegistry的列表。 3、全局的MeterRegistry：工厂类io.micrometer.core.instrument.Metrics中持有一个静态final的CompositeMeterRegistry实例globalRegistry。 当然，使用者也可以自行继承MeterRegistry去实现自定义的MeterRegistry。SimpleMeterRegistry适合做调试的时候使用，它的简单使用方式如下： 123MeterRegistry registry = new SimpleMeterRegistry(); Counter counter = registry.counter(&quot;counter&quot;);counter.increment(); CompositeMeterRegistry实例初始化的时候，内部持有的MeterRegistry列表是空的，如果此时用它新增一个Meter实例，Meter实例的操作是无效的 12345678910CompositeMeterRegistry composite = new CompositeMeterRegistry();Counter compositeCounter = composite.counter(&quot;counter&quot;);compositeCounter.increment(); // &lt;- 实际上这一步操作是无效的,但是不会报错SimpleMeterRegistry simple = new SimpleMeterRegistry();composite.add(simple); // &lt;- 向CompositeMeterRegistry实例中添加SimpleMeterRegistry实例compositeCounter.increment(); // &lt;-计数成功 全局的MeterRegistry的使用方式更加简单便捷，因为一切只需要操作工厂类Metrics的静态方法： 123Metrics.addRegistry(new SimpleMeterRegistry());Counter counter = Metrics.counter(&quot;counter&quot;, &quot;tag-1&quot;, &quot;tag-2&quot;);counter.increment(); Tag与Meter的命名Micrometer中，Meter的命名约定使用英文逗号(dot，也就是”.”)分隔单词。但是对于不同的监控系统，对命名的规约可能并不相同，如果命名规约不一致，在做监控系统迁移或者切换的时候，可能会对新的系统造成破坏。Micrometer中使用英文逗号分隔单词的命名规则，再通过底层的命名转换接口NamingConvention进行转换，最终可以适配不同的监控系统，同时可以消除监控系统不允许的特殊字符的名称和标记等。开发者也可以覆盖NamingConvention实现自定义的命名转换规则：registry.config().namingConvention(myCustomNamingConvention);。在Micrometer中，对一些主流的监控系统或者存储系统的命名规则提供了默认的转换方式，例如当我们使用下面的命名时候： 12MeterRegistry registry = ...registry.timer(&quot;http.server.requests&quot;); 对于不同的监控系统或者存储系统，命名会自动转换如下： 1、Prometheus - http_server_requests_duration_seconds。 2、Atlas - httpServerRequests。 3、Graphite - http.server.requests。 4、InfluxDB - http_server_requests。 其实NamingConvention已经提供了5种默认的转换规则：dot、snakeCase、camelCase、upperCamelCase和slashes。 另外，Tag(标签)是Micrometer的一个重要的功能，严格来说，一个度量框架只有实现了标签的功能，才能真正地多维度进行度量数据收集。Tag的命名一般需要是有意义的，所谓有意义就是可以根据Tag的命名可以推断出它指向的数据到底代表什么维度或者什么类型的度量指标。假设我们需要监控数据库的调用和Http请求调用统计，一般推荐的做法是： 123MeterRegistry registry = ...registry.counter(&quot;database.calls&quot;, &quot;db&quot;, &quot;users&quot;)registry.counter(&quot;http.requests&quot;, &quot;uri&quot;, &quot;/api/users&quot;) 这样，当我们选择命名为”database.calls”的计数器，我们可以进一步选择分组”db”或者”users”分别统计不同分组对总调用数的贡献或者组成。一个反例如下： 12345678MeterRegistry registry = ...registry.counter(&quot;calls&quot;, &quot;class&quot;, &quot;database&quot;, &quot;db&quot;, &quot;users&quot;);registry.counter(&quot;calls&quot;, &quot;class&quot;, &quot;http&quot;, &quot;uri&quot;, &quot;/api/users&quot;); 通过命名”calls”得到的计数器，由于标签混乱，数据是基本无法分组统计分析，这个时候可以认为得到的时间序列的统计数据是没有意义的。可以定义全局的Tag，也就是全局的Tag定义之后，会附加到所有的使用到的Meter上(只要是使用同一MeterRegistry)，全局的Tag可以这样定义： 1234567891011121314MeterRegistry registry = ...registry.counter(&quot;calls&quot;, &quot;class&quot;, &quot;database&quot;, &quot;db&quot;, &quot;users&quot;);registry.counter(&quot;calls&quot;, &quot;class&quot;, &quot;http&quot;, &quot;uri&quot;, &quot;/api/users&quot;); MeterRegistry registry = ...registry.config().commonTags(&quot;stack&quot;, &quot;prod&quot;, &quot;region&quot;, &quot;us-east-1&quot;);// 和上面的意义是一样的registry.config().commonTags(Arrays.asList(Tag.of(&quot;stack&quot;, &quot;prod&quot;), Tag.of(&quot;region&quot;, &quot;us-east-1&quot;))); 像上面这样子使用，就能通过主机，实例，区域，堆栈等操作环境进行多维度深入分析。 还有两点点需要注意： 1、Tag的值必须不为null。 2、Micrometer中，Tag必须成对出现，也就是Tag必须设置为偶数个，实际上它们以Key=Value的形式存在，具体可以看io.micrometer.core.instrument.Tag接口： 12345678910111213public interface Tag extends Comparable&lt;Tag&gt; { String getKey(); String getValue(); static Tag of(String key, String value) { return new ImmutableTag(key, value); } default int compareTo(Tag o) { return this.getKey().compareTo(o.getKey()); }} 当然，有些时候，我们需要过滤一些必要的标签或者名称进行统计，或者为Meter的名称添加白名单，这个时候可以使用MeterFilter。MeterFilter本身提供一些列的静态方法，多个MeterFilter可以叠加或者组成链实现用户最终的过滤策略。例如： 1234MeterRegistry registry = ...registry.config() .meterFilter(MeterFilter.ignoreTags(&quot;http&quot;)) .meterFilter(MeterFilter.denyNameStartsWith(&quot;jvm&quot;)); 表示忽略”http”标签，拒绝名称以”jvm”字符串开头的Meter。更多用法可以参详一下MeterFilter这个类。 Meter的命名和Meter的Tag相互结合，以命名为轴心，以Tag为多维度要素，可以使度量数据的维度更加丰富，便于统计和分析。 Meters前面提到Meter主要包括：Timer，Counter，Gauge，DistributionSummary，LongTaskTimer，FunctionCounter，FunctionTimer和TimeGauge。下面逐一分析它们的作用和个人理解的实际使用场景(应该说是生产环境)。 CounterCounter是一种比较简单的Meter，它是一种单值的度量类型，或者说是一个单值计数器。Counter接口允许使用者使用一个固定值(必须为正数)进行计数。准确来说：Counter就是一个增量为正数的单值计数器。这个举个很简单的使用例子： 1234MeterRegistry meterRegistry = new SimpleMeterRegistry();Counter counter = meterRegistry.counter(&quot;http.request&quot;, &quot;createOrder&quot;, &quot;/order/create&quot;);counter.increment();System.out.println(counter.measure()); // [Measurement{statistic='COUNT', value=1.0}] 使用场景： Counter的作用是记录XXX的总量或者计数值，适用于一些增长类型的统计，例如下单、支付次数、Http请求总量记录等等，通过Tag可以区分不同的场景，对于下单，可以使用不同的Tag标记不同的业务来源或者是按日期划分，对于Http请求总量记录，可以使用Tag区分不同的URL。用下单业务举个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//实体@Datapublic class Order { private String orderId; private Integer amount; private String channel; private LocalDateTime createTime;}public class CounterMain { private static final DateTimeFormatter FORMATTER = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd&quot;); static { Metrics.addRegistry(new SimpleMeterRegistry()); } public static void main(String[] args) throws Exception { Order order1 = new Order(); order1.setOrderId(&quot;ORDER_ID_1&quot;); order1.setAmount(100); order1.setChannel(&quot;CHANNEL_A&quot;); order1.setCreateTime(LocalDateTime.now()); createOrder(order1); Order order2 = new Order(); order2.setOrderId(&quot;ORDER_ID_2&quot;); order2.setAmount(200); order2.setChannel(&quot;CHANNEL_B&quot;); order2.setCreateTime(LocalDateTime.now()); createOrder(order2); Search.in(Metrics.globalRegistry).meters().forEach(each -&gt; { StringBuilder builder = new StringBuilder(); builder.append(&quot;name:&quot;) .append(each.getId().getName()) .append(&quot;,tags:&quot;) .append(each.getId().getTags()) .append(&quot;,type:&quot;).append(each.getId().getType()) .append(&quot;,value:&quot;).append(each.measure()); System.out.println(builder.toString()); }); } private static void createOrder(Order order) { //忽略订单入库等操作 Metrics.counter(&quot;order.create&quot;, &quot;channel&quot;, order.getChannel(), &quot;createTime&quot;, FORMATTER.format(order.getCreateTime())).increment(); }} 控制台输出 12name:order.create,tags:[tag(channel=CHANNEL_A), tag(createTime=2018-11-10)],type:COUNTER,value:[Measurement{statistic='COUNT', value=1.0}]name:order.create,tags:[tag(channel=CHANNEL_B), tag(createTime=2018-11-10)],type:COUNTER,value:[Measurement{statistic='COUNT', value=1.0}] 上面的例子是使用全局静态方法工厂类Metrics去构造Counter实例，实际上，io.micrometer.core.instrument.Counter接口提供了一个内部建造器类Counter.Builder去实例化Counter，Counter.Builder的使用方式如下： 1234567891011public class CounterBuilderMain { public static void main(String[] args) throws Exception{ Counter counter = Counter.builder(&quot;name&quot;) //名称 .baseUnit(&quot;unit&quot;) //基础单位 .description(&quot;desc&quot;) //描述 .tag(&quot;tagKey&quot;, &quot;tagValue&quot;) //标签 .register(new SimpleMeterRegistry());//绑定的MeterRegistry counter.increment(); }} FunctionCounterFunctionCounter是Counter的特化类型，它把计数器数值增加的动作抽象成接口类型ToDoubleFunction，这个接口JDK1.8中对于Function的特化类型接口。FunctionCounter的使用场景和Counter是一致的，这里介绍一下它的用法： 12345678910111213141516171819202122public class FunctionCounterMain { public static void main(String[] args) throws Exception { MeterRegistry registry = new SimpleMeterRegistry(); AtomicInteger n = new AtomicInteger(0); //这里ToDoubleFunction匿名实现其实可以使用Lambda表达式简化为AtomicInteger::get FunctionCounter.builder(&quot;functionCounter&quot;, n, new ToDoubleFunction&lt;AtomicInteger&gt;() { @Override public double applyAsDouble(AtomicInteger value) { return value.get(); } }).baseUnit(&quot;function&quot;) .description(&quot;functionCounter&quot;) .tag(&quot;createOrder&quot;, &quot;CHANNEL-A&quot;) .register(registry); //下面模拟三次计数 n.incrementAndGet(); n.incrementAndGet(); n.incrementAndGet(); }} FunctionCounter使用的一个明显的好处是，我们不需要感知FunctionCounter实例的存在，实际上我们只需要操作作为FunctionCounter实例构建元素之一的AtomicInteger实例即可，这种接口的设计方式在很多框架里面都可以看到。 TimerTimer(计时器)适用于记录耗时比较短的事件的执行时间，通过时间分布展示事件的序列和发生频率。所有的Timer的实现至少记录了发生的事件的数量和这些事件的总耗时，从而生成一个时间序列。Timer的基本单位基于服务端的指标而定，但是实际上我们不需要过于关注Timer的基本单位，因为Micrometer在存储生成的时间序列的时候会自动选择适当的基本单位。Timer接口提供的常用方法如下： 1234567891011121314151617181920212223242526272829303132333435363738public interface Timer extends Meter { ... void record(long var1, TimeUnit var3); default void record(Duration duration) { this.record(duration.toNanos(), TimeUnit.NANOSECONDS); } &lt;T&gt; T record(Supplier&lt;T&gt; var1); &lt;T&gt; T recordCallable(Callable&lt;T&gt; var1) throws Exception; void record(Runnable var1); default Runnable wrap(Runnable f) { return () -&gt; { this.record(f); }; } default &lt;T&gt; Callable&lt;T&gt; wrap(Callable&lt;T&gt; f) { return () -&gt; { return this.recordCallable(f); }; } long count(); double totalTime(TimeUnit var1); default double mean(TimeUnit unit) { return this.count() == 0L ? 0.0D : this.totalTime(unit) / (double)this.count(); } double max(TimeUnit var1); ...} 实际上，比较常用和方便的方法是几个函数式接口入参的方法： 123456Timer timer = ...timer.record(() -&gt; dontCareAboutReturnValue());timer.recordCallable(() -&gt; returnValue());Runnable r = timer.wrap(() -&gt; dontCareAboutReturnValue());Callable c = timer.wrap(() -&gt; returnValue()); 使用场景： 根据个人经验和实践，总结如下： 1、记录指定方法的执行时间用于展示。 2、记录一些任务的执行时间，从而确定某些数据来源的速率，例如消息队列消息的消费速率等。 这里举个实际的例子，要对系统做一个功能，记录指定方法的执行时间，还是用下单方法做例子： 123456789101112131415161718192021222324252627public class TimerMain { private static final Random R = new Random(); static { Metrics.addRegistry(new SimpleMeterRegistry()); } public static void main(String[] args) throws Exception { Order order1 = new Order(); order1.setOrderId(&quot;ORDER_ID_1&quot;); order1.setAmount(100); order1.setChannel(&quot;CHANNEL_A&quot;); order1.setCreateTime(LocalDateTime.now()); Timer timer = Metrics.timer(&quot;timer&quot;, &quot;createOrder&quot;, &quot;cost&quot;); timer.record(() -&gt; createOrder(order1)); } private static void createOrder(Order order) { try { TimeUnit.SECONDS.sleep(R.nextInt(5)); //模拟方法耗时 } catch (InterruptedException e) { //no-op } }} 在实际生产环境中，可以通过spring-aop把记录方法耗时的逻辑抽象到一个切面中，这样就能减少不必要的冗余的模板代码。上面的例子是通过Mertics构造Timer实例，实际上也可以使用Builder构造： 123456MeterRegistry registry = ...Timer timer = Timer .builder(&quot;my.timer&quot;) .description(&quot;a description of what this timer does&quot;) // 可选 .tags(&quot;region&quot;, &quot;test&quot;) // 可选 .register(registry); 另外，Timer的使用还可以基于它的内部类Timer.Sample，通过start和stop两个方法记录两者之间的逻辑的执行耗时。例如： 123456Timer.Sample sample = Timer.start(registry);// 这里做业务逻辑Response response = ...sample.stop(registry.timer(&quot;my.timer&quot;, &quot;response&quot;, response.status())); FunctionTimerFunctionTimer是Timer的特化类型，它主要提供两个单调递增的函数(其实并不是单调递增，只是在使用中一般需要随着时间最少保持不变或者说不减少)：一个用于计数的函数和一个用于记录总调用耗时的函数，它的建造器的入参如下： 12345678public interface FunctionTimer extends Meter { static &lt;T&gt; Builder&lt;T&gt; builder(String name, T obj, ToLongFunction&lt;T&gt; countFunction, ToDoubleFunction&lt;T&gt; totalTimeFunction, TimeUnit totalTimeFunctionUnit) { return new Builder&lt;&gt;(name, obj, countFunction, totalTimeFunction, totalTimeFunctionUnit); } ...} 官方文档中的例子如下： 123456IMap&lt;?, ?&gt; cache = ...; // 假设使用了Hazelcast缓存registry.more().timer(&quot;cache.gets.latency&quot;, Tags.of(&quot;name&quot;, cache.getName()), cache, c -&gt; c.getLocalMapStats().getGetOperationCount(), //实际上就是cache的一个方法，记录缓存生命周期初始化的增量(个数) c -&gt; c.getLocalMapStats().getTotalGetLatency(), // Get操作的延迟时间总量，可以理解为耗时 TimeUnit.NANOSECONDS); 按照个人理解，ToDoubleFunction用于统计事件个数，ToDoubleFunction用于记录执行总时间，实际上两个函数都只是Function函数的变体，还有一个比较重要的是总时间的单位totalTimeFunctionUnit。简单的使用方式如下： 1234567891011121314public class FunctionTimerMain { public static void main(String[] args) throws Exception { //这个是为了满足参数,暂时不需要理会 Object holder = new Object(); AtomicLong totalTimeNanos = new AtomicLong(0); AtomicLong totalCount = new AtomicLong(0); FunctionTimer.builder(&quot;functionTimer&quot;, holder, p -&gt; totalCount.get(), p -&gt; totalTimeNanos.get(), TimeUnit.NANOSECONDS) .register(new SimpleMeterRegistry()); totalTimeNanos.addAndGet(10000000); totalCount.incrementAndGet(); }} LongTaskTimerLongTaskTimer也是一种Timer的特化类型，主要用于记录长时间执行的任务的持续时间，在任务完成之前，被监测的事件或者任务仍然处于运行状态，任务完成的时候，任务执行的总耗时才会被记录下来。LongTaskTimer适合用于长时间持续运行的事件耗时的记录，例如相对耗时的定时任务。在Spring应用中，可以简单地使用@Scheduled和@Timed注解，基于spring-aop完成定时调度任务的总耗时记录： 12345@Timed(value = &quot;aws.scrape&quot;, longTask = true)@Scheduled(fixedDelay = 360000)void scrapeResources() { //这里做相对耗时的业务逻辑} 当然，在非spring体系中也能方便地使用LongTaskTimer： 123456789101112131415public class LongTaskTimerMain { public static void main(String[] args) throws Exception{ MeterRegistry meterRegistry = new SimpleMeterRegistry(); LongTaskTimer longTaskTimer = meterRegistry.more().longTaskTimer(&quot;longTaskTimer&quot;); longTaskTimer.record(() -&gt; { //这里编写Task的逻辑 }); //或者这样 Metrics.more().longTaskTimer(&quot;longTaskTimer&quot;).record(()-&gt; { //这里编写Task的逻辑 }); }} GaugeGauge(仪表)是获取当前度量记录值的句柄，也就是它表示一个可以任意上下浮动的单数值度量Meter。Gauge通常用于变动的测量值，测量值用ToDoubleFunction参数的返回值设置，如当前的内存使用情况，同时也可以测量上下移动的”计数”，比如队列中的消息数量。官网文档中提到Gauge的典型使用场景是用于测量集合或映射的大小或运行状态中的线程数。Gauge一般用于监测有自然上界的事件或者任务，而Counter一般使用于无自然上界的事件或者任务的监测，所以像Http请求总量计数应该使用Counter而非Gauge。MeterRegistry中提供了一些便于构建用于观察数值、函数、集合和映射的Gauge相关的方法： 1234List&lt;String&gt; list = registry.gauge(&quot;listGauge&quot;, Collections.emptyList(), new ArrayList&lt;&gt;(), List::size); List&lt;String&gt; list2 = registry.gaugeCollectionSize(&quot;listSize2&quot;, Tags.empty(), new ArrayList&lt;&gt;()); Map&lt;String, Integer&gt; map = registry.gaugeMapSize(&quot;mapGauge&quot;, Tags.empty(), new HashMap&lt;&gt;()); 上面的三个方法通过MeterRegistry构建Gauge并且返回了集合或者映射实例，使用这些集合或者映射实例就能在其size变化过程中记录这个变更值。更重要的优点是，我们不需要感知Gauge接口的存在，只需要像平时一样使用集合或者映射实例就可以了。此外，Gauge还支持java.lang.Number的子类，java.util.concurrent.atomic包中的AtomicInteger和AtomicLong，还有Guava提供的AtomicDouble： 123AtomicInteger n = registry.gauge(&quot;numberGauge&quot;, new AtomicInteger(0));n.set(1);n.set(2); 除了使用MeterRegistry创建Gauge之外，还可以使用建造器流式创建： 1234567//一般我们不需要操作Gauge实例Gauge gauge = Gauge .builder(&quot;gauge&quot;, myObj, myObj::gaugeValue) .description(&quot;a description of what this gauge does&quot;) // 可选 .tags(&quot;region&quot;, &quot;test&quot;) // 可选 .register(registry); 使用场景： 根据个人经验和实践，总结如下： 1、有自然(物理)上界的浮动值的监测，例如物理内存、集合、映射、数值等。 2、有逻辑上界的浮动值的监测，例如积压的消息、(线程池中)积压的任务等，其实本质也是集合或者映射的监测。 举个相对实际的例子，假设我们需要对登录后的用户发送一条短信或者推送，做法是消息先投放到一个阻塞队列，再由一个线程消费消息进行其他操作： 123456789101112131415161718192021222324252627282930313233public class GaugeMain { private static final MeterRegistry MR = new SimpleMeterRegistry(); private static final BlockingQueue&lt;Message&gt; QUEUE = new ArrayBlockingQueue&lt;&gt;(500); private static BlockingQueue&lt;Message&gt; REAL_QUEUE; static { REAL_QUEUE = MR.gauge(&quot;messageGauge&quot;, QUEUE, Collection::size); } public static void main(String[] args) throws Exception { consume(); Message message = new Message(); message.setUserId(1L); message.setContent(&quot;content&quot;); REAL_QUEUE.put(message); } private static void consume() throws Exception { new Thread(() -&gt; { while (true) { try { Message message = REAL_QUEUE.take(); //handle message System.out.println(message); } catch (InterruptedException e) { //no-op } } }).start(); }} 上面的例子代码写得比较糟糕，只为了演示相关使用方式，切勿用于生产环境。 TimeGaugeTimeGauge是Gauge的特化类型，相比Gauge，它的构建器中多了一个TimeUnit类型的参数，用于指定ToDoubleFunction入参的基础时间单位。这里简单举个使用例子： 123456789101112131415161718192021222324252627282930313233public class TimeGaugeMain { private static final SimpleMeterRegistry R = new SimpleMeterRegistry(); public static void main(String[] args) throws Exception{ AtomicInteger count = new AtomicInteger(); TimeGauge.Builder&lt;AtomicInteger&gt; timeGauge = TimeGauge.builder(&quot;timeGauge&quot;, count, TimeUnit.SECONDS, AtomicInteger::get); timeGauge.register(R); count.addAndGet(10086); print(); count.set(1); print(); } private static void print()throws Exception{ Search.in(R).meters().forEach(each -&gt; { StringBuilder builder = new StringBuilder(); builder.append(&quot;name:&quot;) .append(each.getId().getName()) .append(&quot;,tags:&quot;) .append(each.getId().getTags()) .append(&quot;,type:&quot;).append(each.getId().getType()) .append(&quot;,value:&quot;).append(each.measure()); System.out.println(builder.toString()); }); } }//输出name:timeGauge,tags:[],type:GAUGE,value:[Measurement{statistic='VALUE', value=10086.0}]name:timeGauge,tags:[],type:GAUGE,value:[Measurement{statistic='VALUE', value=1.0}] DistributionSummarySummary(摘要)主要用于跟踪事件的分布，在Micrometer中，对应的类是DistributionSummary(分发摘要)。它的使用方式和Timer十分相似，但是它的记录值并不依赖于时间单位。常见的使用场景：使用DistributionSummary测量命中服务器的请求的有效负载大小。使用MeterRegistry创建DistributionSummary实例如下： 1DistributionSummary summary = registry.summary(&quot;response.size&quot;); 通过建造器流式创建如下： 12345678DistributionSummary summary = DistributionSummary .builder(&quot;response.size&quot;) .description(&quot;a description of what this summary does&quot;) // 可选 .baseUnit(&quot;bytes&quot;) // 可选 .tags(&quot;region&quot;, &quot;test&quot;) // 可选 .scale(100) // 可选 .register(registry); DistributionSummary中有很多构建参数跟缩放和直方图的表示相关，见下一节。 使用场景： 根据个人经验和实践，总结如下： 1、不依赖于时间单位的记录值的测量，例如服务器有效负载值，缓存的命中率等。 举个相对具体的例子： 1234567891011121314151617181920212223242526272829public class DistributionSummaryMain { private static final DistributionSummary DS = DistributionSummary.builder(&quot;cacheHitPercent&quot;) .register(new SimpleMeterRegistry()); private static final LoadingCache&lt;String, String&gt; CACHE = CacheBuilder.newBuilder() .maximumSize(1000) .recordStats() .expireAfterWrite(60, TimeUnit.SECONDS) .build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String s) throws Exception { return selectFromDatabase(); } }); public static void main(String[] args) throws Exception{ String key = &quot;doge&quot;; String value = CACHE.get(key); record(); } private static void record()throws Exception{ CacheStats stats = CACHE.stats(); BigDecimal hitCount = new BigDecimal(stats.hitCount()); BigDecimal requestCount = new BigDecimal(stats.requestCount()); DS.record(hitCount.divide(requestCount,2,BigDecimal.ROUND_HALF_DOWN).doubleValue()); }} 直方图和百分数配置直方图和百分数配置适用于Summary和Timer，这部分相对复杂，等研究透了再补充。 基于SpirngBoot、Prometheus、Grafana集成集成了Micrometer框架的JVM应用使用到Micrometer的API收集的度量数据位于内存之中，因此，需要额外的存储系统去存储这些度量数据，需要有监控系统负责统一收集和处理这些数据，还需要有一些UI工具去展示数据，一般大佬只喜欢看炫酷的图表或者动画。常见的存储系统就是时序数据库，主流的有Influx、Datadog等。比较主流的监控系统(主要是用于数据收集和处理)就是Prometheus(一般叫普罗米修斯，下面就这样叫吧)。而展示的UI目前相对用得比较多的就是Grafana。另外，Prometheus已经内置了一个时序数据库的实现，因此，在做一套相对完善的度量数据监控的系统只需要依赖目标JVM应用，Prometheus组件和Grafana组件即可。下面花一点时间从零开始搭建一个这样的系统，之前写的一篇文章基于Windows系统，操作可能跟生产环境不够接近，这次使用CentOS7。 SpirngBoot中使用MicrometerSpringBoot中的spring-boot-starter-actuator依赖已经集成了对Micrometer的支持，其中的metrics端点的很多功能就是通过Micrometer实现的，prometheus端点默认也是开启支持的，实际上actuator依赖的spring-boot-actuator-autoconfigure中集成了对很多框架的开箱即用的API，其中prometheus包中集成了对Prometheus的支持，使得使用了actuator可以轻易地让项目暴露出prometheus端点，作为Prometheus收集数据的客户端，Prometheus(服务端软件)可以通过此端点收集应用中Micrometer的度量数据。 我们先引入spring-boot-starter-actuator和spring-boot-starter-web，实现一个Counter和Timer作为示例。依赖： 1234567891011121314151617181920212223242526272829303132333435&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.22&lt;/version&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 接着编写一个下单接口和一个消息发送模块，模拟用户下单之后向用户发送消息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130//实体@Datapublic class Message { private String orderId; private Long userId; private String content; } @Data public class Order { private String orderId; private Long userId; private Integer amount; private LocalDateTime createTime; } //控制器和服务类 @RestController public class OrderController { @Autowired private OrderService orderService; @PostMapping(value = &quot;/order&quot;) public ResponseEntity&lt;Boolean&gt; createOrder(@RequestBody Order order){ return ResponseEntity.ok(orderService.createOrder(order)); } } @Slf4j @Service public class OrderService { private static final Random R = new Random(); @Autowired private MessageService messageService; public Boolean createOrder(Order order) { //模拟下单 try { int ms = R.nextInt(50) + 50; TimeUnit.MILLISECONDS.sleep(ms); log.info(&quot;保存订单模拟耗时{}毫秒...&quot;, ms); } catch (Exception e) { //no-op } //记录下单总数 Metrics.counter(&quot;order.count&quot;, &quot;order.channel&quot;, order.getChannel()).increment(); //发送消息 Message message = new Message(); message.setContent(&quot;模拟短信...&quot;); message.setOrderId(order.getOrderId()); message.setUserId(order.getUserId()); messageService.sendMessage(message); return true; } } @Slf4j @Service public class MessageService implements InitializingBean { private static final BlockingQueue&lt;Message&gt; QUEUE = new ArrayBlockingQueue&lt;&gt;(500); private static BlockingQueue&lt;Message&gt; REAL_QUEUE; private static final Executor EXECUTOR = Executors.newSingleThreadExecutor(); private static final Random R = new Random(); static { REAL_QUEUE = Metrics.gauge(&quot;message.gauge&quot;, Tags.of(&quot;message.gauge&quot;, &quot;message.queue.size&quot;), QUEUE, Collection::size); } public void sendMessage(Message message) { try { REAL_QUEUE.put(message); } catch (InterruptedException e) { //no-op } } @Override public void afterPropertiesSet() throws Exception { EXECUTOR.execute(() -&gt; { while (true) { try { Message message = REAL_QUEUE.take(); log.info(&quot;模拟发送短信,orderId:{},userId:{},内容:{},耗时:{}毫秒&quot;, message.getOrderId(), message.getUserId(), message.getContent(), R.nextInt(50)); } catch (Exception e) { throw new IllegalStateException(e); } } }); } } //切面类 @Component @Aspect public class TimerAspect { @Around(value = &quot;execution(* club.throwable.smp.service.*Service.*(..))&quot;) public Object around(ProceedingJoinPoint joinPoint) throws Throwable { Signature signature = joinPoint.getSignature(); MethodSignature methodSignature = (MethodSignature) signature; Method method = methodSignature.getMethod(); Timer timer = Metrics.timer(&quot;method.cost.time&quot;, &quot;method.name&quot;, method.getName()); ThrowableHolder holder = new ThrowableHolder(); Object result = timer.recordCallable(() -&gt; { try { return joinPoint.proceed(); } catch (Throwable e) { holder.throwable = e; } return null; }); if (null != holder.throwable) { throw holder.throwable; } return result; } private class ThrowableHolder { Throwable throwable; }} yaml的配置如下： 12345678910server: port: 9091management: server: port: 10091 endpoints: web: exposure: include: '*' base-path: /management 注意多看spring官方文档关于Actuator的详细描述，在SpringBoot-2.x之后，配置Web端点暴露的权限控制和1.x有很大的不同。总结一下就是：除了shutdown端点之外，其他端点默认都是开启支持的这里仅仅是开启支持，并不是暴露为Web端点，端点必须暴露为Web端点才能被访问，禁用或者开启端点支持的配置方式如下： 1management.endpoint.${端点ID}.enabled=true/false可以查 可以查看actuator-api文档查看所有支持的端点的特性，这个是2.1.0.RELEASE版本的官方文档，不知道日后链接会不会挂掉。端点只开启支持，但是不暴露为Web端点，是无法通过http://{host}:{management.port}/{management.endpoints.web.base-path}/{endpointId}访问的。暴露监控端点为Web端点的配置是： 12management.endpoints.web.exposure.include=info,healthmanagement.endpoints.web.exposure.exclude=prometheus management.endpoints.web.exposure.exclude用于指定不暴露为Web端点的监控端点，指定多个的时候用英文逗号分隔management.endpoints.web.exposure.include默认指定的只有info和health两个端点，我们可以直接指定暴露所有的端点：management.endpoints.web.exposure.include=*，如果采用YAML配置，记得*要加单引号’*‘。暴露所有Web监控端点是一件比较危险的事情，如果需要在生产环境这样做，请务必先确认http://{host}:{management.port}不能通过公网访问(也就是监控端点访问的端口只能通过内网访问，这样可以方便后面说到的Prometheus服务端通过此端口收集数据)。 Prometheus的安装和配置Prometheus目前的最新版本是2.5，鉴于笔者没深入玩过Docker，这里还是直接下载它的压缩包解压安装。 123wget https://github.com/prometheus/prometheus/releases/download/v2.5.0/prometheus-2.5.0.linux-amd64.tar.gztar xvfz prometheus-*.tar.gzcd prometheus-* 先编辑解压出来的目录下的prometheus配置文件prometheus.yml，主要修改scrape_configs节点的属性： 1234567891011scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. # 这里配置需要拉取度量信息的URL路径，这里选择应用程序的prometheus端点 metrics_path: /management/prometheus static_configs: # 这里配置host和port - targets: ['localhost:10091'] 配置拉取度量数据的路径为localhost:10091/management/metrics，此前记得把前一节提到的应用在虚拟机中启动。接着启动Prometheus应用： 12# 参数 --storage.tsdb.path=存储数据的路径，默认路径为./data./prometheus --config.file=prometheus.yml Prometheus引用的默认启动端口是9090，启动成功后，日志如下： 此时，访问ttp://${虚拟机host}:9090/targets就能看到当前Prometheus中执行的Job 访问ttp://${虚拟机host}:9090/graph以查找到我们定义的度量Meter和spring-boot-starter-actuator中已经定义好的一些关于JVM或者Tomcat的度量Meter。我们先对应用的/order接口进行调用，然后查看一下监控前面在应用中定义的rder_count_total``ethod_cost_time_seconds_sum 可以看到，Meter的信息已经被收集和展示，但是显然不够详细和炫酷，这个时候就需要使用Grafana的UI做一下点缀。 Grafana的安装和使用Grafana的安装过程如下： 12wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.3.4-1.x86_64.rpm sudo yum localinstall grafana-5.3.4-1.x86_64.rpm 安装完成后，通过命令service grafana-server start启动即可，默认的启动端口为3000，通过ttp://${host}:3000即可。初始的账号密码都为admin，权限是管理员权限。接着需要在Home面板添加一个数据源，目的是对接Prometheus服务端从而可以拉取它里面的度量数据。数据源添加面板如下： 其实就是指向Prometheus服务端的端口就可以了。接下来可以天马行空地添加需要的面板，就下单数量统计的指标，可以添加一个Graph的面板 配置面板的时候，需要在基础(General)中指定Title： 接着比较重要的是Metrics的配置，需要指定数据源和Prometheus的查询语句： 最好参考一下Prometheus的官方文档，稍微学习一下它的查询语言PromQL的使用方式，一个面板可以支持多个PromQL查询。前面提到的两项是基本配置，其他配置项一般是图表展示的辅助或者预警等辅助功能，这里先不展开，可以取Grafana的官网挖掘一下使用方式。然后我们再调用一下下单接口，过一段时间，图表的数据就会自动更新和展示： 接着添加一下项目中使用的Timer的Meter，便于监控方法的执行时间，完成之后大致如下： 文章作者:throwable 文章链接:http://www.throwable.club/2018/11/17/jvm-micrometer-prometheus/ 版权声明:本博客所有文章除特别声明外，均采用CC BY-NC-SA 4.0许可协议。转载请注明来自Throwable！","link":"/2019/08/13/%E7%BB%99%E4%BD%A0%E7%9A%84SpringBoot%E5%81%9A%E5%9F%8B%E7%82%B9%E7%9B%91%E6%8E%A7-JVM%E5%BA%94%E7%94%A8%E5%BA%A6%E9%87%8F%E6%A1%86%E6%9E%B6Micrometer/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","link":"/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"java基础","slug":"java基础","link":"/tags/java%E5%9F%BA%E7%A1%80/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"技巧","slug":"技巧","link":"/tags/%E6%8A%80%E5%B7%A7/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"java核心","slug":"java核心","link":"/tags/java%E6%A0%B8%E5%BF%83/"},{"name":"mybatisPlus","slug":"mybatisPlus","link":"/tags/mybatisPlus/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/tags/SpringBoot/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"基本命令","slug":"基本命令","link":"/tags/%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"name":"签名","slug":"签名","link":"/tags/%E7%AD%BE%E5%90%8D/"},{"name":"性能","slug":"性能","link":"/tags/%E6%80%A7%E8%83%BD/"},{"name":"servlet","slug":"servlet","link":"/tags/servlet/"},{"name":"heroku","slug":"heroku","link":"/tags/heroku/"},{"name":"nio","slug":"nio","link":"/tags/nio/"},{"name":"图片识别","slug":"图片识别","link":"/tags/%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"分布式事务","slug":"分布式事务","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"面试","slug":"面试","link":"/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"mq","slug":"mq","link":"/tags/mq/"},{"name":"小程序","slug":"小程序","link":"/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"防盗链","slug":"防盗链","link":"/tags/%E9%98%B2%E7%9B%97%E9%93%BE/"},{"name":"事务","slug":"事务","link":"/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"信号量","slug":"信号量","link":"/tags/%E4%BF%A1%E5%8F%B7%E9%87%8F/"}],"categories":[{"name":"Effective Java","slug":"Effective-Java","link":"/categories/Effective-Java/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"基础","slug":"java/基础","link":"/categories/java/%E5%9F%BA%E7%A1%80/"},{"name":"Spring Boot","slug":"Spring-Boot","link":"/categories/Spring-Boot/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Mysql","slug":"数据库/Mysql","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Mysql/"},{"name":"Netty","slug":"Netty","link":"/categories/Netty/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"dubbo","slug":"dubbo","link":"/categories/dubbo/"},{"name":"基础","slug":"Java/基础","link":"/categories/Java/%E5%9F%BA%E7%A1%80/"},{"name":"nodejs","slug":"nodejs","link":"/categories/nodejs/"},{"name":"杂谈","slug":"杂谈","link":"/categories/%E6%9D%82%E8%B0%88/"},{"name":"J2EE","slug":"J2EE","link":"/categories/J2EE/"},{"name":"mq","slug":"mq","link":"/categories/mq/"},{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"},{"name":"服务器","slug":"服务器","link":"/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"网站","slug":"网站","link":"/categories/%E7%BD%91%E7%AB%99/"},{"name":"SpringCloud商城","slug":"SpringCloud商城","link":"/categories/SpringCloud%E5%95%86%E5%9F%8E/"},{"name":"git","slug":"git","link":"/categories/git/"},{"name":"事务","slug":"事务","link":"/categories/%E4%BA%8B%E5%8A%A1/"},{"name":"面试","slug":"面试","link":"/categories/%E9%9D%A2%E8%AF%95/"},{"name":"题目","slug":"面试/题目","link":"/categories/%E9%9D%A2%E8%AF%95/%E9%A2%98%E7%9B%AE/"}]}